{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MTS Generation with COSCI-GAN.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from configs.Metric import Metric\n",
    "from configs.SimulatedData import Proposed\n",
    "from utils.metric import signature_on_batch, signature_metric\n",
    "import mlflow\n",
    "from sklearn.decomposition import PCA\n",
    "from datetime import datetime\n",
    "import io\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs.\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized.\n",
    "    print(e)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config= Proposed()\n",
    "\n",
    "SEQUENCE_LENGTH = config.sequence_lenght_in_sample\n",
    "GRANUARITY = config.granularity\n",
    "OVERLAP = config.overlap\n",
    "BS = config.batch_size\n",
    "EPOCHS = config.epochs\n",
    "NUM_SEQUENCE_TO_GENERATE = config.met_params.sequence_to_generate\n",
    "STEP_LIMITATION = 1000\n",
    "UPDATE_DISCRIMINATOR = 20\n",
    "\n",
    "SIMULATED_DATA_PATH = \"data/simulated_dataset/01 - Source Domain.h5\"\n",
    "N_SAMPLE_WIENER = SEQUENCE_LENGTH//4\n",
    "FEAT_WIENER = 2\n",
    "NOISE_DIM= (N_SAMPLE_WIENER, FEAT_WIENER)\n",
    "\n",
    "LAMBDA_GLOBAL = 0.001\n",
    "LAMBDA_LOCAL = 0.001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dataframe_to_tensorflow_sequences(df:pd.DataFrame, sequence_lenght_in_sample, granularity, shift_between_sequences, batch_size, shuffle=True):\n",
    "    sequence_lenght = int(sequence_lenght_in_sample*granularity)\n",
    "\n",
    "    dset = tf.data.Dataset.from_tensor_slices(df.values)\n",
    "    dset = dset.window(sequence_lenght , shift=shift_between_sequences, stride=granularity).flat_map(lambda x: x.batch(sequence_lenght_in_sample, drop_remainder=True))\n",
    "\n",
    "    if shuffle:\n",
    "        dset= dset.shuffle(256)\n",
    "\n",
    "    dset = dset.batch(batch_size, drop_remainder=True)\n",
    "\n",
    "    dset = dset.cache().prefetch(10)\n",
    "\n",
    "    return dset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_simulated = pd.read_hdf(SIMULATED_DATA_PATH)\n",
    "df_simulated = df_simulated.drop(columns='labels')\n",
    "\n",
    "dset_simulated = convert_dataframe_to_tensorflow_sequences(\n",
    "    df_simulated, \n",
    "    SEQUENCE_LENGTH, \n",
    "    GRANUARITY, \n",
    "    int(OVERLAP* SEQUENCE_LENGTH),\n",
    "    BS\n",
    ")\n",
    "\n",
    "dset_simulated = dset_simulated.take(STEP_LIMITATION)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify if it is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = next(iter(dset_simulated))[0]\n",
    "print(sequence.shape)\n",
    "\n",
    "plt.figure(figsize=(18, 5))\n",
    "plt.title(\"Simulated Sequence.\")\n",
    "for i in range(sequence.shape[1]):\n",
    "    plt.plot(sequence[:, i], label=f'feat {i+1}')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make some Wiener Noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wiener_process(batch:int, n_sample_wiener:int, n_feat_wiener:int):\n",
    "    d_noise = tf.random.normal([batch, n_sample_wiener, n_feat_wiener])\n",
    "    wiener_noise = tf.math.cumsum(d_noise, axis=1)\n",
    "    return wiener_noise\n",
    "\n",
    "\n",
    "seed = wiener_process(NUM_SEQUENCE_TO_GENERATE, N_SAMPLE_WIENER, FEAT_WIENER)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_arrow(A, B, color=\"b\"):\n",
    "    plt.arrow(A[0], A[1], B[0] - A[0], B[1] - A[1],\n",
    "              length_includes_head=True, color=color)\n",
    "    \n",
    "def draw_arrows(xs, ys, color=\"b\"):\n",
    "    for i in range(xs.shape[0]-1):\n",
    "        point0 = [xs[i], ys[i]]\n",
    "        point1 = [xs[i+1], ys[i+1]]\n",
    "        draw_arrow(point0, point1, color=color)\n",
    "\n",
    "plt.figure(figsize=(18, 5))\n",
    "plt.title(\"Example of the wiener process.\")\n",
    "\n",
    "draw_arrows(seed[0,:,0], seed[0,:,1], color=\"tab:blue\")\n",
    "plt.scatter(seed[0,:,0], seed[0,:,1], label='Wiener Process.', color='tab:blue')\n",
    "\n",
    "plt.grid()\n",
    "plt.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Model Architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def make_generator(n_sample_wiener:int, feat_wiener:int):\n",
    "    \n",
    "    init = tf.keras.initializers.RandomNormal()\n",
    "\n",
    "    _content_input = tf.keras.Input((n_sample_wiener, feat_wiener))\n",
    "\n",
    "    # Make a small projection...\n",
    "\n",
    "    x = tf.keras.layers.Flatten()(_content_input)\n",
    "    x = tf.keras.layers.Dense(n_sample_wiener* feat_wiener, name='1', kernel_initializer=init)(x)\n",
    "    x = tf.keras.layers.Reshape((n_sample_wiener, feat_wiener))(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv1DTranspose(256, 5, 2, padding='same', kernel_initializer=init)(x)\n",
    "    x = tf.keras.layers.LeakyReLU()(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv1DTranspose(256, 5, 1, padding='same', kernel_initializer=init)(x)\n",
    "    x = tf.keras.layers.LeakyReLU()(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv1DTranspose(1, 5, 2, padding='same', kernel_initializer=init)(x)\n",
    "    x = tf.keras.layers.LeakyReLU()(x)\n",
    "\n",
    "    model = tf.keras.Model(_content_input, x)\n",
    "    return model\n",
    "\n",
    "\n",
    "generator = make_generator(16, 2)\n",
    "generator.summary()\n",
    "NOISE_DIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_global_discriminator(seq_length:int, n_feat:int):\n",
    "    _input = tf.keras.Input((seq_length, n_feat))\n",
    "    x = tf.keras.layers.Conv1D(8, 5, 2, padding='same')(_input)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dropout(0.25)(x)\n",
    "    _output = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "    model = tf.keras.Model(_input, _output)\n",
    "    early_predictor = tf.keras.Model(_input, x, name=\"Local Discriminator\")\n",
    "\n",
    "    return model, early_predictor\n",
    "\n",
    "\n",
    "def local_discriminator(seq_length:int):\n",
    "    _input = tf.keras.Input((seq_length, 1))\n",
    "\n",
    "    x = tf.keras.layers.Conv1D(32, 5, 2, padding='same')(_input)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv1D(32, 5, 2, padding='same')(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "\n",
    "    x = layers.Flatten()(x)\n",
    "    _output = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "    model = tf.keras.Model(_input, _output, name=\"local discriminator\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generators = [make_generator(16, 2) for _ in range(df_simulated.shape[1])]\n",
    "local_discriminators = [local_discriminator(SEQUENCE_LENGTH) for _ in range(df_simulated.shape[1])]\n",
    "\n",
    "global_discriminator, early_predictor = make_global_discriminator(SEQUENCE_LENGTH, df_simulated.shape[1])\n",
    "\n",
    "global_discriminator.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot a Sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(content_wp, training=True):\n",
    "    signals = np.array([g(content_wp, training=training) for g in generators])\n",
    "    signals = tf.transpose(signals, (1, 2, 0, 3))\n",
    "    signals = tf.reshape(signals, signals.shape[:-1])\n",
    "    return signals\n",
    "\n",
    "def local_discrimination(sequences, training=True):\n",
    "    crit = np.array([d(sequences[:, :, i], training=training) for i, d in enumerate(local_discriminators)])\n",
    "    # crit = tf.transpose(crit, (1, 0, 2))\n",
    "    return crit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated = generate(seed)\n",
    "\n",
    "# after_training_generations\n",
    "def plot_several_generations(generations:np.ndarray, nvertical:int=3, nhoriz:int=3):\n",
    "\n",
    "    legend = [f\"feat {j}\" for j in range(generations.shape[-1])]\n",
    "\n",
    "    plt.figure(figsize=(18, 10))\n",
    "    plt.suptitle(\"Several Generations\")\n",
    "\n",
    "    for i in range(nvertical* nhoriz):\n",
    "        ax = plt.subplot(nvertical, nhoriz, i+ 1)\n",
    "        ax.set_title(f\"sequence {i+1}\")\n",
    "\n",
    "        plt.plot(generations[i])\n",
    "        ax.grid(True)\n",
    "        plt.legend(legend)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_several_generations(generated) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "\n",
    "def similarity_loss(extracted_features:np.ndarray):\n",
    "    anchor = extracted_features[0]\n",
    "    return tf.exp(-(tf.norm(extracted_features[1:]- anchor)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate the reference signature.\n",
    "\n",
    "real_sequence_batch = next(iter(dset_simulated))\n",
    "\n",
    "real_batch_signature= signature_on_batch(real_sequence_batch, config.met_params.ins, config.met_params.outs, config.met_params.signature_length)\n",
    "generated_batch_signature= signature_on_batch(generated, config.met_params.ins, config.met_params.outs, config.met_params.signature_length)\n",
    "\n",
    "signature_metric(real_batch_signature, generated_batch_signature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_opt = [tf.keras.optimizers.RMSprop(1e-2) for _ in generators]\n",
    "discriminator_opt = [tf.keras.optimizers.RMSprop(2e-3) for _ in local_discriminators]\n",
    "global_discriminator_opt = tf.keras.optimizers.RMSprop(2e-3)\n",
    "\n",
    "# generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "# discriminator_optimizer = tf.keras.optimizers.Adam(1e-6)    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_metric = tf.keras.metrics.Mean()\n",
    "local_discriminator_metric = tf.keras.metrics.Mean()\n",
    "global_discriminator_metric = tf.keras.metrics.Mean()\n",
    "similarity_metric = tf.keras.metrics.Mean()\n",
    "correlation_metric = tf.keras.metrics.Mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_str = datetime.now().strftime('%Y-%m-%d_%H_%M_%S')\n",
    "\n",
    "BASE_DIR = f\"log - COSCI-GAN/{date_str} - COSCI-GAN\"\n",
    "TRAIN_LOGS_DIR_PATH = f\"{BASE_DIR}/fit\"\n",
    "GENERATION_LOG = f\"{BASE_DIR}/Generations\"\n",
    "\n",
    "TRAIN_SUMMARY_WRITER = tf.summary.create_file_writer(TRAIN_LOGS_DIR_PATH)\n",
    "\n",
    "\n",
    "def plot_to_buff(generations:np.ndarray, nvertical:int=3, nhoriz:int=3):\n",
    "    legend = [f\"feat {j}\" for j in range(generations.shape[-1])]\n",
    "\n",
    "    fig = plt.figure(figsize=(18, 10))\n",
    "    plt.suptitle(\"Generations After GAN Training.\")\n",
    "\n",
    "    for i in range(nvertical* nhoriz):\n",
    "        ax = plt.subplot(nvertical, nhoriz, i+ 1)\n",
    "        ax.set_title(f\"sequence {i+1}\")\n",
    "\n",
    "        plt.plot(generations[i])\n",
    "        ax.grid(True)\n",
    "        plt.legend(legend)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png')\n",
    "    buf.seek(0)\n",
    "    plt.close(fig)\n",
    "    return buf\n",
    "\n",
    "\n",
    "def log_losses(epoch, plot_buf):\n",
    "    image = tf.image.decode_png(plot_buf.getvalue(), channels=4)\n",
    "    image = tf.expand_dims(image, 0)\n",
    "\n",
    "    with TRAIN_SUMMARY_WRITER.as_default():\n",
    "        tf.summary.scalar(\"Generator Loss\", generator_metric.result(), step=epoch)\n",
    "\n",
    "        tf.summary.scalar(\"Local D loss\", local_discriminator_metric.result(), step=epoch)\n",
    "        tf.summary.scalar(\"Gobal D loss\", global_discriminator_metric.result(), step=epoch)\n",
    "        \n",
    "        tf.summary.scalar(\"Mode Colapsing ?\", similarity_metric.result(), step=epoch)\n",
    "        tf.summary.scalar(\"Correlation Metric\", correlation_metric.result(), step=epoch)\n",
    "\n",
    "        tf.summary.image(\"Training Generations\", image, step=epoch)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_plots(noise, save_to):\n",
    "    generated = generate(seed)\n",
    "\n",
    "    fig =plt.figure(figsize=(18, 5))\n",
    "    plt.title(\"Generation of the GAN during Training.\")\n",
    "    for i in range(generated.shape[-1]):\n",
    "        plt.plot(generated[0, :, i], label=f'feat {i+1}')\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "\n",
    "    plt.savefig(save_to)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "def local_discriminator_loss(crits_on_real, crits_on_fake):\n",
    "    individual_losses = []\n",
    "    preds_shape = crits_on_real.shape[1:]\n",
    "\n",
    "    for i in range(crits_on_real.shape[0]):\n",
    "        l1 = bc(tf.zeros(preds_shape), crits_on_fake[i])\n",
    "        l2 = bc(tf.ones(preds_shape), crits_on_real[i])\n",
    "        loss = (l1+ l2)/2\n",
    "        individual_losses.append(loss)\n",
    "    return individual_losses\n",
    "\n",
    "\n",
    "def local_generator_loss(crit_on_fake):\n",
    "    individual_losses = []\n",
    "    preds_shape = crit_on_fake.shape[1:]\n",
    "\n",
    "    for i in range(crit_on_fake.shape[0]):\n",
    "        individual_losses.append(bc(tf.ones(preds_shape), crit_on_fake[i]))\n",
    "        \n",
    "    return individual_losses\n",
    "\n",
    "def global_discriminator_loss(crit_on_real, crit_on_fake):\n",
    "    l1 = bc(tf.zeros_like(crit_on_fake), crit_on_fake)\n",
    "    l2 = bc(tf.ones_like(crit_on_real), crit_on_real)\n",
    "\n",
    "    loss = (l1+ l2)/2\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def global_generator_loss(crit_on_fake):\n",
    "    loss = bc(tf.ones_like(crit_on_fake), crit_on_fake)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(real_ts, update_discr:bool=False):\n",
    "    alpha=1\n",
    "    noise= wiener_process(BS, N_SAMPLE_WIENER, FEAT_WIENER)\n",
    "\n",
    "    # print(real_ts.shape, tf.reshape(real_ts[:, :, 0], (BS, SEQUENCE_LENGTH, 1)))\n",
    "    if update_discr == True:\n",
    "        with tf.GradientTape(persistent=True) as d_tape:\n",
    "            generated_ts = [g(noise, training=False) for g in generators]\n",
    "\n",
    "            local_crit_gen = tf.convert_to_tensor([local_discriminators[i](generated_ts[i], training=True) for i in range(len(local_discriminators))])\n",
    "            local_crit_real = tf.convert_to_tensor([local_discriminators[i](real_ts[:, :, i], training=True) for i in range(len(local_discriminators))])\n",
    "\n",
    "            local_d_losses = local_discriminator_loss(local_crit_real, local_crit_gen)\n",
    "\n",
    "            # Reshape the generated sequences for the global dicriminator.\n",
    "            generated_reshaped = tf.stack(generated_ts, -1)\n",
    "            generated_reshaped = tf.reshape(generated_ts, (BS, SEQUENCE_LENGTH, generated_reshaped.shape[-1]))\n",
    "\n",
    "            g_crit_gen = global_discriminator(generated_reshaped, training=True)\n",
    "            g_crit_real= global_discriminator(real_ts, training=True)\n",
    "\n",
    "            global_d_loss = global_discriminator_loss(g_crit_real, g_crit_gen)\n",
    "\n",
    "    with tf.GradientTape(persistent=True) as g_tape:\n",
    "        generated_ts = [g(noise, training=True) for g in generators]\n",
    "\n",
    "        local_crit_gen = tf.convert_to_tensor([local_discriminators[i](generated_ts[i], training=False) for i in range(len(local_discriminators))])\n",
    "\n",
    "        # Reshape the generated sequences for the global dicriminator.\n",
    "        generated_reshaped = tf.stack(generated_ts, -1)\n",
    "        generated_reshaped = tf.reshape(generated_ts, (BS, SEQUENCE_LENGTH, generated_reshaped.shape[-1]))\n",
    "\n",
    "        g_crit_gen = global_discriminator(generated_reshaped, training=False)\n",
    "\n",
    "        extracted_features = early_predictor(generated_reshaped, training=False)\n",
    "\n",
    "        local_g_loss = local_generator_loss(local_crit_gen)\n",
    "        global_g_loss= global_generator_loss(g_crit_gen)\n",
    "        s_loss = similarity_loss(extracted_features)\n",
    "\n",
    "        g_loss = [LAMBDA_LOCAL* l_loss + LAMBDA_GLOBAL* global_g_loss+ alpha* s_loss for l_loss in local_g_loss]\n",
    "\n",
    "    if update_discr == True:\n",
    "        # Compute the gradients and update the weight for ...\n",
    "        # Global Discriminator...\n",
    "        global_discr_gradient = d_tape.gradient(global_d_loss, global_discriminator.trainable_variables)\n",
    "        global_discriminator_opt.apply_gradients(zip(global_discr_gradient, global_discriminator.trainable_variables)) \n",
    "\n",
    "        #Local Discriminator...\n",
    "        for i in range(len(local_discriminators)):   \n",
    "            grads = d_tape.gradient(local_d_losses[i], local_discriminators[i].trainable_variables)\n",
    "            discriminator_opt[i].apply_gradients(zip(grads, local_discriminators[i].trainable_variables))\n",
    "\n",
    "    # And Local Generators !\n",
    "    for i in range(len(generators)):   \n",
    "        # print(i)\n",
    "        grads = g_tape.gradient(g_loss[i], generators[i].trainable_variables)\n",
    "        generator_opt[i].apply_gradients(zip(grads, generators[i].trainable_variables))\n",
    "\n",
    "    # Save metric for display\n",
    "    if update_discr == True:\n",
    "        local_discriminator_metric(tf.reduce_mean(local_d_losses))\n",
    "        global_discriminator_metric(global_d_loss)\n",
    "        \n",
    "    generator_metric(tf.reduce_mean(local_g_loss))\n",
    "    similarity_metric(s_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, epochs):\n",
    "  losses = []\n",
    "  total_steps = \"?\"\n",
    "  for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "\n",
    "    generator_metric.reset_states()\n",
    "    global_discriminator_metric.reset_states()\n",
    "    local_discriminator_metric.reset_states()\n",
    "\n",
    "    for s, image_batch in enumerate(dataset):\n",
    "      update_discriminator = s % UPDATE_DISCRIMINATOR == 0\n",
    "      train_step(image_batch, update_discr=update_discriminator)\n",
    "\n",
    "      print(f\"\\r e {epoch}/{epochs}, s {s}/{total_steps}: Gen {generator_metric.result():0.4f}; Global discriminator: {global_discriminator_metric.result():0.4f}; Local discriminator: {local_discriminator_metric.result():0.4f}; Sim loss: {similarity_metric.result():0.4f}\", end=\"\")\n",
    "\n",
    "    if epoch == 0:\n",
    "      total_steps = s\n",
    "\n",
    "    stop = time.time()\n",
    "    print()\n",
    "    print(f\"\\r[+] Epoch {epoch}/{epochs} in {(stop-start):0.4f} seconds. ({(stop-start)/total_steps:0.4f} s/step)\")\n",
    "\n",
    "    generate_plots(seed, f\"imgs/GAN_generations/{epoch}.png\")\n",
    "    # Make generations on seed\n",
    "    seed_generation = generate(seed, training=False)\n",
    "    buff = plot_to_buff(seed_generation)\n",
    "\n",
    "    batch_signature = signature_on_batch(seed_generation, [0, 1], [2, 3, 4, 5], config.met_params.signature_length)\n",
    "    signature_difference = signature_metric(real_batch_signature, batch_signature)\n",
    "    correlation_metric(signature_difference)\n",
    "    \n",
    "    l = [generator_metric.result(), global_discriminator_metric.result(), local_discriminator_metric.result()]\n",
    "    losses.append(l)\n",
    "    log_losses(epoch, buff)\n",
    "\n",
    "  return np.array(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_losses = train(dset_simulated, EPOCHS)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 5))\n",
    "plt.title(\"Training Losses.\")\n",
    "plt.plot(training_losses[:, 0], \".-\", label=\"Generator Loss\")\n",
    "plt.plot(training_losses[:, 1], \".-\", label=\"Discriminator Loss\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's Test it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "after_training_generations = generator(seed, training=False)\n",
    "\n",
    "plt.figure(figsize=(18, 5))\n",
    "plt.title(\"Generation of the GAN whitout Training.\")\n",
    "for i in range(after_training_generations.shape[-1]):\n",
    "    plt.plot(after_training_generations[0, :, i], label=f'feat {i+1}')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "after_training_decision = discriminator(after_training_generations)\n",
    "after_training_decision[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test if Mode Colapsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after_training_generations\n",
    "def plot_several_generations(generations:np.ndarray, nvertical:int=3, nhoriz:int=3):\n",
    "\n",
    "    legend = [f\"feat {j}\" for j in range(generations.shape[-1])]\n",
    "\n",
    "    plt.figure(figsize=(18, 10))\n",
    "    plt.suptitle(\"Generations After GAN Training.\")\n",
    "\n",
    "    for i in range(nvertical* nhoriz):\n",
    "        ax = plt.subplot(nvertical, nhoriz, i+ 1)\n",
    "        ax.set_title(f\"sequence {i+1}\")\n",
    "\n",
    "        plt.plot(generations[i])\n",
    "        ax.grid(True)\n",
    "        plt.legend(legend)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_several_generations(after_training_generations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
