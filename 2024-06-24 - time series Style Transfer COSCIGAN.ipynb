{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Style Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from configs.SimulatedData import Proposed\n",
    "from dataset.tf_pipeline import make_train_valid_dset\n",
    "from datetime import datetime\n",
    "import io\n",
    "import json\n",
    "import os\n",
    "from utils.metric import signature_on_batch, signature_metric\n",
    "import mlflow\n",
    "from sklearn.decomposition import PCA\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs.\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized.\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Proposed()\n",
    "date_str = datetime.now().strftime('%Y-%m-%d_%H_%M_%S')\n",
    "\n",
    "CONTENT_DATASET_PATH = \"data/simulated_dataset/01 - Source Domain.h5\"\n",
    "STYLE_DATASET_PATHS = [\"data/simulated_dataset/output_noise/0.25.h5\", \"data/simulated_dataset/output_noise/0.75.h5\"]\n",
    "\n",
    "EXPERIMENT_NAME = f\"{date_str} - Style Transfer Algorithm\"\n",
    "SAVE_FOLDER = f\"experiments_logs/{EXPERIMENT_NAME}\"\n",
    "\n",
    "SEQUENCE_LENGTH = config.sequence_lenght_in_sample\n",
    "GRANUARITY = config.granularity\n",
    "OVERLAP = config.overlap\n",
    "BS = 20 # config.batch_size\n",
    "VALID_BATCH_SIZE = 50\n",
    "EPOCHS = 50 # config.epochs\n",
    "NUM_SEQUENCE_TO_GENERATE = config.met_params.sequence_to_generate\n",
    "N_SIGNAL= 7\n",
    "N_CLASSES = 2\n",
    "\n",
    "STYLE_VECTOR_SIZE = 16\n",
    "FEAT_WIENER = 2\n",
    "N_SAMPLE_WIENER = SEQUENCE_LENGTH//4\n",
    "NOISE_DIM= (N_SAMPLE_WIENER, FEAT_WIENER)\n",
    "N_VALIDATION_SEQUENCE = 500\n",
    "DISCR_STEP = 5\n",
    "DISCR_THREASOLD = 0.9\n",
    "\n",
    "L_STYLE_GENERATION= 0.01\n",
    "L_RECONSTR= 0.5\n",
    "\n",
    "L_CONTENT= 1\n",
    "L_DIS= 1\n",
    "TRIPLET_R = 1\n",
    "L_TRIPLET= 1\n",
    "L_REALNESS= 0.05\n",
    "L_ADV=  0.05\n",
    "\n",
    "L_LOCAL = 0.1\n",
    "L_GLOBAL = 0.1\n",
    "\n",
    "REDUCE_TRAIN_SET = False\n",
    "\n",
    "BASE_DIR = f\"logs - final algo/{EXPERIMENT_NAME}\"\n",
    "TRAIN_LOGS_DIR_PATH = f\"{BASE_DIR}/train\"\n",
    "VALID_LOGS_DIR_PATH = f\"{BASE_DIR}/valid\"\n",
    "GENERATION_LOG = f\"{BASE_DIR}/Generations\"\n",
    "os.makedirs(GENERATION_LOG)\n",
    "os.makedirs(SAVE_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# server_ip = \"192.168.210.102\"\n",
    "# mlflow_port_number= \"5001\"\n",
    "\n",
    "# mlflow.set_tracking_uri(f'http://{server_ip}:{mlflow_port_number}') \n",
    "# exp = mlflow.get_experiment_by_name(\"Style Transfer Algorithm\")\n",
    "\n",
    "# run = mlflow.start_run(run_name=date_str) \n",
    "# mlflow.tensorflow.autolog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_configuration():\n",
    "    d = {\n",
    "        \"content_dataset\":CONTENT_DATASET_PATH,\n",
    "        \"style_datasets\":STYLE_DATASET_PATHS,\n",
    "        \"SEQUENCE_LENGTH\":SEQUENCE_LENGTH,\n",
    "        \"granularity\":GRANUARITY,\n",
    "        \"overlap\":OVERLAP,\n",
    "        \"epochs\":EPOCHS,\n",
    "        \"batch_size\":BS,\n",
    "        \"style_vector_size\":STYLE_VECTOR_SIZE,\n",
    "        \"feat_wiener\":FEAT_WIENER,\n",
    "        \"n_sample_wiener\":N_SAMPLE_WIENER,\n",
    "        \"triplet_r\":TRIPLET_R,\n",
    "        \"n_validation_sequence\":N_VALIDATION_SEQUENCE,\n",
    "        \"l_style\":L_STYLE_GENERATION,\n",
    "        \"l_content\":L_CONTENT,\n",
    "        \"l_triplet\":L_TRIPLET,\n",
    "        \"l_realness\":L_REALNESS,\n",
    "        \"l_adv\":L_ADV,\n",
    "        \"l_reconstr\":L_RECONSTR,    \n",
    "        \"log_dir\":BASE_DIR\n",
    "    }\n",
    "\n",
    "    json_object = json.dumps(d)\n",
    "    mlflow.log_params({\n",
    "        \"SEQUENCE_LENGTH\":SEQUENCE_LENGTH,\n",
    "        \"granularity\":GRANUARITY,\n",
    "        \"overlap\":OVERLAP,\n",
    "        \"epochs\":EPOCHS,\n",
    "        \"batch_size\":BS,\n",
    "        \"style_vector_size\":STYLE_VECTOR_SIZE,\n",
    "        \"feat_wiener\":FEAT_WIENER,\n",
    "        \"n_sample_wiener\":N_SAMPLE_WIENER,\n",
    "        \"triplet_r\":TRIPLET_R,\n",
    "        \"l_style\":L_STYLE_GENERATION,\n",
    "        \"l_content\":L_CONTENT,\n",
    "        \"l_triplet\":L_TRIPLET,\n",
    "        \"l_realness\":L_REALNESS,\n",
    "        \"l_adv\":L_ADV,\n",
    "        \"l_reconstr\":L_RECONSTR,\n",
    "    })\n",
    "\n",
    "    with open(f\"{SAVE_FOLDER}/parameters.json\", \"w\") as outfile:\n",
    "        outfile.write(json_object)\n",
    "\n",
    "\n",
    "save_configuration()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_valid_split(df, train_size:float=.7):\n",
    "    dset_size = df.shape[0]\n",
    "    train_index = int(dset_size* train_size)\n",
    "\n",
    "    train_split = df.loc[:train_index]\n",
    "    valid_split = df.loc[train_index:]\n",
    "\n",
    "    return train_split, valid_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pd_to_tf_dset(df_path:str, train_batch_size:int=BS, valid_batch_size:int=VALID_BATCH_SIZE):\n",
    "    _df= pd.read_hdf(df_path).astype(np.float32)\n",
    "    _df = _df.drop(columns=['labels'])\n",
    "\n",
    "    content_train, content_valid = make_train_valid_dset(\n",
    "        _df, \n",
    "        SEQUENCE_LENGTH, \n",
    "        GRANUARITY, \n",
    "        int(OVERLAP* SEQUENCE_LENGTH),\n",
    "        train_batch_size,\n",
    "        valid_batch_size,\n",
    "        reduce_train_set=REDUCE_TRAIN_SET\n",
    "    )\n",
    "\n",
    "    return content_train, content_valid\n",
    "\n",
    "\n",
    "def make_style_dataset(train_batch_size:int=BS, valid_batch_size:int=VALID_BATCH_SIZE):\n",
    "    style_train_datasets, style_valid_datasets = [], []\n",
    "\n",
    "    for s_i, style_path in enumerate(STYLE_DATASET_PATHS):\n",
    "        dset_style_train, dset_style_valid = pd_to_tf_dset(style_path, train_batch_size)\n",
    "\n",
    "        dset_style_train = dset_style_train.map(lambda batch: (batch, tf.zeros(batch.shape[0])+ s_i)).cache()\n",
    "        dset_style_valid = dset_style_valid.map(lambda batch: (batch, tf.zeros(batch.shape[0])+ s_i)).cache()\n",
    "\n",
    "        dset_style_train = dset_style_train.unbatch()\n",
    "        dset_style_valid = dset_style_valid.unbatch()\n",
    "    \n",
    "        style_train_datasets.append(dset_style_train)\n",
    "        style_valid_datasets.append(dset_style_valid)\n",
    "\n",
    "    style_dset_train = tf.data.Dataset.sample_from_datasets(style_train_datasets).batch(train_batch_size, drop_remainder=True)\n",
    "    style_dset_valid = tf.data.Dataset.sample_from_datasets(style_valid_datasets).batch(valid_batch_size, drop_remainder=True)\n",
    "\n",
    "    return style_dset_train, style_dset_valid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_dset_train, content_dset_valid = pd_to_tf_dset(CONTENT_DATASET_PATH, BS*2, VALID_BATCH_SIZE*2)\n",
    "style1_dset_train, style1_dset_valid = pd_to_tf_dset(STYLE_DATASET_PATHS[0], BS, VALID_BATCH_SIZE)\n",
    "style2_dset_train, style2_dset_valid = pd_to_tf_dset(STYLE_DATASET_PATHS[1], BS, VALID_BATCH_SIZE)\n",
    "# style_dset_train, style_dset_valid = make_style_dataset(BS, VALID_BATCH_SIZE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Content Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define AdaIN Layers for Time Series\n",
    "class AdaIN(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(AdaIN, self).__init__()\n",
    "\n",
    "    def get_mean_std(self, x, eps=1e-5):\n",
    "        _mean, _variance = tf.nn.moments(x, axes=[1], keepdims=True)\n",
    "        standard_dev = tf.sqrt(_variance+ eps)\n",
    "        return _mean, standard_dev\n",
    "\n",
    "    def call(self, content_input, style_input):\n",
    "        # print(content_input.shape, style_input.shape)\n",
    "        content_mean, content_std = self.get_mean_std(content_input)\n",
    "        style_mean, style_std = self.get_mean_std(style_input)\n",
    "        adain_res =style_std* (content_input - content_mean) / content_std+ style_mean\n",
    "        return adain_res"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COSCIGAN Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_part(content_input, n_sample_wiener:int, feat_wiener:int, style_input):\n",
    "    init = tf.keras.initializers.RandomNormal()\n",
    "\n",
    "    # Make a small projection...\n",
    "    _content_input = tf.keras.layers.Flatten()(content_input)\n",
    "    _content_input = tf.keras.layers.Dense(n_sample_wiener* feat_wiener, kernel_initializer=init)(_content_input)\n",
    "    _content_input = tf.keras.layers.Reshape((n_sample_wiener, feat_wiener))(_content_input)\n",
    "\n",
    "    # Make the style input \n",
    "    _style_input = tf.keras.layers.Dense(16)(style_input)\n",
    "    _style_input = tf.keras.layers.Reshape((16, 1))(_style_input)\n",
    "\n",
    "    _stage2_style_input = tf.keras.layers.Dense(32)(style_input)\n",
    "    _stage2_style_input = tf.keras.layers.Reshape((32, 1))(_stage2_style_input)\n",
    "\n",
    "    x = AdaIN()(_content_input, _style_input)\n",
    "\n",
    "    x = tf.keras.layers.Conv1DTranspose(16, 5, 1, padding='same', kernel_initializer=init)(x)\n",
    "    x = tf.keras.layers.LeakyReLU()(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv1DTranspose(16, 5, 2, padding='same', kernel_initializer=init)(x)\n",
    "    x = tf.keras.layers.LeakyReLU()(x)\n",
    "\n",
    "    x = AdaIN()(x, _stage2_style_input)\n",
    "    x = tf.keras.layers.Conv1DTranspose(62, 5, 1, padding='same', kernel_initializer=init)(x)\n",
    "    x = tf.keras.layers.LeakyReLU()(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv1DTranspose(1, 5, 2, padding='same', kernel_initializer=init)(x)\n",
    "    x = tf.keras.layers.LeakyReLU()(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "def make_generator(n_sample_wiener:int, feat_wiener:int, style_vector_size:int, n_generators:int):\n",
    "    \n",
    "    input = tf.keras.Input((n_sample_wiener, feat_wiener), name=f\"Content_Input\")\n",
    "    style_input = tf.keras.Input((style_vector_size,), name=\"Style_Input\") \n",
    "    gens_outputs = []\n",
    "\n",
    "    for _ in range(n_generators):\n",
    "        gens_outputs.append(generator_part(input, n_sample_wiener, feat_wiener, style_input))\n",
    "\n",
    "    test = tf.keras.layers.concatenate(gens_outputs, axis=-1)\n",
    "\n",
    "    model = tf.keras.Model([input, style_input], test)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_global_discriminator(seq_length:int, n_signals:int, n_classes:int):\n",
    "    _input = tf.keras.Input((seq_length, n_signals))\n",
    "\n",
    "    x = tf.keras.layers.Conv1D(32, 5, 2, padding='same')(_input)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv1D(32, 5, 2, padding='same')(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "\n",
    "    x = layers.Flatten()(x)\n",
    "\n",
    "    _output = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    _class_output = layers.Dense(n_classes, activation=\"sigmoid\")(x)\n",
    "\n",
    "    model = tf.keras.Model(_input, [_output, _class_output], name=\"global_discriminator\")\n",
    "    early_predictor = tf.keras.Model(_input, x, name=\"early_discriminator\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def local_discriminator_part(_input, n_classes:int):\n",
    "\n",
    "    x = tf.keras.layers.Conv1D(16, 5, 2, padding='same')(_input)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "\n",
    "    x = layers.Flatten()(x)\n",
    "    stage1_dropouted = layers.Dropout(0.25)(x)\n",
    "\n",
    "    _output = layers.Dense(1, activation=\"sigmoid\")(stage1_dropouted)\n",
    "    # _class_output = layers.Dense(n_classes, activation=\"sigmoid\")(stage1_dropouted)\n",
    "\n",
    "    return _output\n",
    "\n",
    "\n",
    "def create_local_discriminator(n_signals:int, sequence_length:int, n_styles:int):\n",
    "    sig_inputs = tf.keras.Input((sequence_length, n_signals))\n",
    "    splited_inputs = tf.split(sig_inputs, n_signals, axis=-1)\n",
    "\n",
    "    crit_outputs = []\n",
    "\n",
    "    for sig_input in splited_inputs:\n",
    "        crit_output = local_discriminator_part(sig_input, n_styles)\n",
    "        crit_outputs.append(crit_output)\n",
    "\n",
    "    crit_outputs = tf.keras.layers.concatenate(crit_outputs, axis=-1, name=\"crit_output\")\n",
    "\n",
    "    return tf.keras.Model(sig_inputs, crit_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_content_encoder(seq_length:int, n_feat:int, feat_wiener:int):\n",
    "    init = tf.keras.initializers.RandomNormal(seed=42)\n",
    "\n",
    "    _input = tf.keras.Input((seq_length, n_feat))\n",
    "\n",
    "    x = tf.keras.layers.Conv1D(64, 5, 2, padding='same', kernel_initializer=init)(_input)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.LeakyReLU()(x)\n",
    "        \n",
    "    x = tf.keras.layers.Conv1D(128, 5, 1, padding='same', kernel_initializer=init)(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.LeakyReLU()(x)\n",
    "    \n",
    "    x = tf.keras.layers.Conv1D(128, 5, 2, padding='same', kernel_initializer=init)(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.LeakyReLU()(x)\n",
    "    \n",
    "    x = tf.keras.layers.Conv1D(feat_wiener, 5, 1, padding='same', kernel_initializer=init, activation=\"linear\")(x)\n",
    "\n",
    "    model = tf.keras.Model(_input, x)\n",
    "    return model\n",
    "\n",
    "def make_style_encoder(seq_length:int, n_feat:int, vector_output_shape:int):\n",
    "    init = tf.keras.initializers.RandomNormal(seed=42)\n",
    "\n",
    "    _input = tf.keras.Input((seq_length, n_feat))\n",
    "\n",
    "    x = tf.keras.layers.Conv1D(64, 5, 2, padding='same', kernel_initializer=init)(_input) \n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.LeakyReLU()(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv1D(128, 5, 1, padding='same', kernel_initializer=init)(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.LeakyReLU()(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv1D(128, 5, 2, padding='same', kernel_initializer=init)(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.LeakyReLU()(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv1D(256, 5, 1, padding='same', kernel_initializer=init)(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.LeakyReLU()(x)\n",
    "\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dense(128, activation=None)(x)\n",
    "    x = tf.keras.layers.LeakyReLU()(x)\n",
    "    x = tf.keras.layers.Dense(vector_output_shape, activation=\"linear\")(x)\n",
    "\n",
    "    model = tf.keras.Model(_input, x)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = make_generator(N_SAMPLE_WIENER, FEAT_WIENER, STYLE_VECTOR_SIZE, N_SIGNAL)\n",
    "# tf.keras.utils.plot_model(decoder, show_shapes=True, to_file='decoder.png')\n",
    "decoder.count_params()\n",
    "tf.keras.utils.plot_model(decoder, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_encoder = make_content_encoder(SEQUENCE_LENGTH, N_SIGNAL, FEAT_WIENER)\n",
    "# tf.keras.utils.plot_model(content_encoder, show_shapes=True, to_file='content_encoder.png')\n",
    "# content_encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_encoder = make_style_encoder(SEQUENCE_LENGTH, N_SIGNAL, STYLE_VECTOR_SIZE)\n",
    "# tf.keras.utils.plot_model(style_encoder, show_shapes=True, to_file='style_encoder.png')\n",
    "# style_encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_discriminator = make_global_discriminator(SEQUENCE_LENGTH, N_SIGNAL, N_CLASSES)\n",
    "tf.keras.utils.plot_model(global_discriminator, show_shapes=True, to_file='global_discriminator.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_discriminator = create_local_discriminator(N_SIGNAL, SEQUENCE_LENGTH, N_CLASSES)\n",
    "tf.keras.utils.plot_model(local_discriminator, show_shapes=True, to_file='local_discriminator.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(content_batch, style_batch):\n",
    "    content = content_encoder(content_batch, training=False)\n",
    "    style = style_encoder(style_batch, training=False)\n",
    "    generated = decoder([content, style], training=False)\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(dset, n_batches):\n",
    "    _arr = np.array([c for c in dset.take(n_batches)])\n",
    "    return _arr.reshape((-1, _arr.shape[-2], _arr.shape[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_seed_content = next(iter(content_dset_valid))\n",
    "valid_seed_style = next(iter(style1_dset_valid))\n",
    "\n",
    "train_seed_content = next(iter(content_dset_train))\n",
    "train_seed_style = next(iter(style1_dset_train))\n",
    "\n",
    "generated_sequence = generate(valid_seed_content[:int(VALID_BATCH_SIZE)], valid_seed_style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_prediction(x, threashold=.5):\n",
    "    return tf.cast(x > threashold, tf.float32)* x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make seed sequence for particular style.\n",
    "\n",
    "# train_dset_s1, valid_dset_s1 = pd_to_tf_dset(STYLE_DATASET_PATHS[0])\n",
    "# train_dset_s2, valid_dset_s2 = pd_to_tf_dset(STYLE_DATASET_PATHS[1])\n",
    "\n",
    "# content_dset_train, content_dset_valid\n",
    "# style1_dset_train, style1_dset_valid\n",
    "# style2_dset_train, style2_dset_valid\n",
    "\n",
    "seed_content_train = get_batches(content_dset_train, 25)\n",
    "seed_content_valid = get_batches(content_dset_valid, 25)\n",
    "\n",
    "seed_style1_train = get_batches(style1_dset_train, 50)\n",
    "seed_style1_valid = get_batches(style1_dset_valid, 50)\n",
    "\n",
    "seed_style2_train = get_batches(style2_dset_train, 50)\n",
    "seed_style2_valid = get_batches(style2_dset_valid, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_generated_sequence2(content_sequences, style1_sequences, style2_sequences, show=False):\n",
    "\n",
    "    # Make Generated sequence for visualization.\n",
    "    content_of_content = content_encoder(content_sequences, training=False)\n",
    "    style_of_style1= style_encoder(style1_sequences, training=False)\n",
    "    style1_generated = decoder([content_of_content, style_of_style1], training=False)\n",
    "\n",
    "    style_of_style2 = style_encoder(style2_sequences, training=False)\n",
    "    style2_generated = decoder([content_of_content, style_of_style2], training=False)\n",
    "\n",
    "    c_style1_generated = content_encoder(style1_generated, training=False)\n",
    "    s_style1_generated = style_encoder(style1_generated, training=False)\n",
    "\n",
    "    c_style2_generated = content_encoder(style2_generated, training=False)\n",
    "    s_style2_generated = style_encoder(style2_generated, training=False)\n",
    "\n",
    "    style1_signature = signature_on_batch(style1_sequences, config.met_params.ins, config.met_params.outs, config.met_params.signature_length)\n",
    "    style2_signature = signature_on_batch(style2_sequences, config.met_params.ins, config.met_params.outs, config.met_params.signature_length)\n",
    "    gen_s1_signature = signature_on_batch(style1_generated, config.met_params.ins, config.met_params.outs, config.met_params.signature_length)\n",
    "    gen_s2_signature = signature_on_batch(style2_generated, config.met_params.ins, config.met_params.outs, config.met_params.signature_length)\n",
    "\n",
    "    x = np.arange(style1_signature.shape[1]) - style1_signature.shape[1]/2\n",
    "\n",
    "    averaged_rs1_sig = np.mean(style1_signature, axis=0)\n",
    "    averaged_rs2_sig = np.mean(style2_signature, axis=0)\n",
    "    averaged_gs1_sig = np.mean(gen_s1_signature, axis=0)\n",
    "    averaged_gs2_sig = np.mean(gen_s2_signature, axis=0)\n",
    "\n",
    "    sig_mins = np.min([averaged_rs1_sig, averaged_rs2_sig, averaged_gs1_sig, averaged_gs2_sig])- 5\n",
    "    sig_max = np.max([averaged_rs1_sig, averaged_rs2_sig, averaged_gs1_sig, averaged_gs2_sig]) +5\n",
    "\n",
    "    \n",
    "    # Reduce the Style Vector for visualization purposes.\n",
    "    pca = PCA(2)\n",
    "    style_vectors = np.vstack(\n",
    "        [   style_of_style1, \n",
    "            style_of_style2, \n",
    "            s_style1_generated, \n",
    "            s_style2_generated\n",
    "        ])\n",
    "\n",
    "    pca.fit(style_vectors)\n",
    "\n",
    "    reduced_style1 = pca.transform(style_of_style1)\n",
    "    reduced_style2 = pca.transform(style_of_style2)\n",
    "    reduced_style1_generated = pca.transform(s_style1_generated)\n",
    "    reduced_style2_generated = pca.transform(s_style2_generated)\n",
    "\n",
    "\n",
    "    all_values = np.array([content_sequences, style1_sequences, style2_sequences])\n",
    "    _min, _max = np.min(all_values)-1, np.max(all_values)+ 1\n",
    "\n",
    "    fig= plt.figure(figsize=(18, 8))\n",
    "    spec= fig.add_gridspec(3, 8)\n",
    "\n",
    "    ax00 = fig.add_subplot(spec[0:2, :2])\n",
    "    ax00.set_title('Content Sequence. ($C_0$)')\n",
    "    ax00.plot(content_sequences[0])\n",
    "    ax00.set_ylim(_min, _max)\n",
    "    ax00.grid(True)\n",
    "    ax00.legend()\n",
    "\n",
    "# #######\n",
    "    ax01 = fig.add_subplot(spec[0, 2:4])\n",
    "    ax01.set_title('Style Sequence 1. ($S_0$)')\n",
    "    ax01.plot(style1_sequences[0])\n",
    "    ax01.set_ylim(_min, _max)\n",
    "    ax01.grid(True)\n",
    "\n",
    "    ax11 = fig.add_subplot(spec[1, 2:4])\n",
    "    ax11.set_title(\"Style Sequence 2. ($S_1$)\")\n",
    "    ax11.plot(style2_sequences[0])\n",
    "    ax11.set_ylim(_min, _max)\n",
    "    ax11.grid(True)\n",
    "\n",
    "# #######\n",
    "    ax02 = fig.add_subplot(spec[0, 4:6])\n",
    "    ax02.set_title('Generated Sequence. ($C_0; S_0$)')\n",
    "    ax02.plot(style1_generated[0])\n",
    "    ax02.set_ylim(_min, _max)\n",
    "    ax02.grid(True) \n",
    "\n",
    "    ax12 = fig.add_subplot(spec[1, 4:6])\n",
    "    ax12.set_title('Generated Sequence. ($C_0; S_1$)')\n",
    "    ax12.plot(style2_generated[1])\n",
    "    ax12.set_ylim(_min, _max)\n",
    "    ax12.grid(True) \n",
    "\n",
    "# #####\n",
    "\n",
    "    ax03 = fig.add_subplot(spec[0, 6:])\n",
    "    ax03.set_title(\"Signature Style 1.\")\n",
    "\n",
    "\n",
    "    plt.plot(x, averaged_rs1_sig[:, 0], \"g\")\n",
    "    plt.plot(x, averaged_rs1_sig[:, 1], \"g\")\n",
    "    plt.plot(x, averaged_rs1_sig[:, 2], \"g\")\n",
    "\n",
    "    plt.plot(x, averaged_gs1_sig[:, 0], \"b\")\n",
    "    plt.plot(x, averaged_gs1_sig[:, 1], \"b\")\n",
    "    plt.plot(x, averaged_gs1_sig[:, 2], \"b\")\n",
    "\n",
    "\n",
    "    plt.fill_between(x, averaged_rs1_sig[:, 0].reshape((-1,)), averaged_rs1_sig[:, 1].reshape((-1,)), color=\"g\", alpha=0.25, label=\"Signature From Real data, Style 1\")\n",
    "    plt.fill_between(x, averaged_gs1_sig[:, 0].reshape((-1,)), averaged_gs1_sig[:, 1].reshape((-1,)), color=\"b\", alpha=0.25, label=\"Signature From Sim  data, Style 1\")\n",
    "\n",
    "    ax03.set_ylim(-1, 1)\n",
    "    ax03.grid(True)\n",
    "    ax03.legend()\n",
    "    \n",
    "\n",
    "    ax04 = fig.add_subplot(spec[1, 6:])\n",
    "    ax04.set_title(\"Signature Style 2.\")\n",
    "\n",
    "    plt.plot(x, averaged_rs2_sig[:, 0], \"g\")\n",
    "    plt.plot(x, averaged_rs2_sig[:, 1], \"g\")\n",
    "    plt.plot(x, averaged_rs2_sig[:, 2], \"g\")\n",
    "\n",
    "    plt.plot(x, averaged_gs2_sig[:, 0], \"b\")\n",
    "    plt.plot(x, averaged_gs2_sig[:, 1], \"b\")\n",
    "    plt.plot(x, averaged_gs2_sig[:, 2], \"b\")\n",
    "\n",
    "    plt.fill_between(x, averaged_rs2_sig[:, 0].reshape((-1,)), averaged_rs2_sig[:, 1].reshape((-1,)), color=\"g\", alpha=0.25, label=\"Signature From Real data, Style 2\")\n",
    "    plt.fill_between(x, averaged_gs2_sig[:, 0].reshape((-1,)), averaged_gs2_sig[:, 1].reshape((-1,)), color=\"b\", alpha=0.25, label=\"Signature From Sim  data, Style 2\")\n",
    "\n",
    "    ax04.grid(True)\n",
    "    ax04.set_ylim(-1, 1)\n",
    "    ax04.legend()\n",
    "\n",
    "\n",
    "# #####\n",
    "    ax10 = fig.add_subplot(spec[2, :3])\n",
    "    ax10.set_title('Content Space.')\n",
    "    ax10.scatter(content_of_content[0, :, 0], content_of_content[0, :, 1],  label='Content of content.')\n",
    "    ax10.scatter(c_style1_generated[0, :, 0], c_style1_generated[0, :, 1], label='Content of Generated style 1')\n",
    "    ax10.scatter(c_style2_generated[0, :, 0], c_style2_generated[0, :, 1],  label='Content of Generated style 2')\n",
    "    ax10.grid(True)\n",
    "    ax10.legend()\n",
    "\n",
    "    ax11 = fig.add_subplot(spec[2, 3:])\n",
    "    ax11.set_title('Style Space, Reduced with PCA.')\n",
    "    ax11.scatter(reduced_style1[:, 0], reduced_style1[:, 1], label='Style 1.', alpha=0.25)\n",
    "    ax11.scatter(reduced_style2[:, 0], reduced_style2[:, 1], label='Style 2.', alpha=0.25)\n",
    "    ax11.scatter(reduced_style1_generated[:, 0], reduced_style1_generated[:, 1], label='Generations style 1.', alpha=0.25)\n",
    "    ax11.scatter(reduced_style2_generated[:, 0], reduced_style2_generated[:, 1], label='Generations style 2.', alpha=0.25)\n",
    "\n",
    "    ax11.grid(True)\n",
    "    ax11.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    return fig\n",
    "\n",
    "plot_generated_sequence2(seed_content_valid, seed_style1_valid, seed_style2_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fig_to_buff(fig):\n",
    "    buf = io.BytesIO()\n",
    "    fig.savefig(buf, format='png')\n",
    "    buf.seek(0)\n",
    "    plt.close(fig)\n",
    "    return buf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "error_classif = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "def recontruction_loss(true:tf.Tensor, generated:tf.Tensor):\n",
    "    diff = generated- true\n",
    "    result = tf.math.reduce_mean(tf.square(diff))\n",
    "    return result\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "\n",
    "\n",
    "def fixed_point_content(encoded_content_real, encoded_content_fake):\n",
    "    diff = encoded_content_fake- encoded_content_real\n",
    "    return tf.reduce_mean(tf.square(diff))\n",
    "\n",
    "def style_classsification_loss(y_pred, y_true):\n",
    "    return error_classif(y_true, y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard Logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Metrics\n",
    "# Generator Losses\n",
    "met_generator_train = tf.keras.metrics.Mean(name=\"Total Generator Loss\")\n",
    "\n",
    "met_generator_reconstruction_train = tf.keras.metrics.Mean(name=\"Generator Reconstruction Loss\")\n",
    "\n",
    "met_generator_local_realness_train = tf.keras.metrics.Mean(name=\"Generator local Realness loss Train\")\n",
    "met_generator_global_realness_train = tf.keras.metrics.Mean(name=\"Generator Global Realness loss Train\")\n",
    "\n",
    "# Style Encoder Loss\n",
    "met_triplet_train = tf.keras.metrics.Mean(name=\"Total Triplet Loss\")\n",
    "met_disentanglement_train = tf.keras.metrics.Mean(name=\"Disentanglement Loss\")\n",
    "met_style_encoder_train = tf.keras.metrics.Mean(name=\"Style Loss\")\n",
    "\n",
    "# Content encoder Loss\n",
    "met_content_encoder_train= tf.keras.metrics.Mean(name=\"Content Encoder Loss\")\n",
    "\n",
    "# Central Discriminator\n",
    "met_central_d_train= tf.keras.metrics.Mean(name=\"Central Discriminator Loss\")\n",
    "met_central_d_style_real_train = tf.keras.metrics.Mean(name=\"Central Discriminator Loss Real Style Classif\")\n",
    "met_central_d_style_fake_train = tf.keras.metrics.Mean(name=\"Central Discriminator Loss Fake Style Classif\")\n",
    "\n",
    "# Channel Discriminator.\n",
    "met_channel_d_train= tf.keras.metrics.Mean(name=\"Channel Discriminator Loss\")\n",
    "met_channel_d_style_real_train = tf.keras.metrics.Mean(name=\"Channel Discriminator Real Style Classification\")\n",
    "met_channel_d_style_fake_train = tf.keras.metrics.Mean(name=\"Channel Discriminator Fake Style Classification\")\n",
    "\n",
    "# Correlation Metric\n",
    "met_corr_style1_train = tf.keras.metrics.Mean(name=\"Correlation Metric Style 1\")\n",
    "met_corr_style2_train = tf.keras.metrics.Mean(name=\"Correlation Metric Style 2\")\n",
    "\n",
    "\n",
    "# Valid Metrics\n",
    "# Generator Metric\n",
    "met_generator_valid = tf.keras.metrics.Mean(name=\"Total Generator Loss\")\n",
    "\n",
    "met_generator_reconstruction_valid = tf.keras.metrics.Mean(name=\"Generator Reconstruction Loss\")\n",
    "\n",
    "met_generator_local_realness_valid = tf.keras.metrics.Mean(name=\"Generator local Realness loss valid\")\n",
    "met_generator_global_realness_valid = tf.keras.metrics.Mean(name=\"Generator Global Realness loss valid\")\n",
    "\n",
    "# Style Encoder Loss\n",
    "met_triplet_valid = tf.keras.metrics.Mean(name=\"Total Triplet Loss\")\n",
    "met_disentanglement_valid = tf.keras.metrics.Mean(name=\"Disentanglement Loss\")\n",
    "met_style_encoder_valid = tf.keras.metrics.Mean(name=\"Style Loss\")\n",
    "\n",
    "# Content encoder Loss\n",
    "met_content_encoder_valid= tf.keras.metrics.Mean(name=\"Content Encoder Loss\")\n",
    "\n",
    "# Central Discriminator\n",
    "met_central_d_valid= tf.keras.metrics.Mean(name=\"Central Discriminator Loss\")\n",
    "met_central_d_style_real_valid = tf.keras.metrics.Mean(name=\"Central Discriminator Loss Real Style Classif\")\n",
    "met_central_d_style_fake_valid = tf.keras.metrics.Mean(name=\"Central Discriminator Loss Fake Style Classif\")\n",
    "\n",
    "# Channel Discriminator.\n",
    "met_channel_d_valid= tf.keras.metrics.Mean(name=\"Channel Discriminator Loss\")\n",
    "met_channel_d_style_real_valid = tf.keras.metrics.Mean(name=\"Channel Discriminator Real Style Classification\")\n",
    "met_channel_d_style_fake_valid = tf.keras.metrics.Mean(name=\"Channel Discriminator Fake Style Classification\")\n",
    "\n",
    "# Correlation Metric\n",
    "met_corr_style1_valid = tf.keras.metrics.Mean(name=\"Correlation Metric Style 1\")\n",
    "met_corr_style2_valid = tf.keras.metrics.Mean(name=\"Correlation Metric Style 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SUMMARY_WRITER = tf.summary.create_file_writer(TRAIN_LOGS_DIR_PATH)\n",
    "VALID_SUMMARY_WRITER = tf.summary.create_file_writer(VALID_LOGS_DIR_PATH)\n",
    "\n",
    "def log_train_losses(epoch, plot_buf):\n",
    "    image = tf.image.decode_png(plot_buf.getvalue(), channels=4)\n",
    "    image = tf.expand_dims(image, 0)\n",
    "\n",
    "    with TRAIN_SUMMARY_WRITER.as_default():\n",
    "        tf.summary.scalar(\"00 - Correlation Metric Style 1\", met_corr_style1_train.result(), step=epoch)\n",
    "        tf.summary.scalar(\"01 - Correlation Metric Style 2\", met_corr_style2_train.result(), step=epoch)\n",
    "\n",
    "        tf.summary.scalar(\"10 - Total Generator Loss\", met_generator_train.result(), step=epoch)\n",
    "        tf.summary.scalar(\"11 - Reconstruction from Content\", met_generator_reconstruction_train.result(), step=epoch)\n",
    "\n",
    "        tf.summary.scalar(\"12 - Central Realness\", met_generator_global_realness_train.result(), step=epoch)\n",
    "        tf.summary.scalar(\"13 - Local Realness\", met_generator_local_realness_train.result(), step=epoch)\n",
    "\n",
    "        tf.summary.scalar(\"20 - Style Loss\", met_style_encoder_train.result(), step=epoch)\n",
    "        tf.summary.scalar(\"21 - Triplet Loss\", met_triplet_train.result(), step=epoch)\n",
    "        tf.summary.scalar(\"22 - Disentanglement Loss\", met_disentanglement_train.result(), step=epoch)\n",
    "\n",
    "        tf.summary.scalar(\"30 - Content Loss\", met_content_encoder_train.result(), step=epoch)\n",
    "\n",
    "        tf.summary.scalar(\"40 - Global Discriminator Loss\", met_central_d_train.result(), step=epoch)\n",
    "        tf.summary.scalar(\"40 - Local Discriminator Loss\", met_channel_d_train.result(), step=epoch)\n",
    "\n",
    "        tf.summary.scalar(\"41 - Global Discriminator Style Loss (Real Data)\", met_central_d_style_real_train.result(), step=epoch)\n",
    "        tf.summary.scalar(\"41 - Global Discriminator Style Loss (Fake Data)\", met_central_d_style_fake_train.result(), step=epoch)\n",
    "\n",
    "        tf.summary.scalar(\"42 - Local Discriminator Style Loss (Real Data)\", met_channel_d_style_real_train.result(), step=epoch)\n",
    "        tf.summary.scalar(\"42 - Local Discriminator Style Loss (Fake Data)\", met_channel_d_style_fake_train.result(), step=epoch)\n",
    "\n",
    "        tf.summary.image(\"Training Generations\", image, step=epoch)\n",
    "\n",
    "\n",
    "def log_valid_losses(epoch):\n",
    "    with VALID_SUMMARY_WRITER.as_default():\n",
    "        tf.summary.scalar(\"00 - Correlation Metric Style 1\", met_corr_style1_valid.result(), step=epoch)\n",
    "        tf.summary.scalar(\"01 - Correlation Metric Style 2\", met_corr_style2_valid.result(), step=epoch)\n",
    "\n",
    "        tf.summary.scalar(\"10 - Total Generator Loss\", met_generator_valid.result(), step=epoch)\n",
    "        tf.summary.scalar(\"11 - Reconstruction from Content\", met_generator_reconstruction_valid.result(), step=epoch)\n",
    "\n",
    "        tf.summary.scalar(\"12 - Central Realness\", met_generator_global_realness_valid.result(), step=epoch)\n",
    "        tf.summary.scalar(\"13 - Local Realness\", met_generator_local_realness_valid.result(), step=epoch)\n",
    "\n",
    "        tf.summary.scalar(\"20 - Style Loss\", met_style_encoder_valid.result(), step=epoch)\n",
    "        tf.summary.scalar(\"21 - Triplet Loss\", met_triplet_valid.result(), step=epoch)\n",
    "        tf.summary.scalar(\"22 - Disentanglement Loss\", met_disentanglement_valid.result(), step=epoch)\n",
    "\n",
    "        tf.summary.scalar(\"30 - Content Loss\", met_content_encoder_valid.result(), step=epoch)\n",
    "\n",
    "        tf.summary.scalar(\"40 - Global Discriminator Loss\", met_central_d_valid.result(), step=epoch)\n",
    "        tf.summary.scalar(\"40 - Local Discriminator Loss\", met_channel_d_valid.result(), step=epoch)\n",
    "\n",
    "        tf.summary.scalar(\"41 - Global Discriminator Style Loss (Real Data)\", met_central_d_style_real_valid.result(), step=epoch)\n",
    "        tf.summary.scalar(\"41 - Global Discriminator Style Loss (Fake Data)\", met_central_d_style_fake_valid.result(), step=epoch)\n",
    "\n",
    "        tf.summary.scalar(\"42 - Local Discriminator Style Loss (Real Data)\", met_channel_d_style_real_valid.result(), step=epoch)\n",
    "        tf.summary.scalar(\"42 - Local Discriminator Style Loss (Fake Data)\", met_channel_d_style_fake_valid.result(), step=epoch)\n",
    "        \n",
    "\n",
    "def reset_metric_states():\n",
    "    met_generator_train.reset_states()\n",
    "    met_generator_reconstruction_train.reset_states()\n",
    "    met_generator_local_realness_train.reset_states()\n",
    "    met_generator_global_realness_train.reset_states()\n",
    "    met_triplet_train.reset_states()\n",
    "    met_disentanglement_train.reset_states()\n",
    "    met_style_encoder_train.reset_states()\n",
    "    met_content_encoder_train.reset_states()\n",
    "    met_central_d_train.reset_states()\n",
    "    met_central_d_style_real_train.reset_states()\n",
    "    met_central_d_style_fake_train.reset_states()\n",
    "    met_channel_d_train.reset_states()\n",
    "    met_channel_d_style_real_train.reset_states()\n",
    "    met_channel_d_style_fake_train.reset_states()\n",
    "    met_corr_style1_train.reset_states()\n",
    "    met_corr_style2_train.reset_states()\n",
    "\n",
    "\n",
    "def reset_valid_states():\n",
    "    met_generator_valid.reset_states()\n",
    "    met_generator_reconstruction_valid.reset_states()\n",
    "    met_generator_local_realness_valid.reset_states()\n",
    "    met_generator_global_realness_valid.reset_states()\n",
    "    met_triplet_valid.reset_states()\n",
    "    met_disentanglement_valid.reset_states()\n",
    "    met_style_encoder_valid.reset_states()\n",
    "    met_content_encoder_valid.reset_states()\n",
    "    met_central_d_valid.reset_states()\n",
    "    met_central_d_style_real_valid.reset_states()\n",
    "    met_central_d_style_fake_valid.reset_states()\n",
    "    met_channel_d_valid.reset_states()\n",
    "    met_channel_d_style_real_valid.reset_states()\n",
    "    met_channel_d_style_fake_valid.reset_states()\n",
    "    met_corr_style1_valid.reset_states()\n",
    "    met_corr_style2_valid.reset_states()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate model for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_encoder = make_content_encoder(SEQUENCE_LENGTH, N_SIGNAL, FEAT_WIENER)\n",
    "style_encoder = make_style_encoder(SEQUENCE_LENGTH, N_SIGNAL, STYLE_VECTOR_SIZE)\n",
    "decoder = make_generator(N_SAMPLE_WIENER, FEAT_WIENER, STYLE_VECTOR_SIZE ,N_SIGNAL)\n",
    "global_discriminator = make_global_discriminator(SEQUENCE_LENGTH, N_SIGNAL, N_CLASSES)\n",
    "local_discriminator = create_local_discriminator(N_SIGNAL, SEQUENCE_LENGTH, N_CLASSES)\n",
    "\n",
    "opt_content_encoder = tf.keras.optimizers.Adam(learning_rate=0.002)\n",
    "opt_style_encoder = tf.keras.optimizers.Adam(learning_rate=0.002)\n",
    "opt_decoder = tf.keras.optimizers.Adam(learning_rate=0.002)\n",
    "\n",
    "local_discriminator_opt = tf.keras.optimizers.Adam(learning_rate=0.002)\n",
    "global_discriminator_opt = tf.keras.optimizers.Adam(learning_rate=0.002)\n",
    "\n",
    "# opt_content_encoder = tf.keras.optimizers.RMSprop(learning_rate=1e-4)\n",
    "# opt_style_encoder = tf.keras.optimizers.RMSprop(learning_rate=1e-4)\n",
    "# opt_decoder = tf.keras.optimizers.RMSprop(learning_rate=1e-3)\n",
    "# opt_discr = tf.keras.optimizers.RMSprop(learning_rate=1e-5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "indexes = np.array(list(product(range(BS), range(BS))))\n",
    "other_index = np.arange(BS)* BS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_indexes = np.array([ i* BS+i for i in range(BS) for _ in range(BS) ])\n",
    "pos_indexes = np.array([ BS*j + i for i in range(BS) for j in range(BS)])\n",
    "neg_indexes = np.array([ (j*BS + (i+1)%BS) for i in range(BS) for j in range(BS)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_style_voctor_for_dis_loss(style_vector:tf.Tensor, batch_size:int):\n",
    "    anchor_indexes = np.array([ i* batch_size+i for i in range(batch_size) for _ in range(batch_size-1) ])\n",
    "    return tf.gather(style_vector, anchor_indexes)\n",
    "\n",
    "def get_anchor_positive_negative_from_batch(style_from_style_ts:tf.Tensor, style_of_generations:tf.Tensor, batch_size:int):\n",
    "    anchor_indexes = np.array([ i* batch_size+i for i in range(batch_size) for _ in range(batch_size) ])\n",
    "    pos_indexes = np.array([ batch_size*j + i for i in range(batch_size) for j in range(batch_size)])\n",
    "    neg_indexes = np.array([ (j*batch_size + (i+1)%batch_size) for i in range(batch_size) for j in range(batch_size)])\n",
    "\n",
    "    # Anchor are for example:\n",
    "    # [(c0, s0), (c0, s0), \n",
    "    # (c1, s1), (c1, s1), ...]\n",
    "    anchors = tf.gather(style_from_style_ts, anchor_indexes)\n",
    "    # Different content, Same Style:\n",
    "    # [(c1, s0), (c2, s0),(c2, s0),\n",
    "    #  (c1, s1), (c2, s1), (c3, s1)...]\n",
    "    # E_s(G(x, y1))\n",
    "    pos_vector= tf.gather(style_of_generations, pos_indexes)\n",
    "    # Same content but different style\n",
    "    # [(c1, s1), (c2, s1),(c2, s1),\n",
    "    #  (c1, s2), (c2, s2), (c3, s2)...]\n",
    "    # # E_s(G(x, y2)) \n",
    "    neg_vector = tf.gather(style_of_generations, neg_indexes)\n",
    "\n",
    "    return anchors, pos_vector, neg_vector\n",
    "\n",
    "def get_dissantanglement_loss_component(style_of_generations, style_of_style, batch_size:int):\n",
    "    anchor_indexes = np.array([ i* batch_size+i for i in range(batch_size) for _ in range(batch_size-1) ])\n",
    "    pos_indexes = np.array([ batch_size*j + i for i in range(batch_size) for j in range(batch_size) if i !=j ])\n",
    "\n",
    "    es_y = tf.gather(style_of_style, anchor_indexes)\n",
    "    es_x1_y = tf.gather(style_of_generations, anchor_indexes)\n",
    "    es_x2_y = tf.gather(style_of_generations, pos_indexes)\n",
    "\n",
    "    return es_y, es_x1_y, es_x2_y\n",
    "\n",
    "\n",
    "def l2(x:tf.Tensor, y:tf.Tensor):\n",
    "    diff = tf.square(y- x)\n",
    "    _distance = tf.reduce_sum(diff, axis=-1)\n",
    "    return _distance\n",
    "\n",
    "\n",
    "def fixed_point_content(encoded_content_real, encoded_content_fake):\n",
    "    diff = l2(encoded_content_real, encoded_content_fake)\n",
    "    return tf.reduce_mean(diff)\n",
    "\n",
    "def fixed_point_triplet_style_loss(anchor_encoded_style, positive_encoded_style, negative_encoded_style):\n",
    "    # shape: [BS, Style_length]\n",
    "    negative_distance = l2(negative_encoded_style, anchor_encoded_style)\n",
    "    positive_distance = l2(positive_encoded_style, anchor_encoded_style)\n",
    "\n",
    "    triplet = TRIPLET_R+ positive_distance- negative_distance\n",
    "    zeros = tf.zeros_like(triplet)\n",
    "    triplet = tf.math.maximum(triplet, zeros)\n",
    "\n",
    "    loss = tf.reduce_mean(triplet)\n",
    "    return loss\n",
    "\n",
    "def fixed_point_disentanglement(\n",
    "        es_x1_y:tf.Tensor, \n",
    "        es_x2_y:tf.Tensor, \n",
    "        es_y:tf.Tensor\n",
    "        ):\n",
    "\n",
    "    diff1 = l2(es_x1_y, es_x2_y)\n",
    "    diff2 = l2(es_x1_y, es_y)\n",
    "\n",
    "    loss = diff1- diff2\n",
    "    zeros = tf.zeros_like(loss)\n",
    "    loss = tf.math.maximum(loss, zeros)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def style_constrain_loss(style_of_style_batch, style_of_generation):\n",
    "    diff = l2(style_of_style_batch, style_of_generation)\n",
    "    diff = tf.reduce_mean(diff)\n",
    "    return diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc = tf.keras.losses.BinaryCrossentropy()\n",
    "error_classif = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "def local_discriminator_loss(crits_on_real, crits_on_fake):\n",
    "    individual_losses = []\n",
    "\n",
    "    for i in range(crits_on_real.shape[0]):\n",
    "        l1 = bc(tf.zeros_like(crits_on_real), crits_on_fake[i])\n",
    "        l2 = bc(tf.ones_like(crits_on_real), crits_on_real[i])\n",
    "        loss = (l1+ l2)/2\n",
    "        individual_losses.append(loss)\n",
    "        \n",
    "    return individual_losses\n",
    "\n",
    "\n",
    "def local_generator_loss(crit_on_fake):\n",
    "    individual_losses = []\n",
    "    true_label = tf.zeros(crit_on_fake[0].shape)\n",
    "\n",
    "    for i in range(crit_on_fake.shape[0]):\n",
    "        individual_losses.append(bc(true_label, crit_on_fake[i]))\n",
    "        \n",
    "    return tf.reduce_mean(individual_losses)\n",
    "\n",
    "\n",
    "def style_classsification_loss(y_pred, y_true):\n",
    "    return error_classif(y_true, y_pred)\n",
    "\n",
    "\n",
    "def global_discriminator_loss(crit_on_real, crit_on_fake):\n",
    "    l1 = bc(tf.zeros_like(crit_on_fake), crit_on_fake)\n",
    "    l2 = bc(tf.ones_like(crit_on_real), crit_on_real)\n",
    "    loss = (l1+ l2)/2\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def global_generator_loss(crit_on_fake):\n",
    "    loss = bc(tf.ones_like(crit_on_fake), crit_on_fake)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _pairwise_distance(a_embeddings, b_embeddings):\n",
    "    # Get the dot product between all embeddings\n",
    "    # shape (batch_size, batch_size)\n",
    "    dot_product = tf.matmul(a_embeddings, tf.transpose(b_embeddings))\n",
    "\n",
    "    # Get squared L2 norm for each embedding. We can just take the diagonal of `dot_product`.\n",
    "    # This also provides more numerical stability (the diagonal of the result will be exactly 0).\n",
    "    # shape (batch_size,)\n",
    "    square_norm = tf.linalg.diag_part(dot_product)\n",
    "\n",
    "    # Compute the pairwise distance matrix as we have:\n",
    "    # ||a - b||^2 = ||a||^2  - 2 <a, b> + ||b||^2\n",
    "    # shape (batch_size, batch_size)\n",
    "    distances = tf.expand_dims(square_norm, 0) - 2.0 * dot_product + tf.expand_dims(square_norm, 1)\n",
    "\n",
    "    # Because of computation errors, some distances might be negative so we put everything >= 0.0\n",
    "    distances = tf.maximum(distances, 0.0)\n",
    "\n",
    "    return distances\n",
    "\n",
    "def get_positives(labels):\n",
    "    # will assign 1 where this is a positive \n",
    "    positive_mask = np.array([ tf.cast(labels == l, tf.float32) for l in labels])\n",
    "    positive_mask = positive_mask - np.identity(positive_mask.shape[0])\n",
    "    return positive_mask\n",
    "\n",
    "def get_negative(labels):\n",
    "    neg_labels= np.array([ tf.cast(labels != l, tf.float32) for l in labels])\n",
    "    return neg_labels\n",
    "\n",
    "\n",
    "def get_triplet_loss(anchor_embedding, positive_embedding, negative_embedding):\n",
    "    positive_distance= _pairwise_distance(anchor_embedding, positive_embedding)\n",
    "    negative_distance= _pairwise_distance(anchor_embedding, negative_embedding)\n",
    "\n",
    "    positive_index= tf.argmax(positive_distance, axis=1)\n",
    "\n",
    "    pos_embedding = tf.gather(positive_embedding, positive_index)\n",
    " \n",
    "    neg_indexes = tf.argmin(negative_distance, axis=1)\n",
    "    \n",
    "    neg_embeddings= tf.gather(negative_embedding, neg_indexes)\n",
    "\n",
    "    positive_distances= l2(anchor_embedding, pos_embedding)\n",
    "    negative_distances= l2(anchor_embedding, neg_embeddings)\n",
    "\n",
    "    loss = tf.reduce_mean(tf.maximum(TRIPLET_R+ positive_distances - negative_distances, 0))\n",
    "\n",
    "    return loss\n",
    "\n",
    "def compute_metric(generated_style:tf.Tensor, true_style:tf.Tensor):\n",
    "    true_signature = signature_on_batch(true_style, config.met_params.ins, config.met_params.outs, config.met_params.signature_length)\n",
    "    generated_signature= signature_on_batch(generated_style, config.met_params.ins, config.met_params.outs, config.met_params.signature_length)\n",
    "\n",
    "    return signature_metric(true_signature, generated_signature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant([1, 2, 3])\n",
    "\n",
    "b = tf.stack((a,a))\n",
    "c = tf.reduce_mean(b, 0)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def discriminator_step(content_sequence1, content_sequence2, style1_sequences, style2_sequences):\n",
    "    # Discriminator Step\n",
    "    with tf.GradientTape(persistent=True) as discr_tape:\n",
    "        # Sequence generations.\n",
    "        c1 = content_encoder(content_sequence1, training=False)\n",
    "\n",
    "        s1 = style_encoder(style1_sequences, training=False)\n",
    "        s2 = style_encoder(style2_sequences, training=False)\n",
    "\n",
    "        style1_generated= decoder([c1, s1], training=False)\n",
    "        style2_generated= decoder([c1, s2], training=False)\n",
    "\n",
    "        # Global on Real\n",
    "        g_crit_real1, g_style_classif_real1 = global_discriminator(style1_sequences, training=True)\n",
    "        g_crit_real2, g_style_classif_real2 = global_discriminator(style2_sequences, training=True)\n",
    "\n",
    "        # Global on Generated\n",
    "        g_crit_fake1, _ = global_discriminator(style1_generated, training=True)\n",
    "        g_crit_fake2, _ = global_discriminator(style2_generated, training=True)\n",
    "\n",
    "        # Local on Real\n",
    "        l_crit1_real = local_discriminator(style1_sequences, training=True)\n",
    "        l_crit2_real = local_discriminator(style2_sequences, training=True)\n",
    "\n",
    "        # Local on fake\n",
    "        l_crit_fake1 = local_discriminator(style1_generated, training=True)\n",
    "        l_crit_fake2 = local_discriminator(style2_generated, training=True)\n",
    "\n",
    "        # Compute the loss for GLOBAL the Discriminator\n",
    "        g_crit_loss1 = discriminator_loss(g_crit_real1, g_crit_fake1)\n",
    "        g_crit_loss2 = discriminator_loss(g_crit_real2, g_crit_fake2)\n",
    "\n",
    "        g_crit_loss = tf.stack((g_crit_loss1, g_crit_loss2))\n",
    "        g_crit_loss = tf.reduce_mean(g_crit_loss, 0)\n",
    "        \n",
    "        style_labels = tf.zeros((BS, 1))\n",
    "        g_style1_real = style_classsification_loss(g_style_classif_real1, style_labels+ 0.)\n",
    "        g_style2_real = style_classsification_loss(g_style_classif_real2, style_labels +1.)\n",
    "        g_style_real = tf.stack((g_style1_real, g_style2_real))\n",
    "        g_style_real = tf.reduce_mean(g_style_real, 0)\n",
    "\n",
    "        l_loss1 = local_discriminator_loss(l_crit1_real, l_crit_fake1)\n",
    "        l_loss2 = local_discriminator_loss(l_crit2_real, l_crit_fake2)\n",
    "\n",
    "        l_loss = tf.stack((l_loss1, l_loss2))\n",
    "        l_loss = tf.reduce_mean(l_loss, 0)\n",
    "\n",
    "    # (GOBAL DISCRIMINATOR): Real / Fake and style\n",
    "    global_discr_gradient = discr_tape.gradient([g_crit_loss, g_style_real], global_discriminator.trainable_variables)\n",
    "    global_discriminator_opt.apply_gradients(zip(global_discr_gradient, global_discriminator.trainable_variables)) \n",
    "    \n",
    "    grads = discr_tape.gradient(l_loss, local_discriminator.trainable_variables)\n",
    "    local_discriminator_opt.apply_gradients(zip(grads, local_discriminator.trainable_variables))\n",
    "\n",
    "    met_central_d_train(g_crit_loss)\n",
    "    met_central_d_style_real_train(g_style_real)\n",
    "\n",
    "    met_channel_d_train(l_loss)\n",
    "\n",
    "    \n",
    "@tf.function\n",
    "def generator_step(content_sequence1, content_sequence2, style1_sequences, style2_sequences):\n",
    "\n",
    "    # Here, things get a little bit more complicated :)\n",
    "    with tf.GradientTape() as content_tape, tf.GradientTape() as style_tape, tf.GradientTape() as decoder_tape:\n",
    "        contents = tf.concat([content_sequence1, content_sequence2], 0)\n",
    "        cs = content_encoder(contents, training=True)\n",
    "        s_cs = style_encoder(contents, training=True)\n",
    "        id_generated = decoder([cs, s_cs], training=True)\n",
    "        reconstr_loss = recontruction_loss(contents, id_generated)\n",
    "\n",
    "        ####\n",
    "        contents = tf.concat([content_sequence1, content_sequence2, content_sequence1, content_sequence2], 0)\n",
    "        styles = tf.concat([style1_sequences, style1_sequences, style2_sequences, style2_sequences], 0)\n",
    "        _bs = content_sequence1.shape[0]\n",
    "\n",
    "        encoded_content= content_encoder(contents, training=True)\n",
    "        encoded_styles = style_encoder(styles, training=True)\n",
    "\n",
    "        generations = decoder([encoded_content, encoded_styles], training=True)\n",
    "\n",
    "        s_generations = style_encoder(generations, training=True)\n",
    "        c_generations = content_encoder(generations, training=True)\n",
    "        \n",
    "        style_labels = np.zeros((4* _bs,))\n",
    "        style_labels[2* _bs:]= 1.\n",
    "\n",
    "        # Discriminator pass for the adversarial loss for the generator.\n",
    "        crit_on_fake, style_classif_fakes = global_discriminator(generations, training=False)\n",
    "\n",
    "        # Local Discriminator on Fake Data.\n",
    "        l_crit_on_fake = local_discriminator(generations, training=False)\n",
    "\n",
    "\n",
    "        # Channel Discriminator losses\n",
    "        local_realness_loss = local_generator_loss(l_crit_on_fake)\n",
    "        \n",
    "        # Global Generator losses.\n",
    "        global_style_loss = style_classsification_loss(style_classif_fakes, style_labels)\n",
    "        global_realness_loss = generator_loss(crit_on_fake)\n",
    "\n",
    "\n",
    "        c1s = tf.concat([\n",
    "            encoded_content[:_bs],                  # 2\n",
    "            encoded_content[2*_bs:3* _bs]           # 3\n",
    "        ], 0)\n",
    "\n",
    "        c2s = tf.concat([\n",
    "            encoded_content[_bs:2* _bs],            # 2\n",
    "            encoded_content[3*_bs:]                 # 4 \n",
    "        ], 0)\n",
    "\n",
    "        generated_c1s = tf.concat([\n",
    "            c_generations[:_bs],\n",
    "            c_generations[2*_bs:3* _bs]\n",
    "        ], 0)\n",
    "\n",
    "        generated_c2s = tf.concat([\n",
    "            c_generations[_bs:2* _bs],\n",
    "            c_generations[3*_bs:]\n",
    "        ], 0)\n",
    "\n",
    "        s_c1_s1 = s_generations[:_bs]\n",
    "        s_c1_s2 = s_generations[2*_bs: 3*_bs]\n",
    "        s_c2_s2 = s_generations[3*_bs:] \n",
    "\n",
    "        s1s = encoded_styles[:_bs]\n",
    "        s2s = encoded_styles[2* _bs:3* _bs]\n",
    "\n",
    "        content_preservation1 = fixed_point_content(c1s, generated_c1s)\n",
    "        content_preservation2 = fixed_point_content(c2s, generated_c2s)\n",
    "        content_preservation = (content_preservation1+ content_preservation2)/2\n",
    "\n",
    "        triplet_style =  get_triplet_loss(s1s, s_c1_s1, s_c1_s2)\n",
    "        content_style_disentenglement = fixed_point_disentanglement(s_c2_s2, s_c1_s2, s2s)\n",
    "\n",
    "        content_encoder_loss = L_CONTENT* content_preservation\n",
    "        style_encoder_loss = L_TRIPLET* triplet_style + L_DIS* content_style_disentenglement\n",
    "\n",
    "        g_loss = L_RECONSTR* reconstr_loss+ L_GLOBAL* global_realness_loss + L_GLOBAL* global_style_loss + L_LOCAL* local_realness_loss\n",
    "\n",
    "    # Make the Networks Learn!\n",
    "    content_grad=content_tape.gradient(content_encoder_loss, content_encoder.trainable_variables)\n",
    "    style_grad = style_tape.gradient(style_encoder_loss, style_encoder.trainable_variables)\n",
    "    decoder_grad = decoder_tape.gradient(g_loss, decoder.trainable_variables)\n",
    "        \n",
    "    opt_content_encoder.apply_gradients(zip(content_grad, content_encoder.trainable_variables))\n",
    "    opt_style_encoder.apply_gradients(zip(style_grad, style_encoder.trainable_variables))\n",
    "    opt_decoder.apply_gradients(zip(decoder_grad, decoder.trainable_variables))\n",
    "\n",
    "    met_generator_train(g_loss)\n",
    "    met_generator_reconstruction_train(reconstr_loss)\n",
    "\n",
    "    met_generator_local_realness_train(local_realness_loss)\n",
    "    met_generator_global_realness_train(global_realness_loss)\n",
    "\n",
    "    met_central_d_style_fake_train(global_style_loss)\n",
    "\n",
    "    met_disentanglement_train(content_style_disentenglement)\n",
    "    met_triplet_train(triplet_style)\n",
    "    met_style_encoder_train(style_encoder_loss)\n",
    "    met_content_encoder_train(content_preservation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def generator_valid(content_sequence1, content_sequence2, style1_sequences, style2_sequences):\n",
    "    contents = tf.concat([content_sequence1, content_sequence2], 0)\n",
    "    cs = content_encoder(contents, training=True)\n",
    "    s_cs = style_encoder(contents, training=True)\n",
    "    id_generated = decoder([cs, s_cs], training=True)\n",
    "    reconstr_loss = recontruction_loss(contents, id_generated)\n",
    "\n",
    "    ####\n",
    "    contents = tf.concat([content_sequence1, content_sequence2, content_sequence1, content_sequence2], 0)\n",
    "    styles = tf.concat([style1_sequences, style1_sequences, style2_sequences, style2_sequences], 0)\n",
    "    _bs = content_sequence1.shape[0]\n",
    "\n",
    "    encoded_content= content_encoder(contents, training=True)\n",
    "    encoded_styles = style_encoder(styles, training=True)\n",
    "\n",
    "    generations = decoder([encoded_content, encoded_styles], training=True)\n",
    "\n",
    "    s_generations = style_encoder(generations, training=True)\n",
    "    c_generations = content_encoder(generations, training=True)\n",
    "    \n",
    "    style_labels = np.zeros((4* _bs,))\n",
    "    style_labels[2* _bs:]= 1.\n",
    "\n",
    "    # Discriminator pass for the adversarial loss for the generator.\n",
    "    crit_on_fake, style_classif_fakes = global_discriminator(generations, training=False)\n",
    "\n",
    "    # Local Discriminator on Fake Data.\n",
    "    l_crit_on_fake = local_discriminator(generations, training=False)\n",
    "\n",
    "\n",
    "    # Channel Discriminator losses\n",
    "    local_realness_loss = local_generator_loss(l_crit_on_fake)\n",
    "    \n",
    "    # Global Generator losses.\n",
    "    global_style_loss = style_classsification_loss(style_classif_fakes, style_labels)\n",
    "    global_realness_loss = generator_loss(crit_on_fake)\n",
    "\n",
    "\n",
    "    c1s = tf.concat([\n",
    "        encoded_content[:_bs],                  # 2\n",
    "        encoded_content[2*_bs:3* _bs]           # 3\n",
    "    ], 0)\n",
    "\n",
    "    c2s = tf.concat([\n",
    "        encoded_content[_bs:2* _bs],            # 2\n",
    "        encoded_content[3*_bs:]                 # 4 \n",
    "    ], 0)\n",
    "\n",
    "    generated_c1s = tf.concat([\n",
    "        c_generations[:_bs],\n",
    "        c_generations[2*_bs:3* _bs]\n",
    "    ], 0)\n",
    "\n",
    "    generated_c2s = tf.concat([\n",
    "        c_generations[_bs:2* _bs],\n",
    "        c_generations[3*_bs:]\n",
    "    ], 0)\n",
    "\n",
    "    s_c1_s1 = s_generations[:_bs]\n",
    "    s_c1_s2 = s_generations[2*_bs: 3*_bs]\n",
    "    s_c2_s2 = s_generations[3*_bs:] \n",
    "\n",
    "    s1s = encoded_styles[:_bs]\n",
    "    s2s = encoded_styles[2* _bs:3* _bs]\n",
    "\n",
    "    content_preservation1 = fixed_point_content(c1s, generated_c1s)\n",
    "    content_preservation2 = fixed_point_content(c2s, generated_c2s)\n",
    "    content_preservation = (content_preservation1+ content_preservation2)/2\n",
    "\n",
    "    triplet_style =  get_triplet_loss(s1s, s_c1_s1, s_c1_s2)\n",
    "    content_style_disentenglement = fixed_point_disentanglement(s_c2_s2, s_c1_s2, s2s)\n",
    "\n",
    "    style_encoder_loss = L_TRIPLET* triplet_style + L_DIS* content_style_disentenglement\n",
    "\n",
    "    g_loss = L_RECONSTR* reconstr_loss+ L_GLOBAL* global_realness_loss + L_GLOBAL* global_style_loss + L_LOCAL* local_realness_loss\n",
    "    \n",
    "    met_generator_valid(g_loss)\n",
    "    met_generator_reconstruction_valid(reconstr_loss)\n",
    "\n",
    "    met_generator_local_realness_valid(local_realness_loss)\n",
    "    met_generator_global_realness_valid(global_realness_loss)\n",
    "\n",
    "    met_central_d_style_fake_valid(global_style_loss)\n",
    "\n",
    "    met_content_encoder_valid(content_preservation)\n",
    "    \n",
    "    met_style_encoder_valid(style_encoder_loss)\n",
    "    met_triplet_valid(triplet_style)\n",
    "    met_disentanglement_valid(content_style_disentenglement)\n",
    "    \n",
    "\n",
    "@tf.function\n",
    "def discriminator_valid(content_sequence1, content_sequence2, style1_sequences, style2_sequences):\n",
    "    c1 = content_encoder(content_sequence1, training=False)\n",
    "\n",
    "    s1 = style_encoder(style1_sequences, training=False)\n",
    "    s2 = style_encoder(style2_sequences, training=False)\n",
    "\n",
    "    style1_generated= decoder([c1, s1], training=False)\n",
    "    style2_generated= decoder([c1, s2], training=False)\n",
    "\n",
    "    # Global on Real\n",
    "    g_crit_real1, g_style_classif_real1 = global_discriminator(style1_sequences, training=True)\n",
    "    g_crit_real2, g_style_classif_real2 = global_discriminator(style2_sequences, training=True)\n",
    "\n",
    "    # Global on Generated\n",
    "    g_crit_fake1, _ = global_discriminator(style1_generated, training=True)\n",
    "    g_crit_fake2, _ = global_discriminator(style2_generated, training=True)\n",
    "\n",
    "    # Local on Real\n",
    "    l_crit1_real = local_discriminator(style1_sequences, training=True)\n",
    "    l_crit2_real = local_discriminator(style2_sequences, training=True)\n",
    "\n",
    "    # Local on fake\n",
    "    l_crit_fake1 = local_discriminator(style1_generated, training=True)\n",
    "    l_crit_fake2 = local_discriminator(style2_generated, training=True)\n",
    "\n",
    "    # Compute the loss for GLOBAL the Discriminator\n",
    "    g_crit_loss1 = discriminator_loss(g_crit_real1, g_crit_fake1)\n",
    "    g_crit_loss2 = discriminator_loss(g_crit_real2, g_crit_fake2)\n",
    "\n",
    "    g_crit_loss = tf.stack((g_crit_loss1, g_crit_loss2))\n",
    "    g_crit_loss = tf.reduce_mean(g_crit_loss, 0)\n",
    "    \n",
    "    style_labels = tf.zeros((content_sequence1.shape[0], 1))\n",
    "    g_style1_real = style_classsification_loss(g_style_classif_real1, style_labels+ 0.)\n",
    "    g_style2_real = style_classsification_loss(g_style_classif_real2, style_labels +1.)\n",
    "    g_style_real = tf.stack((g_style1_real, g_style2_real))\n",
    "    g_style_real = tf.reduce_mean(g_style_real, 0)\n",
    "\n",
    "    l_loss1 = local_discriminator_loss(l_crit1_real, l_crit_fake1)\n",
    "    l_loss2 = local_discriminator_loss(l_crit2_real, l_crit_fake2)\n",
    "\n",
    "    l_loss = tf.stack((l_loss1, l_loss2))\n",
    "    l_loss = tf.reduce_mean(l_loss, 0)\n",
    "\n",
    "    met_central_d_valid(g_crit_loss)\n",
    "    met_central_d_style_real_valid(g_style_real)\n",
    "\n",
    "    met_channel_d_valid(l_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    total_batch = \"?\"\n",
    "    for e in range(EPOCHS):\n",
    "        reset_metric_states()\n",
    "        # reset_valid_states()\n",
    "        \n",
    "        print(\"[+] Train Step...\")\n",
    "        for i, (content_batch, style1_sequences, style2_sequences) in enumerate(zip(content_dset_train, style1_dset_train, style2_dset_train)):\n",
    "            content_sequence1 = content_batch[:int(BS)]\n",
    "            content_sequence2 = content_batch[int(BS):]\n",
    "\n",
    "            if i%DISCR_STEP == 0:\n",
    "                discriminator_step(content_sequence1, content_sequence2, style1_sequences, style2_sequences)\n",
    "\n",
    "            generator_step(content_sequence1, content_sequence2, style1_sequences, style2_sequences)\n",
    "            \n",
    "            print(f\"\\r e:{e}/{EPOCHS}; {i}/{total_batch}. G_loss {met_generator_train.result():0.2f}; Triplet Loss {met_triplet_train.result():0.2f}; Disentanglement Loss: {met_disentanglement_train.result():0.2f}; Content Loss {met_content_encoder_train.result():0.2f} Local D [Crit; Style]: [{met_channel_d_train.result():0.2f}; {met_channel_d_style_real_train.result():0.2f}]; Global D [Crit; Style]: [{met_central_d_train.result():0.2f}; {met_central_d_style_fake_train.result():0.2f}]       \", end=\"\")\n",
    "\n",
    "        print()\n",
    "        print(\"[+] Validation Step...\")\n",
    "        for vb, (content_batch, style1_sequences, style2_sequences) in enumerate(zip(content_dset_valid, style1_dset_valid, style2_dset_valid)):\n",
    "            content_sequence1 = content_batch[:int(VALID_BATCH_SIZE)]\n",
    "            content_sequence2 = content_batch[int(VALID_BATCH_SIZE):]\n",
    "\n",
    "            discriminator_valid(content_sequence1, content_sequence2, style1_sequences, style2_sequences)\n",
    "            generator_valid(content_sequence1, content_sequence2, style1_sequences, style2_sequences)\n",
    "\n",
    "            print(f\"\\r e:{e}/{EPOCHS}; {vb}/{total_batch}. G_loss {met_generator_valid.result():0.2f}; Triplet Loss {met_triplet_valid.result():0.2f}; Disentanglement Loss: {met_disentanglement_valid.result():0.2f}; Content Loss {met_content_encoder_valid.result():0.2f} Local D [Crit; Style]: [{met_channel_d_valid.result():0.2f}; {met_channel_d_style_real_valid.result():0.2f}]; Global D [Crit; Style]: [{met_central_d_valid.result():0.2f}; {met_central_d_style_fake_valid.result():0.2f}]       \", end=\"\")\n",
    "    \n",
    "        # Make Generations Train Set\n",
    "        generation_style1_train = generate(seed_content_train, seed_style1_train)\n",
    "        generation_style1_valid = generate(seed_content_valid, seed_style1_valid)\n",
    "\n",
    "        generation_style2_train = generate(seed_content_train, seed_style2_train)\n",
    "        generation_style2_valid = generate(seed_content_valid, seed_style2_valid)\n",
    "        \n",
    "        metric_s1_train = compute_metric(generation_style1_train, seed_style1_train)\n",
    "        metric_s1_valid = compute_metric(generation_style1_valid, seed_style1_valid)\n",
    "\n",
    "        metric_s2_train = compute_metric(generation_style2_train, seed_style2_train)\n",
    "        metric_s2_valid = compute_metric(generation_style2_valid, seed_style2_valid)\n",
    "\n",
    "\n",
    "        met_corr_style1_train(metric_s1_train)\n",
    "        met_corr_style2_train(metric_s2_train)\n",
    "\n",
    "        met_corr_style1_valid(metric_s1_valid)\n",
    "        met_corr_style2_valid(metric_s2_valid)\n",
    "\n",
    "        vis_fig = plot_generated_sequence2(seed_content_valid, seed_style1_valid, seed_style2_valid)\n",
    "        plot_buff = fig_to_buff(vis_fig)\n",
    "        \n",
    "        print(\"\\n******* Signature Difference *******\")\n",
    "        print(f\"Style 1:\")\n",
    "        print(f\"Train: {metric_s1_train:0.2f}; Valid: {metric_s1_valid:0.2f}\")\n",
    "\n",
    "        print(f\"Style 2:\")\n",
    "        print(f\"Train: {metric_s2_train:0.2f}; Valid: {metric_s2_valid:0.2f}\")\n",
    "\n",
    "        print(\"*******\")\n",
    "\n",
    "        log_train_losses(e, plot_buff)\n",
    "        log_valid_losses(e)\n",
    "        print()\n",
    "    \n",
    "        if e == 0:\n",
    "            total_batch = i \n",
    "\n",
    "train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TSTR Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.NaiveClassifier import make_naive_discriminator\n",
    "\n",
    "def make_classification_dataset(df:pd.DataFrame, overlap:float=.5):\n",
    "\n",
    "    dset_train, dset_valid = make_train_valid_dset(\n",
    "        df, \n",
    "        SEQUENCE_LENGTH, \n",
    "        GRANUARITY, \n",
    "        int(overlap* SEQUENCE_LENGTH),\n",
    "        64,\n",
    "        64,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    # Extract labels from dataset.\n",
    "    dset_train = dset_train.map(lambda s: (s[:, :, :-1], s[:, int(SEQUENCE_LENGTH//2), -1]))\n",
    "    dset_valid = dset_valid.map(lambda s: (s[:, :, :-1], s[:, int(SEQUENCE_LENGTH//2), -1]))\n",
    "\n",
    "    return dset_train, dset_valid\n",
    "\n",
    "def stylize_dataset(content_dataset:tf.data.Dataset, style_dataset:tf.data.Dataset):\n",
    "    mixted_dataset = tf.data.Dataset.zip((content_dataset, style_dataset))\n",
    "    stylized_dataset = mixted_dataset.map(lambda _cont, _sty: (generate(_cont[0], _sty[0]), _cont[1]))\n",
    "    return stylized_dataset\n",
    "\n",
    "\n",
    "def train_naive_classifier(train_classif_dataset:tf.data.Dataset, valid_classif_dataset:tf.data.Dataset)->tf.keras.models.Model:\n",
    "    naive_model = make_naive_discriminator((SEQUENCE_LENGTH, N_SIGNAL), 5)\n",
    "    _epochs = 10\n",
    "    \n",
    "    history = naive_model.fit(train_classif_dataset, validation_data=valid_classif_dataset, epochs=_epochs)\n",
    "\n",
    "    return naive_model, history\n",
    "\n",
    "def tstr_test(style_label:int):\n",
    "    # In this test, we will \n",
    "    # Train a naive classifier on the stylized dataset by the network.\n",
    "    # Test this naive model on the real stylized dataset.\n",
    "    _df_stylized_real= pd.read_hdf(STYLE_DATASET_PATHS[style_label]).astype(np.float32)\n",
    "    _df_content_dset = pd.read_hdf(CONTENT_DATASET_PATH).astype(np.float32)\n",
    "\n",
    "    _content_dset_train, _content_dset_valid = make_classification_dataset(_df_content_dset)\n",
    "    _style_dset_train, _style_dset_valid = make_classification_dataset(_df_stylized_real)\n",
    "\n",
    "    # Generate the 'stylized' dataset \n",
    "    _generated_train = stylize_dataset(_content_dset_train, _style_dset_train)\n",
    "    _generated_valid = stylize_dataset(_content_dset_valid, _style_dset_valid)\n",
    "\n",
    "    # Train a classifier on the stylized dataset.\n",
    "    print(\"[+] Train on Generated Data.\")\n",
    "    naive_12_model, naive_on_generated_hist = train_naive_classifier(_generated_train, _generated_valid)\n",
    "    # Evaluate on generated\n",
    "    naive_gen_trained_on_gen = naive_12_model.evaluate(_generated_valid)[1]\n",
    "    naive_gen_trained_on_real = naive_12_model.evaluate(_content_dset_valid)[1]\n",
    "\n",
    "\n",
    "    # Train on the real Dataset.\n",
    "    print(\"[+] Train on Real Data.\")\n",
    "    naive_on_real, naive_on_real_hist = train_naive_classifier(_content_dset_train, _content_dset_valid)\n",
    "    eval_on_real = naive_on_real.evaluate(_content_dset_valid)[1]\n",
    "    eval_on_generated= naive_on_real.evaluate(_generated_valid)[1]\n",
    "\n",
    "\n",
    "    print(\"fTrained on Generated dataset. (Style Label {style_label})\")\n",
    "    print(f\"[+] Acc on Generated valid set: {naive_gen_trained_on_gen:0.2f}.\")\n",
    "    print(f\"[+] Acc on Real valid set:      {naive_gen_trained_on_real:0.2f}.\")\n",
    "\n",
    "    print(f\"Trained on Real dataset. (Style Label {style_label})\")\n",
    "    print(f\"[+] Acc on Generated valid set: {eval_on_generated:0.2f}.\")\n",
    "    print(f\"[+] Acc on Real valid set:      {eval_on_real:0.2f}.\")\n",
    "\n",
    "    print('[+] Show Histories.')\n",
    "\n",
    "    plt.figure(figsize=(18, 5))\n",
    "    ax = plt.subplot(111)\n",
    "    ax.grid(True)\n",
    "    ax.set_title('Loss durring the training on *real* data.')\n",
    "    plt.plot(naive_on_real_hist.history['loss'], \".-\", label=\"Train Loss.\")\n",
    "    plt.plot(naive_on_real_hist.history['val_loss'], \".-\", label=\"Valid Loss.\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(18, 5))\n",
    "    ax = plt.subplot(111)\n",
    "    ax.grid(True)\n",
    "    ax.set_title(f'Loss durring the training on *generated* data. (Style Label {style_label})')\n",
    "    plt.plot(naive_on_generated_hist.history['loss'], \".-\", label=\"Train Loss.\")\n",
    "    plt.plot(naive_on_generated_hist.history['val_loss'], \".-\", label=\"Valid Loss.\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    cols = [\"Style Label\", \"Naive trained on Gen, Gen Acc\", \"Naive trained on Gen, Real Acc\", \"Naive trained on Real, Gen Acc\", \"Naive trained on Real, Real Acc\"]\n",
    "    df = pd.DataFrame([[style_label, naive_gen_trained_on_gen, naive_gen_trained_on_real, eval_on_real, eval_on_generated]], columns=cols)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_results(folder_to_save:str):\n",
    "    # Create the folder.\n",
    "    file_path = f\"{folder_to_save}/scores.xlsx\"\n",
    "\n",
    "    style_0_results = tstr_test(0)\n",
    "    style_1_results = tstr_test(1)\n",
    "\n",
    "    full_df = pd.concat((style_0_results, style_1_results), ignore_index=True)\n",
    "    full_df = full_df.set_index(['Style Label'])\n",
    "\n",
    "    full_df.to_excel(file_path)\n",
    "\n",
    "log_results(SAVE_FOLDER)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
