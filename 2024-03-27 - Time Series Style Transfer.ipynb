{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Style Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from configs.Metric import Metric\n",
    "from configs.SimulatedData import Proposed\n",
    "from dataset.tf_pipeline import convert_dataframe_to_tensorflow_sequences\n",
    "from datetime import datetime\n",
    "import io\n",
    "import os\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "\n",
    "gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Proposed()\n",
    "\n",
    "D1_PATH = \"data/simulated_dataset/01 - Source Domain.h5\"\n",
    "D2_PATH = \"data/simulated_dataset/amplitude_shift/4.5_4.5.h5\"\n",
    "\n",
    "SEQUENCE_LENGTH = config.sequence_lenght_in_sample\n",
    "GRANUARITY = config.granularity\n",
    "OVERLAP = config.overlap\n",
    "BS =  config.batch_size\n",
    "EPOCHS = config.epochs\n",
    "NUM_SEQUENCE_TO_GENERATE = config.met_params.sequence_to_generate\n",
    "\n",
    "STYLE_VECTOR_SIZE = 32\n",
    "FEAT_WIENER = 2\n",
    "N_SAMPLE_WIENER = SEQUENCE_LENGTH//4\n",
    "NOISE_DIM= (N_SAMPLE_WIENER, FEAT_WIENER)\n",
    "N_VALIDATION_SEQUENCE = 500\n",
    "TRIPLET_R = 10"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_d1= pd.read_hdf(D1_PATH).astype(np.float32)\n",
    "df_d2= pd.read_hdf(D2_PATH).astype(np.float32)\n",
    "\n",
    "\n",
    "dset_d1 = convert_dataframe_to_tensorflow_sequences(\n",
    "    df_d1, \n",
    "    SEQUENCE_LENGTH, \n",
    "    GRANUARITY, \n",
    "    int(OVERLAP* SEQUENCE_LENGTH),\n",
    "    BS\n",
    ")\n",
    "\n",
    "dset_d2 = convert_dataframe_to_tensorflow_sequences(\n",
    "    df_d2, \n",
    "    SEQUENCE_LENGTH, \n",
    "    GRANUARITY, \n",
    "    int(OVERLAP* SEQUENCE_LENGTH),\n",
    "    BS\n",
    ")\n",
    "\n",
    "dset_d1_train = dset_d1.skip(N_VALIDATION_SEQUENCE)\n",
    "dset_d1_valid = dset_d1.take(N_VALIDATION_SEQUENCE)\n",
    "\n",
    "dset_d2_train = dset_d2.skip(N_VALIDATION_SEQUENCE)\n",
    "dset_d2_valid = dset_d2.take(N_VALIDATION_SEQUENCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = next(iter(dset_d1))\n",
    "\n",
    "mean, variance = tf.nn.moments(x, axes=[1], keepdims=True)\n",
    "standard_dev = tf.sqrt(variance)\n",
    "\n",
    "standard_dev.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Content Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define AdaIN Layers for Time Series\n",
    "class AdaIN(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(AdaIN, self).__init__()\n",
    "\n",
    "    def get_mean_std(self, x, eps=1e-5):\n",
    "        _mean, _variance = tf.nn.moments(x, axes=[1], keepdims=True)\n",
    "        standard_dev = tf.sqrt(_variance+ eps)\n",
    "        return _mean, standard_dev\n",
    "\n",
    "    def call(self, content_input, style_input):\n",
    "        # print(content_input.shape, style_input.shape)\n",
    "        content_mean, content_std = self.get_mean_std(content_input)\n",
    "        style_mean, style_std = self.get_mean_std(style_input)\n",
    "        adain_res =style_std* (content_input - content_mean) / content_std+ style_mean\n",
    "        return adain_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_content_encoder(seq_length:int, n_feat:int, feat_wiener:int):\n",
    "    _input = tf.keras.Input((seq_length, n_feat,))\n",
    "\n",
    "    x = tf.keras.layers.Conv1D(128, 5, 2, padding='same')(_input)\n",
    "    x = tf.keras.layers.LeakyReLU()(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv1D(feat_wiener, 5, 2, padding='same')(x)\n",
    "    x = tf.keras.layers.LeakyReLU()(x)\n",
    "\n",
    "    model = tf.keras.Model(_input, x)\n",
    "\n",
    "    return model\n",
    "\n",
    "def make_style_encoder(seq_length:int, n_feat:int, vector_output_shape:int):\n",
    "    _input = tf.keras.Input((seq_length, n_feat))\n",
    "\n",
    "    x = tf.keras.layers.Conv1D(128, 5, 2, padding='same')(_input)\n",
    "    x = tf.keras.layers.LeakyReLU()(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv1D(128, 5, 2, padding='same')(x)\n",
    "    x = tf.keras.layers.LeakyReLU()(x)\n",
    "\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dense(vector_output_shape)(x)\n",
    "\n",
    "    model = tf.keras.Model(_input, x)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def make_decoder(n_sample_wiener:int, feat_wiener:int, style_vector_size:int, out_feat:int):\n",
    "    _content_input = tf.keras.Input((n_sample_wiener, feat_wiener))\n",
    "    _style_input = tf.keras.Input((style_vector_size, 1))\n",
    "\n",
    "    _upsampled_style= tf.keras.layers.UpSampling1D()(_style_input)\n",
    "\n",
    "    x = tf.keras.layers.Conv1DTranspose(128, 5, 2, padding='same')(_content_input)\n",
    "    x = AdaIN()(x, _style_input)\n",
    "    x = tf.keras.layers.Conv1DTranspose(128, 5, 1, padding='same')(x)\n",
    "    x = tf.keras.layers.LeakyReLU()(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv1DTranspose(out_feat, 5, 2, padding='same')(x)\n",
    "    x = AdaIN()(x, _upsampled_style)\n",
    "    x = tf.keras.layers.Conv1DTranspose(out_feat, 5, 1, padding='same')(x)\n",
    "    x = tf.keras.layers.LeakyReLU()(x)\n",
    "\n",
    "    model = tf.keras.Model([_content_input, _style_input], x)\n",
    "    return model\n",
    "\n",
    "\n",
    "def make_discriminator(seq_length:int, n_feat:int):\n",
    "    _input = tf.keras.Input((seq_length, n_feat))\n",
    "    x = tf.keras.layers.Conv1D(16, 5, 2, padding='same')(_input)\n",
    "    x = tf.keras.layers.LeakyReLU()(x)\n",
    "    x = tf.keras.layers.Dropout(0.3)(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv1D(16, 5, 2, padding='same')(x)\n",
    "    x = tf.keras.layers.LeakyReLU()(x)\n",
    "    x = tf.keras.layers.Dropout(0.3)(x)\n",
    "\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    _output = tf.keras.layers.Dense(1)(x)\n",
    "\n",
    "    model = tf.keras.Model(_input, _output)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_encoder = make_content_encoder(SEQUENCE_LENGTH, df_d1.shape[1], FEAT_WIENER)\n",
    "# content_encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_encoder = make_style_encoder(SEQUENCE_LENGTH, df_d1.shape[1], STYLE_VECTOR_SIZE)\n",
    "# style_encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = make_decoder(N_SAMPLE_WIENER, FEAT_WIENER, STYLE_VECTOR_SIZE ,df_d1.shape[1])\n",
    "# decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_discriminator = make_discriminator(SEQUENCE_LENGTH, df_d1.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleTransferModel(tf.keras.Model):\n",
    "    def __init__(self, seq_length:int, n_feat:int, style_vector_shape:int, n_sample_wiener:int, feat_wiener:int, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.content_encoder= make_content_encoder(seq_length, n_feat, feat_wiener)\n",
    "        self.style_encoder = make_style_encoder(seq_length, n_feat, style_vector_shape)\n",
    "        self.decoder = make_decoder(n_sample_wiener, feat_wiener, style_vector_shape, n_feat)\n",
    "\n",
    "    def __call__(self, content_sequence, style_sequence):\n",
    "        encoded_content = self.content_encoder(content_sequence)\n",
    "        encoded_style = self.style_encoder(style_sequence)\n",
    "\n",
    "        print(encoded_content.shape, encoded_style.shape)\n",
    "\n",
    "        generated_sequence = self.decoder([encoded_content, encoded_style])\n",
    "\n",
    "        return generated_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(content_batch, style_batch):\n",
    "    content = content_encoder(content_batch, training=False)\n",
    "    style = style_encoder(style_batch, training=False)\n",
    "    generated = decoder([content, style], training=False)\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_content_batch = next(iter(dset_d1_valid))\n",
    "seed_style_batch = next(iter(dset_d2_valid))\n",
    "\n",
    "generated_sequence = generate(seed_content_batch, seed_style_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_style_transfered_time_series(content_ts, style_ts, generated_ts, show=True, save_to:str=None):\n",
    "    all_values = np.array([content_ts, style_ts, generated_ts])\n",
    "    _min, _max = np.min(all_values)-1, np.max(all_values)+ 1\n",
    "    n_series = 3\n",
    "\n",
    "    fig = plt.figure(figsize=(18, 10))\n",
    "    plt.suptitle(\"Visualization of the generations\", fontsize=18)\n",
    "    for i in range(n_series):\n",
    "        ax = plt.subplot(3, n_series, n_series* i+ 1)\n",
    "        ax.set_title(f\"*[{i}]* Content Time Series >\")\n",
    "        ax.plot(content_ts[i])\n",
    "        ax.set_ylim(_min, _max)\n",
    "        ax.grid(True)\n",
    "\n",
    "        ax = plt.subplot(3, n_series, n_series* i+ 2)\n",
    "        ax.set_title(f\"*[{i}]* Style Time Series. >\")\n",
    "        ax.plot(style_ts[i])\n",
    "        ax.set_ylim(_min, _max)\n",
    "        ax.grid(True)\n",
    "\n",
    "        ax = plt.subplot(3, n_series, n_series* i+ 3)\n",
    "        ax.set_title(f\"*[{i}]* Generated Time Series.\")\n",
    "        ax.plot(generated_ts[i])\n",
    "        ax.set_ylim(_min, _max)\n",
    "        ax.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if not save_to is None:\n",
    "        fig.savefig(save_to)\n",
    "    \n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "    return fig\n",
    "\n",
    "generation_figure = plot_style_transfered_time_series(seed_content_batch, seed_style_batch, generated_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fig_to_buff(fig):\n",
    "    buf = io.BytesIO()\n",
    "    fig.savefig(buf, format='png')\n",
    "    buf.seek(0)\n",
    "    plt.close(fig)\n",
    "    return buf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "def recontruction_loss(true:tf.Tensor, generated:tf.Tensor):\n",
    "    diff = generated- true\n",
    "    result = tf.math.reduce_mean(tf.square(diff))\n",
    "    return result\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "\n",
    "\n",
    "def fixed_point_content(encoded_content_real, encoded_content_fake):\n",
    "    diff = encoded_content_fake- encoded_content_real\n",
    "    return tf.reduce_mean(tf.square(diff))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard Logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Metrics\n",
    "met_generator_train = tf.keras.metrics.Mean(name=\"Train generator Loss\")\n",
    "met_style_encoder_train = tf.keras.metrics.Mean(name=\"Train Style Encoder Loss\")\n",
    "met_content_encoder_train= tf.keras.metrics.Mean(name=\"Train Content Encoder Loss\")\n",
    "met_disc_loss_train= tf.keras.metrics.Mean(name=\"Train Discriminatir Loss\")\n",
    "\n",
    "# Valid Metrics\n",
    "met_generator_valid = tf.keras.metrics.Mean(name=\"valid generator Loss\")\n",
    "met_style_encoder_valid = tf.keras.metrics.Mean(name=\"valid Style Encoder Loss\")\n",
    "met_content_encoder_valid= tf.keras.metrics.Mean(name=\"valid Content Encoder Loss\")\n",
    "met_disc_loss_valid= tf.keras.metrics.Mean(name=\"valid Discriminatir Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_str = datetime.now().strftime('%Y-%m-%d_%H_%M_%S')\n",
    "\n",
    "BASE_DIR = f\"logs/{date_str} - Style Transfer Algorithm\"\n",
    "TRAIN_LOGS_DIR_PATH = f\"{BASE_DIR}/train\"\n",
    "VALID_LOGS_DIR_PATH = f\"{BASE_DIR}/valid\"\n",
    "GENERATION_LOG = f\"{BASE_DIR}/Generations\"\n",
    "os.makedirs(GENERATION_LOG)\n",
    "\n",
    "TRAIN_SUMMARY_WRITER = tf.summary.create_file_writer(TRAIN_LOGS_DIR_PATH)\n",
    "VALID_SUMMARY_WRITER = tf.summary.create_file_writer(VALID_LOGS_DIR_PATH)\n",
    "\n",
    "def log_train_losses(epoch, plot_buf):\n",
    "    image = tf.image.decode_png(plot_buf.getvalue(), channels=4)\n",
    "    image = tf.expand_dims(image, 0)\n",
    "\n",
    "    with TRAIN_SUMMARY_WRITER.as_default():\n",
    "        tf.summary.scalar(\"Generator Loss\", met_generator_train.result(), step=epoch)\n",
    "        tf.summary.scalar(\"Style Loss\", met_style_encoder_train.result(), step=epoch)\n",
    "        tf.summary.scalar(\"Content Loss\", met_content_encoder_train.result(), step=epoch)\n",
    "        tf.summary.scalar(\"Discriminator Loss\", met_disc_loss_train.result(), step=epoch)\n",
    "        \n",
    "        tf.summary.image(\"Training Generations\", image, step=epoch)\n",
    "\n",
    "\n",
    "def log_valid_losses(epoch):\n",
    "    with VALID_SUMMARY_WRITER.as_default():\n",
    "        tf.summary.scalar(\"Generator Loss\", met_generator_valid.result(), step=epoch)\n",
    "        tf.summary.scalar(\"Style Loss\", met_style_encoder_valid.result(), step=epoch)\n",
    "        tf.summary.scalar(\"Content Loss\", met_content_encoder_valid.result(), step=epoch)\n",
    "        tf.summary.scalar(\"Discriminator Loss\", met_disc_loss_valid.result(), step=epoch)\n",
    "\n",
    "\n",
    "def reset_metric_states():\n",
    "    met_generator_train.reset_states()\n",
    "    met_style_encoder_train.reset_states()\n",
    "    met_content_encoder_train.reset_states()\n",
    "    met_disc_loss_train.reset_states()\n",
    "\n",
    "def reset_valid_states():\n",
    "    met_generator_valid.reset_states()\n",
    "    met_style_encoder_valid.reset_states()\n",
    "    met_content_encoder_valid.reset_states()\n",
    "    met_disc_loss_valid.reset_states()\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate model for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_encoder = make_content_encoder(SEQUENCE_LENGTH, df_d1.shape[1], FEAT_WIENER)\n",
    "style_encoder = make_style_encoder(SEQUENCE_LENGTH, df_d1.shape[1], STYLE_VECTOR_SIZE)\n",
    "decoder = make_decoder(N_SAMPLE_WIENER, FEAT_WIENER, STYLE_VECTOR_SIZE ,df_d1.shape[1])\n",
    "\n",
    "opt_content_encoder = tf.keras.optimizers.RMSprop(1e-6)\n",
    "opt_style_encoder = tf.keras.optimizers.RMSprop(1e-6)\n",
    "opt_decoder = tf.keras.optimizers.RMSprop(1e-3)\n",
    "opt_discr = tf.keras.optimizers.RMSprop(1e-6)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "indexes = np.array(list(product(range(BS), range(BS))))\n",
    "other_index = np.arange(BS)* BS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test anchor, positive and negative sample calculation...\n",
    "# anchor_indexes = np.array([ i* BS+i for i in range(BS) for _ in range(BS-1) ])\n",
    "# pos_indexes = np.array([ BS*j + i for i in range(BS) for j in range(BS) if i !=j ])\n",
    "# neg_indexes = np.array([ j+BS*i  for i in range(BS) for j in range(BS) if i !=j] )\n",
    "\n",
    "# print(\"Anchor Samples:\\n\", indexes[anchor_indexes, :])\n",
    "# print(\"Positive Samples:\\n\", indexes[pos_indexes, :])\n",
    "# print(\"Negative Samples:\\n\", indexes[neg_indexes, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_style_voctor_for_dis_loss(style_vector:tf.Tensor, batch_size:int):\n",
    "    anchor_indexes = np.array([ i* batch_size+i for i in range(batch_size) for _ in range(batch_size-1) ])\n",
    "    return tf.gather(style_vector, anchor_indexes)\n",
    "\n",
    "def get_anchor_positive_negative_from_batch(style_from_style_ts:tf.Tensor, style_of_generations:tf.Tensor, batch_size:int):\n",
    "    anchor_indexes = np.array([ i* batch_size+i for i in range(batch_size) for _ in range(batch_size-1) ])\n",
    "    pos_indexes = np.array([ batch_size*j + i for i in range(batch_size) for j in range(batch_size) if i !=j ])\n",
    "    neg_indexes = np.array([ j+batch_size*i  for i in range(batch_size) for j in range(batch_size) if i !=j] )\n",
    "\n",
    "    # Anchor are for example:\n",
    "    # [(c0, s0), (c0, s0), (c0, s0), \n",
    "    # (c1, s1), (c1, s1), (c1, s1), ... (c3, s3)]\n",
    "    anchors = tf.gather(style_from_style_ts, anchor_indexes)\n",
    "    # Different content, Same Style:\n",
    "    # [(c1, s0), (c1, s1), (c1, s2)...]\n",
    "    pos_vector= tf.gather(style_of_generations, pos_indexes)\n",
    "    # Same content but different style\n",
    "    neg_vector = tf.gather(style_of_generations, neg_indexes)\n",
    "\n",
    "    return anchors, pos_vector, neg_vector\n",
    "\n",
    "def get_dissantanglement_loss_component(style_of_generations, style_of_style, batch_size:int):\n",
    "    anchor_indexes = np.array([ i* batch_size+i for i in range(batch_size) for _ in range(batch_size-1) ])\n",
    "    pos_indexes = np.array([ batch_size*j + i for i in range(batch_size) for j in range(batch_size) if i !=j ])\n",
    "\n",
    "    es_y = tf.gather(style_of_style, anchor_indexes)\n",
    "    es_x1_y = tf.gather(style_of_generations, anchor_indexes)\n",
    "    es_x2_y = tf.gather(style_of_generations, pos_indexes)\n",
    "\n",
    "    return es_y, es_x1_y, es_x2_y\n",
    "    \n",
    "\n",
    "def l2(x:tf.Tensor, y:tf.Tensor):\n",
    "    diff = tf.square(x- y)\n",
    "    _distance = tf.reduce_sum(diff, axis=-1)\n",
    "    return tf.sqrt(_distance)\n",
    "\n",
    "\n",
    "def fixed_point_triplet_style_loss(anchor_encoded_style, positive_encoded_style, negative_encoded_style):\n",
    "    # shape: [BS, Style_length]\n",
    "\n",
    "    positive_distance = l2(positive_encoded_style, negative_encoded_style)\n",
    "    negative_distance = l2(positive_encoded_style, anchor_encoded_style)\n",
    "\n",
    "    triplet = TRIPLET_R+ positive_distance- negative_distance\n",
    "    zeros = tf.zeros_like(triplet)\n",
    "    triplet = tf.math.maximum(triplet, zeros)\n",
    "\n",
    "    loss = tf.reduce_mean(triplet)\n",
    "\n",
    "    return loss\n",
    "\n",
    "def fixed_point_disentanglement(\n",
    "        es_x1_y:tf.Tensor, \n",
    "        es_x2_y:tf.Tensor, \n",
    "        es_y:tf.Tensor\n",
    "        ):\n",
    "\n",
    "    diff1 = l2(es_x1_y, es_x2_y)\n",
    "    diff2 = l2(es_x1_y, es_y)\n",
    "\n",
    "    loss = diff1- diff2\n",
    "    zeros = tf.zeros_like(loss)\n",
    "    loss = tf.math.maximum(loss, zeros)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(content_batch:tf.Tensor, style_batch:tf.Tensor):\n",
    "    lambda_reconstr= 1\n",
    "    lambda_realness= 1\n",
    "    lambda_adv= 1\n",
    "    lambda_content= 1\n",
    "    lambda_triplet= 1\n",
    "    lambda_dis= 1\n",
    "\n",
    "    with tf.GradientTape() as content_tape, tf.GradientTape() as style_tape, tf.GradientTape() as decoder_tape, tf.GradientTape() as discr_tape:\n",
    "        extended_content_images = tf.gather(content_batch, indexes[:, 0])\n",
    "        extended_style_images = tf.gather(style_batch, indexes[:, 1])\n",
    "        \n",
    "        # Get the content form the content batch\n",
    "        content_of_content = content_encoder(extended_content_images, training=True)\n",
    "        # Get the Style from the style batch\n",
    "        style_of_style = style_encoder(extended_style_images, training=True)\n",
    "\n",
    "        # Generate the time series given the content and the style.\n",
    "        generated_ts = decoder([content_of_content, style_of_style], training=True)\n",
    "\n",
    "        # Get the content and style form generated data\n",
    "        content_of_generations = content_encoder(generated_ts, training=True)\n",
    "        style_of_generations = style_encoder(generated_ts, training=True)\n",
    "\n",
    "        # Get content image stylized with a image from the style batch\n",
    "        # E.g.  (c1, s1), (c2, s2)... (cn, sn)\n",
    "        reduced_stylized = tf.gather(generated_ts, other_index)\n",
    "\n",
    "        crit_on_fake = global_discriminator(reduced_stylized, training=True)\n",
    "        crit_on_real = global_discriminator(content_batch, training=True)\n",
    "\n",
    "        reconstr_from_content = recontruction_loss(extended_content_images, generated_ts) \n",
    "        realness = generator_loss(reduced_stylized)\n",
    "\n",
    "        global_dicriminator_loss = discriminator_loss(crit_on_real, crit_on_fake)\n",
    "        \n",
    "        content_similarity = fixed_point_content(content_of_content, content_of_generations)\n",
    "\n",
    "        anchors, positive_vectors, negative_vectors = get_anchor_positive_negative_from_batch(style_of_style, style_of_generations, BS)\n",
    "        es_y, es_x1_y, es_x2_y =  get_dissantanglement_loss_component(style_of_generations, style_of_style, BS)\n",
    "\n",
    "        # sorted_styles = get_style_voctor_for_dis_loss(style_of_style, BS)\n",
    "        triplet_style = fixed_point_triplet_style_loss(anchors, positive_vectors, negative_vectors)\n",
    "\n",
    "        dis_loss = fixed_point_disentanglement(es_y, es_x1_y, es_x2_y)\n",
    "\n",
    "        d_loss = lambda_adv* global_dicriminator_loss\n",
    "        content_encoder_loss = lambda_content* content_similarity\n",
    "        style_encoder_loss =  lambda_triplet* triplet_style + lambda_dis* dis_loss \n",
    "        g_loss = lambda_reconstr* reconstr_from_content+ lambda_realness* realness + lambda_content* content_similarity + lambda_triplet* triplet_style + lambda_dis* dis_loss \n",
    "\n",
    "\n",
    "    content_grad=content_tape.gradient(content_encoder_loss, content_encoder.trainable_variables)\n",
    "    style_grad = style_tape.gradient(style_encoder_loss, style_encoder.trainable_variables)\n",
    "    decoder_grad = decoder_tape.gradient(g_loss, decoder.trainable_variables)\n",
    "    discr_grads = discr_tape.gradient(d_loss, global_discriminator.trainable_variables)\n",
    "\n",
    "    opt_content_encoder.apply_gradients(zip(content_grad, content_encoder.trainable_variables))\n",
    "    opt_style_encoder.apply_gradients(zip(style_grad, style_encoder.trainable_variables))\n",
    "    opt_decoder.apply_gradients(zip(decoder_grad, decoder.trainable_variables))\n",
    "    opt_discr.apply_gradients(zip(discr_grads, global_discriminator.trainable_variables))\n",
    "\n",
    "\n",
    "    met_generator_train(g_loss)\n",
    "    met_style_encoder_train(style_encoder_loss)\n",
    "    met_content_encoder_train(content_encoder_loss)\n",
    "    met_disc_loss_train(d_loss)\n",
    "\n",
    "@tf.function\n",
    "def valid_step(content_batch:tf.Tensor, style_batch:tf.Tensor):\n",
    "    lambda_reconstr= 1\n",
    "    lambda_realness= 1\n",
    "    lambda_adv= 1\n",
    "    lambda_content= 1\n",
    "    lambda_triplet= 1\n",
    "    lambda_dis= 1\n",
    "\n",
    "    extended_content_images = tf.gather(content_batch, indexes[:, 0])\n",
    "    extended_style_images = tf.gather(style_batch, indexes[:, 1])\n",
    "    \n",
    "    # Get the content form the content batch\n",
    "    content_of_content = content_encoder(extended_content_images, training=True)\n",
    "    # Get the Style from the style batch\n",
    "    style_of_style = style_encoder(extended_style_images, training=True)\n",
    "\n",
    "    # Generate the time series given the content and the style.\n",
    "    generated_ts = decoder([content_of_content, style_of_style], training=True)\n",
    "\n",
    "    # Get the content and style form generated data\n",
    "    content_of_generations = content_encoder(generated_ts, training=True)\n",
    "    style_of_generations = style_encoder(generated_ts, training=True)\n",
    "\n",
    "    # Get content image stylized with a image from the style batch\n",
    "    # E.g.  (c1, s1), (c2, s2)... (cn, sn)\n",
    "    reduced_stylized = tf.gather(generated_ts, other_index)\n",
    "\n",
    "    crit_on_fake = global_discriminator(reduced_stylized, training=True)\n",
    "    crit_on_real = global_discriminator(content_batch, training=True)\n",
    "\n",
    "    reconstr_from_content = recontruction_loss(extended_content_images, generated_ts) \n",
    "    realness = generator_loss(reduced_stylized)\n",
    "\n",
    "    global_dicriminator_loss = discriminator_loss(crit_on_real, crit_on_fake)\n",
    "    \n",
    "    content_similarity = fixed_point_content(content_of_content, content_of_generations)\n",
    "\n",
    "    anchors, positive_vectors, negative_vectors = get_anchor_positive_negative_from_batch(style_of_style, style_of_generations, BS)\n",
    "    es_y, es_x1_y, es_x2_y =  get_dissantanglement_loss_component(style_of_generations, style_of_style, BS)\n",
    "\n",
    "    # sorted_styles = get_style_voctor_for_dis_loss(style_of_style, BS)\n",
    "    triplet_style = fixed_point_triplet_style_loss(anchors, positive_vectors, negative_vectors)\n",
    "\n",
    "    dis_loss = fixed_point_disentanglement(es_y, es_x1_y, es_x2_y)\n",
    "\n",
    "    g_loss = lambda_reconstr* reconstr_from_content+ lambda_realness* realness\n",
    "    d_loss = lambda_adv* global_dicriminator_loss\n",
    "    content_encoder_loss = lambda_content* content_similarity\n",
    "    style_encoder_loss =  lambda_triplet* triplet_style + lambda_dis* dis_loss \n",
    "\n",
    "    met_generator_valid(g_loss)\n",
    "    met_style_encoder_valid(style_encoder_loss)\n",
    "    met_content_encoder_valid(content_encoder_loss)\n",
    "    met_disc_loss_valid(d_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    total_batch = \"?\"\n",
    "    force_valid_steps = 5000\n",
    "    for e in range(EPOCHS):\n",
    "        reset_metric_states()\n",
    "        reset_valid_states()\n",
    "        filename = f'{GENERATION_LOG}/{e}.png'\n",
    "        print(\"[+] Train Step...\")\n",
    "        for i, (content_batch, style_batch) in enumerate(zip(dset_d1_train, dset_d2_train)):\n",
    "            train_step(content_batch, style_batch)\n",
    "            print(f\"\\r e:{e}/{EPOCHS}; {i}/{total_batch}. G_loss {met_generator_train.result():0.2f} style loss {met_style_encoder_train.result():0.2f} content loss {met_content_encoder_train.result():0.2f} discr loss {met_disc_loss_train.result():0.2f}        \", end=\"\")\n",
    "            # return\n",
    "\n",
    "        print()\n",
    "        print(\"[+] Validation Step...\")\n",
    "        for vb, (content_batch, style_batch) in enumerate(zip(dset_d1_valid, dset_d2_valid)):\n",
    "            valid_step(content_batch, style_batch)\n",
    "            print(f\"\\r e:{e}/{EPOCHS}; {vb+1}/500. G_loss {met_generator_valid.result():0.2f} style loss {met_style_encoder_valid.result():0.2f} content loss {met_content_encoder_valid.result():0.2f} discr loss {met_disc_loss_valid.result():0.2f}        \", end=\"\")\n",
    "    \n",
    "        # Make Generations\n",
    "        generations = generate(seed_content_batch, seed_style_batch)\n",
    "        vis_fig = plot_style_transfered_time_series(seed_content_batch, seed_style_batch, generations, show=False, save_to=filename)\n",
    "        plot_buff = fig_to_buff(vis_fig)\n",
    "        log_train_losses(e, plot_buff)\n",
    "        log_valid_losses(e)\n",
    "        print()\n",
    "\n",
    "        if e == 0:\n",
    "            total_batch = i \n",
    "\n",
    "        \n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_sequence = generate(seed_content_batch, seed_style_batch)\n",
    "\n",
    "plot_style_transfered_time_series(seed_content_batch, seed_style_batch, generated_sequence, show=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
