{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Style Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from configs.SimulatedData import Proposed\n",
    "from dataset.tf_pipeline import make_train_valid_dset\n",
    "from datetime import datetime\n",
    "import io\n",
    "import json\n",
    "import os\n",
    "from utils.metric import signature_on_batch, signature_metric\n",
    "import mlflow\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs.\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized.\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Proposed()\n",
    "date_str = datetime.now().strftime('%Y-%m-%d_%H_%M_%S')\n",
    "\n",
    "D1_PATH = \"data/simulated_dataset/01 - Source Domain.h5\"\n",
    "D2_PATH = \"data/simulated_dataset/output_noise/0.75.h5\"\n",
    "EXPERIMENT_NAME = f\"{date_str} - Style Transfer Algorithm\"\n",
    "SAVE_FOLDER = f\"experiments_logs/{EXPERIMENT_NAME}\"\n",
    "\n",
    "SEQUENCE_LENGTH = config.sequence_lenght_in_sample\n",
    "GRANUARITY = config.granularity\n",
    "OVERLAP = config.overlap\n",
    "BS = config.batch_size\n",
    "EPOCHS = config.epochs\n",
    "NUM_SEQUENCE_TO_GENERATE = config.met_params.sequence_to_generate\n",
    "\n",
    "STYLE_VECTOR_SIZE = 16\n",
    "FEAT_WIENER = 2\n",
    "N_SAMPLE_WIENER = SEQUENCE_LENGTH//4\n",
    "NOISE_DIM= (N_SAMPLE_WIENER, FEAT_WIENER)\n",
    "N_VALIDATION_SEQUENCE = 500\n",
    "\n",
    "L_STYLE_GENERATION= 1\n",
    "L_RECONSTR= 0.1\n",
    "\n",
    "L_CONTENT= 1\n",
    "L_DIS= 1\n",
    "TRIPLET_R = 1\n",
    "L_TRIPLET= 1\n",
    "L_REALNESS= 1\n",
    "L_ADV= 1\n",
    "\n",
    "\n",
    "BASE_DIR = f\"logs/{EXPERIMENT_NAME}\"\n",
    "TRAIN_LOGS_DIR_PATH = f\"{BASE_DIR}/train\"\n",
    "VALID_LOGS_DIR_PATH = f\"{BASE_DIR}/valid\"\n",
    "GENERATION_LOG = f\"{BASE_DIR}/Generations\"\n",
    "os.makedirs(GENERATION_LOG)\n",
    "os.makedirs(SAVE_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "server_ip = \"192.168.210.102\"\n",
    "mlflow_port_number= \"5001\"\n",
    "\n",
    "mlflow.set_tracking_uri(f'http://{server_ip}:{mlflow_port_number}') \n",
    "exp = mlflow.get_experiment_by_name(\"Style Transfer Algorithm\")\n",
    "\n",
    "run = mlflow.start_run(run_name=date_str) \n",
    "mlflow.tensorflow.autolog()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_configuration():\n",
    "    d = {\n",
    "        \"d1_path\":D1_PATH,\n",
    "        \"d2_path\":D2_PATH,\n",
    "        \"SEQUENCE_LENGTH\":SEQUENCE_LENGTH,\n",
    "        \"granularity\":GRANUARITY,\n",
    "        \"overlap\":OVERLAP,\n",
    "        \"epochs\":EPOCHS,\n",
    "        \"batch_size\":BS,\n",
    "        \"style_vector_size\":STYLE_VECTOR_SIZE,\n",
    "        \"feat_wiener\":FEAT_WIENER,\n",
    "        \"n_sample_wiener\":N_SAMPLE_WIENER,\n",
    "        \"triplet_r\":TRIPLET_R,\n",
    "        \"n_validation_sequence\":N_VALIDATION_SEQUENCE,\n",
    "        \"l_style\":L_STYLE_GENERATION,\n",
    "        \"l_content\":L_CONTENT,\n",
    "        \"l_triplet\":L_TRIPLET,\n",
    "        \"l_realness\":L_REALNESS,\n",
    "        \"l_adv\":L_ADV,\n",
    "        \"l_reconstr\":L_RECONSTR,    \n",
    "        \"log_dir\":BASE_DIR\n",
    "    }\n",
    "\n",
    "    json_object = json.dumps(d)\n",
    "    mlflow.log_params({\n",
    "        \"SEQUENCE_LENGTH\":SEQUENCE_LENGTH,\n",
    "        \"granularity\":GRANUARITY,\n",
    "        \"overlap\":OVERLAP,\n",
    "        \"epochs\":EPOCHS,\n",
    "        \"batch_size\":BS,\n",
    "        \"style_vector_size\":STYLE_VECTOR_SIZE,\n",
    "        \"feat_wiener\":FEAT_WIENER,\n",
    "        \"n_sample_wiener\":N_SAMPLE_WIENER,\n",
    "        \"triplet_r\":TRIPLET_R,\n",
    "        \"l_style\":L_STYLE_GENERATION,\n",
    "        \"l_content\":L_CONTENT,\n",
    "        \"l_triplet\":L_TRIPLET,\n",
    "        \"l_realness\":L_REALNESS,\n",
    "        \"l_adv\":L_ADV,\n",
    "        \"l_reconstr\":L_RECONSTR,\n",
    "    })\n",
    "\n",
    "    with open(f\"{SAVE_FOLDER}/parameters.json\", \"w\") as outfile:\n",
    "        outfile.write(json_object)\n",
    "\n",
    "\n",
    "save_configuration()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_valid_split(df, train_size:float=.7):\n",
    "    dset_size = df.shape[0]\n",
    "    train_index = int(dset_size* train_size)\n",
    "\n",
    "    train_split = df.loc[:train_index]\n",
    "    valid_split = df.loc[train_index:]\n",
    "\n",
    "    return train_split, valid_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_d1= pd.read_hdf(D1_PATH).astype(np.float32)\n",
    "df_d2= pd.read_hdf(D2_PATH).astype(np.float32)\n",
    "\n",
    "df_d1 = df_d1.drop(columns=['labels'])\n",
    "df_d2 = df_d2.drop(columns=['labels'])\n",
    "\n",
    "N_FEAT = df_d1.shape[1]\n",
    "\n",
    "dset_d1_train, dset_d1_valid = make_train_valid_dset(\n",
    "    df_d1, \n",
    "    SEQUENCE_LENGTH, \n",
    "    GRANUARITY, \n",
    "    int(OVERLAP* SEQUENCE_LENGTH),\n",
    "    BS,\n",
    "    reduce_train_set=True\n",
    ")\n",
    "\n",
    "dset_d2_train, dset_d2_valid = make_train_valid_dset(\n",
    "    df_d2, \n",
    "    SEQUENCE_LENGTH, \n",
    "    GRANUARITY, \n",
    "    int(OVERLAP* SEQUENCE_LENGTH),\n",
    "    BS,\n",
    "    reduce_train_set=True\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Content Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define AdaIN Layers for Time Series\n",
    "class AdaIN(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(AdaIN, self).__init__()\n",
    "\n",
    "    def get_mean_std(self, x, eps=1e-5):\n",
    "        _mean, _variance = tf.nn.moments(x, axes=[1], keepdims=True)\n",
    "        standard_dev = tf.sqrt(_variance+ eps)\n",
    "        return _mean, standard_dev\n",
    "\n",
    "    def call(self, content_input, style_input):\n",
    "        # print(content_input.shape, style_input.shape)\n",
    "        content_mean, content_std = self.get_mean_std(content_input)\n",
    "        style_mean, style_std = self.get_mean_std(style_input)\n",
    "        adain_res =style_std* (content_input - content_mean) / content_std+ style_mean\n",
    "        return adain_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_content_encoder(seq_length:int, n_feat:int, feat_wiener:int):\n",
    "    _input = tf.keras.Input((seq_length, n_feat,))\n",
    "\n",
    "    x = tf.keras.layers.Conv1D(128, 5, 2, padding='same')(_input)\n",
    "    # x = tf.keras.layers.LeakyReLU()(x)\n",
    "    \n",
    "    x = tf.keras.layers.Conv1D(128, 5, 1, padding='same')(x)\n",
    "    # x = tf.keras.layers.LeakyReLU()(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv1D(128, 5, 2, padding='same')(x)\n",
    "    # x = tf.keras.layers.LeakyReLU()(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv1D(feat_wiener, 5, 1, padding='same')(x)\n",
    "    # x = tf.keras.layers.LeakyReLU()(x)\n",
    "\n",
    "    model = tf.keras.Model(_input, x)\n",
    "\n",
    "    return model\n",
    "\n",
    "def make_style_encoder(seq_length:int, n_feat:int, vector_output_shape:int):\n",
    "    _input = tf.keras.Input((seq_length, n_feat))\n",
    "\n",
    "    x = tf.keras.layers.Conv1D(128, 5, 2, padding='same')(_input)\n",
    "    # x = tf.keras.layers.LeakyReLU()(x)\n",
    "    \n",
    "    x = tf.keras.layers.Conv1D(128, 5, 1, padding='same')(x)\n",
    "    # x = tf.keras.layers.Conv1D(128, 5, 1, padding='same')(x)\n",
    "    # x = tf.keras.layers.LeakyReLU()(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv1D(128, 5, 2, padding='same')(x)\n",
    "    # x = tf.keras.layers.LeakyReLU()(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv1D(128, 5, 1, padding='same')(x)\n",
    "    # x = tf.keras.layers.Conv1D(128, 5, 1, padding='same')(x)\n",
    "    # x = tf.keras.layers.LeakyReLU()(x)\n",
    "\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    # x = tf.keras.layers.Dense(100, activation=\"relu\")(x)\n",
    "    # x = tf.keras.layers.Dense(50, activation=\"relu\")(x)\n",
    "    x = tf.keras.layers.Dense(vector_output_shape)(x)\n",
    "\n",
    "    model = tf.keras.Model(_input, x)\n",
    "    return model\n",
    "\n",
    "\n",
    "def make_decoder(n_sample_wiener:int, feat_wiener:int, style_vector_size:int, out_feat:int):\n",
    "    _content_input = tf.keras.Input((n_sample_wiener, feat_wiener))\n",
    "    _style_input = tf.keras.Input((style_vector_size, 1)) \n",
    "    _style_input = tf.keras.layers.Flatten()(_style_input)\n",
    "\n",
    "    stage_1_style = tf.keras.layers.Dense(16, name='1')(_style_input)\n",
    "    stage_1_style = tf.keras.layers.Reshape((16, 1))(stage_1_style)\n",
    "\n",
    "    stage_2_style = tf.keras.layers.Dense(32, name='2')(_style_input)\n",
    "    stage_2_style = tf.keras.layers.Reshape((32, 1))(stage_2_style)\n",
    "\n",
    "    x = AdaIN()(_content_input, stage_1_style)\n",
    "    x = tf.keras.layers.Conv1DTranspose(256, 5, 1, padding='same')(x)\n",
    "    # x = AdaIN()(x, stage_1_style)\n",
    "    x = tf.keras.layers.Conv1DTranspose(256, 5, 1, padding='same')(x)\n",
    "    # x = AdaIN()(x, stage_1_style)\n",
    "    x = tf.keras.layers.Conv1DTranspose(256, 5, 2, padding='same')(x)\n",
    "    x = tf.keras.layers.LeakyReLU()(x)\n",
    "\n",
    "    x = AdaIN()(x, stage_2_style)\n",
    "    x = tf.keras.layers.Conv1DTranspose(128, 5, 1, padding='same')(x)\n",
    "    # x = AdaIN()(x, stage_2_style)\n",
    "    x = tf.keras.layers.Conv1DTranspose(128, 5, 1, padding='same')(x)\n",
    "    # x = AdaIN()(x, stage_2_style)\n",
    "    x = tf.keras.layers.Conv1DTranspose(out_feat, 5, 2, padding='same')(x)\n",
    "    x = tf.keras.layers.LeakyReLU()(x)\n",
    "\n",
    "    model = tf.keras.Model([_content_input, _style_input], x)\n",
    "    return model\n",
    "\n",
    "\n",
    "def make_discriminator(seq_length:int, n_feat:int):\n",
    "    _input = tf.keras.Input((seq_length, n_feat))\n",
    "    x = tf.keras.layers.Conv1D(16, 5, 2, padding='same')(_input)\n",
    "    x = tf.keras.layers.LeakyReLU()(x)\n",
    "    # x = tf.keras.layers.Dropout(0.3)(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv1D(16, 5, 2, padding='same')(x)\n",
    "    x = tf.keras.layers.LeakyReLU()(x)\n",
    "    # x = tf.keras.layers.Dropout(0.3)(x)\n",
    "\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    _output = tf.keras.layers.Dense(1)(x)\n",
    "\n",
    "    model = tf.keras.Model(_input, _output)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_encoder = make_content_encoder(SEQUENCE_LENGTH, df_d1.shape[1], FEAT_WIENER)\n",
    "# content_encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_encoder = make_style_encoder(SEQUENCE_LENGTH, df_d1.shape[1], STYLE_VECTOR_SIZE)\n",
    "style_encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = make_decoder(N_SAMPLE_WIENER, FEAT_WIENER, STYLE_VECTOR_SIZE ,df_d1.shape[1])\n",
    "# decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_discriminator = make_discriminator(SEQUENCE_LENGTH, df_d1.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleTransferModel(tf.keras.Model):\n",
    "    def __init__(self, seq_length:int, n_feat:int, style_vector_shape:int, n_sample_wiener:int, feat_wiener:int, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.content_encoder= make_content_encoder(seq_length, n_feat, feat_wiener)\n",
    "        self.style_encoder = make_style_encoder(seq_length, n_feat, style_vector_shape)\n",
    "        self.decoder = make_decoder(n_sample_wiener, feat_wiener, style_vector_shape, n_feat)\n",
    "\n",
    "    def __call__(self, content_sequence, style_sequence):\n",
    "        encoded_content = self.content_encoder(content_sequence)\n",
    "        encoded_style = self.style_encoder(style_sequence)\n",
    "\n",
    "        generated_sequence = self.decoder([encoded_content, encoded_style])\n",
    "\n",
    "        return generated_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(content_batch, style_batch):\n",
    "    content = content_encoder(content_batch, training=False)\n",
    "    style = style_encoder(style_batch, training=False)\n",
    "    generated = decoder([content, style], training=False)\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_content_batch = next(iter(dset_d1_valid))\n",
    "seed_style_batch = next(iter(dset_d2_valid))\n",
    "\n",
    "generated_sequence = generate(seed_content_batch, seed_style_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def plot_generated_sequence2(content_sequences, style_sequences, show=False):\n",
    "\n",
    "    # Compute what we need\n",
    "    content_of_content = content_encoder(content_sequences, training=False)\n",
    "    style_of_content = style_encoder(content_sequences, training=False)\n",
    "\n",
    "    content_of_style= content_encoder(style_sequences, training=False)\n",
    "    style_of_style= style_encoder(style_sequences, training=False)\n",
    "\n",
    "    generated_sequences = decoder([content_of_content, style_of_style], training=False)\n",
    "\n",
    "    content_of_generated = content_encoder(generated_sequences, training=False)\n",
    "    style_of_generated = style_encoder(generated_sequences, training=False)\n",
    "\n",
    "    # Reduce the Style Vector for visualization purposes.\n",
    "    pca = PCA(2)\n",
    "    all_together = np.vstack([style_of_content, style_of_style, style_of_generated])\n",
    "\n",
    "    pca.fit(all_together)\n",
    "\n",
    "    reduced_style_style = pca.transform(style_of_style)\n",
    "    reduced_style_content=pca.transform(style_of_content)\n",
    "    reduced_style_generations=pca.transform(style_of_generated)\n",
    "\n",
    "    all_values = np.array([content_sequences, style_sequences, generated_sequences])\n",
    "    _min, _max = np.min(all_values)-1, np.max(all_values)+ 1\n",
    "\n",
    "\n",
    "    fig= plt.figure(figsize=(18, 8))\n",
    "    spec= fig.add_gridspec(3, 6)\n",
    "\n",
    "    ax00 = fig.add_subplot(spec[0, :2])\n",
    "    ax00.set_title('Content Sequence.')\n",
    "    ax00.plot(content_sequences[0, :, :2], label=df_d1.columns[:2])\n",
    "    ax00.set_ylim(_min, _max)\n",
    "    ax00.grid(True)\n",
    "    ax00.legend()\n",
    "\n",
    "    ax10 = fig.add_subplot(spec[1, :2])\n",
    "    ax10.plot(content_sequences[0, :, 2:], label=df_d1.columns[2:])\n",
    "    ax10.set_ylim(_min, _max)\n",
    "    ax10.grid(True)\n",
    "    ax10.legend()\n",
    "\n",
    "#######\n",
    "    ax01 = fig.add_subplot(spec[0, 2:4])\n",
    "    ax01.set_title('Style Sequence.')\n",
    "    ax01.plot(style_sequences[0, :, :2])\n",
    "    ax01.set_ylim(_min, _max)\n",
    "    ax01.grid(True)\n",
    "\n",
    "    ax11 = fig.add_subplot(spec[1, 2:4])\n",
    "    ax11.plot(style_sequences[0, :, 2:])\n",
    "    ax11.set_ylim(_min, _max)\n",
    "    ax11.grid(True)\n",
    "#######\n",
    "    ax02 = fig.add_subplot(spec[0, 4:])\n",
    "    ax02.set_title('Generated Sequence.')\n",
    "    ax02.plot(generated_sequences[0, :, :2])\n",
    "    ax02.set_ylim(_min, _max)\n",
    "    ax02.grid(True) \n",
    "\n",
    "    ax12 = fig.add_subplot(spec[1, 4:])\n",
    "    ax12.plot(generated_sequences[1, :, 2:])\n",
    "    ax12.set_ylim(_min, _max)\n",
    "    ax12.grid(True) \n",
    "\n",
    "\n",
    "\n",
    "#####\n",
    "    ax10 = fig.add_subplot(spec[2, :3])\n",
    "    ax10.set_title('Content Space.')\n",
    "    ax10.scatter(content_of_content[0, :, 0], content_of_content[0, :, 1],  label='content of content.')\n",
    "    ax10.scatter(content_of_style[0, :, 0], content_of_style[0, :, 1], label='content of style.')\n",
    "    ax10.scatter(content_of_generated[0, :, 0], content_of_generated[0, :, 1],  label='content of generated.')\n",
    "    ax10.grid(True)\n",
    "    ax10.legend()\n",
    "\n",
    "    ax11 = fig.add_subplot(spec[2, 3:])\n",
    "    ax11.set_title('Style Space, Reduced with PCA.')\n",
    "    ax11.scatter(reduced_style_content[:, 0], reduced_style_content[:, 1], label='style of content.')\n",
    "    ax11.scatter(reduced_style_style[:, 0], reduced_style_style[:, 1], label='style of style.')\n",
    "    ax11.scatter(reduced_style_generations[:, 0], reduced_style_generations[:, 1], label='style of genrations.')\n",
    "\n",
    "    ax11.grid(True)\n",
    "    ax11.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    return fig\n",
    "\n",
    "plot_generated_sequence2(seed_content_batch, seed_style_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fig_to_buff(fig):\n",
    "    buf = io.BytesIO()\n",
    "    fig.savefig(buf, format='png')\n",
    "    buf.seek(0)\n",
    "    plt.close(fig)\n",
    "    return buf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "def recontruction_loss(true:tf.Tensor, generated:tf.Tensor):\n",
    "    diff = generated- true\n",
    "    result = tf.math.reduce_mean(tf.square(diff))\n",
    "    return result\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "\n",
    "\n",
    "def fixed_point_content(encoded_content_real, encoded_content_fake):\n",
    "    diff = encoded_content_fake- encoded_content_real\n",
    "    return tf.reduce_mean(tf.square(diff))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard Logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Metrics\n",
    "met_generator_train = tf.keras.metrics.Mean(name=\"Train generator Loss\")\n",
    "met_style_encoder_train = tf.keras.metrics.Mean(name=\"Train Style Encoder Loss\")\n",
    "met_content_encoder_train= tf.keras.metrics.Mean(name=\"Train Content Encoder Loss\")\n",
    "met_disc_loss_train= tf.keras.metrics.Mean(name=\"Train Discriminatir Loss\")\n",
    "met_style_reconstruction = tf.keras.metrics.Mean(name=\"Style Reconstruction\")\n",
    "\n",
    "# Valid Metrics\n",
    "met_generator_valid = tf.keras.metrics.Mean(name=\"valid generator Loss\")\n",
    "met_style_encoder_valid = tf.keras.metrics.Mean(name=\"valid Style Encoder Loss\")\n",
    "met_content_encoder_valid= tf.keras.metrics.Mean(name=\"valid Content Encoder Loss\")\n",
    "met_disc_loss_valid= tf.keras.metrics.Mean(name=\"valid Discriminatir Loss\")\n",
    "met_correlation_metric_valid = tf.keras.metrics.Mean(name=\"Correlation Metric\")\n",
    "met_style_reconstruction_valid = tf.keras.metrics.Mean(name=\"Style Reconstruction\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SUMMARY_WRITER = tf.summary.create_file_writer(TRAIN_LOGS_DIR_PATH)\n",
    "VALID_SUMMARY_WRITER = tf.summary.create_file_writer(VALID_LOGS_DIR_PATH)\n",
    "\n",
    "def log_train_losses(epoch, plot_buf):\n",
    "    image = tf.image.decode_png(plot_buf.getvalue(), channels=4)\n",
    "    image = tf.expand_dims(image, 0)\n",
    "\n",
    "    with TRAIN_SUMMARY_WRITER.as_default():\n",
    "        tf.summary.scalar(\"Generator Loss\", met_generator_train.result(), step=epoch)\n",
    "        tf.summary.scalar(\"Style Loss\", met_style_encoder_train.result(), step=epoch)\n",
    "        tf.summary.scalar(\"Content Loss\", met_content_encoder_train.result(), step=epoch)\n",
    "        tf.summary.scalar(\"Discriminator Loss\", met_disc_loss_train.result(), step=epoch)\n",
    "        tf.summary.scalar(\"Style contrain on the generator\", met_style_reconstruction.result(), step=epoch)\n",
    "        \n",
    "        tf.summary.image(\"Training Generations\", image, step=epoch)\n",
    "\n",
    "\n",
    "def log_valid_losses(epoch):\n",
    "    with VALID_SUMMARY_WRITER.as_default():\n",
    "        tf.summary.scalar(\"Generator Loss\", met_generator_valid.result(), step=epoch)\n",
    "        tf.summary.scalar(\"Style Loss\", met_style_encoder_valid.result(), step=epoch)\n",
    "        tf.summary.scalar(\"Content Loss\", met_content_encoder_valid.result(), step=epoch)\n",
    "        tf.summary.scalar(\"Discriminator Loss\", met_disc_loss_valid.result(), step=epoch)\n",
    "        tf.summary.scalar(\"Correlation Metric\", met_correlation_metric_valid.result(), step=epoch)\n",
    "        tf.summary.scalar(\"Style contrain on the generator\", met_style_reconstruction_valid.result(), step=epoch)\n",
    "        \n",
    "\n",
    "\n",
    "def reset_metric_states():\n",
    "    met_generator_train.reset_states()\n",
    "    met_style_encoder_train.reset_states()\n",
    "    met_content_encoder_train.reset_states()\n",
    "    met_disc_loss_train.reset_states()\n",
    "\n",
    "def reset_valid_states():\n",
    "    met_generator_valid.reset_states()\n",
    "    met_style_encoder_valid.reset_states()\n",
    "    met_content_encoder_valid.reset_states()\n",
    "    met_disc_loss_valid.reset_states()\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate model for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_encoder = make_content_encoder(SEQUENCE_LENGTH, df_d1.shape[1], FEAT_WIENER)\n",
    "style_encoder = make_style_encoder(SEQUENCE_LENGTH, df_d1.shape[1], STYLE_VECTOR_SIZE)\n",
    "decoder = make_decoder(N_SAMPLE_WIENER, FEAT_WIENER, STYLE_VECTOR_SIZE ,df_d1.shape[1])\n",
    "\n",
    "opt_content_encoder = tf.keras.optimizers.RMSprop(learning_rate=1e-6)\n",
    "opt_style_encoder = tf.keras.optimizers.RMSprop(learning_rate=2e-6)\n",
    "opt_decoder = tf.keras.optimizers.RMSprop(learning_rate=1e-3)\n",
    "opt_discr = tf.keras.optimizers.RMSprop(learning_rate=1e-6)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "indexes = np.array(list(product(range(BS), range(BS))))\n",
    "other_index = np.arange(BS)* BS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_indexes = np.array([ i* BS+i for i in range(BS) for _ in range(BS) ])\n",
    "pos_indexes = np.array([ BS*j + i for i in range(BS) for j in range(BS)])\n",
    "neg_indexes = np.array([ (j*BS + (i+1)%BS) for i in range(BS) for j in range(BS)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_style_voctor_for_dis_loss(style_vector:tf.Tensor, batch_size:int):\n",
    "    anchor_indexes = np.array([ i* batch_size+i for i in range(batch_size) for _ in range(batch_size-1) ])\n",
    "    return tf.gather(style_vector, anchor_indexes)\n",
    "\n",
    "def get_anchor_positive_negative_from_batch(style_from_style_ts:tf.Tensor, style_of_generations:tf.Tensor, batch_size:int):\n",
    "    anchor_indexes = np.array([ i* batch_size+i for i in range(batch_size) for _ in range(batch_size) ])\n",
    "    pos_indexes = np.array([ batch_size*j + i for i in range(batch_size) for j in range(batch_size)])\n",
    "    neg_indexes = np.array([ (j*batch_size + (i+1)%batch_size) for i in range(batch_size) for j in range(batch_size)])\n",
    "\n",
    "    # Anchor are for example:\n",
    "    # [(c0, s0), (c0, s0), \n",
    "    # (c1, s1), (c1, s1), ...]\n",
    "    anchors = tf.gather(style_from_style_ts, anchor_indexes)\n",
    "    # Different content, Same Style:\n",
    "    # [(c1, s0), (c2, s0),(c2, s0),\n",
    "    #  (c1, s1), (c2, s1), (c3, s1)...]\n",
    "    # E_s(G(x, y1))\n",
    "    pos_vector= tf.gather(style_of_generations, pos_indexes)\n",
    "    # Same content but different style\n",
    "    # [(c1, s1), (c2, s1),(c2, s1),\n",
    "    #  (c1, s2), (c2, s2), (c3, s2)...]\n",
    "    # # E_s(G(x, y2)) \n",
    "    neg_vector = tf.gather(style_of_generations, neg_indexes)\n",
    "\n",
    "    return anchors, pos_vector, neg_vector\n",
    "\n",
    "def get_dissantanglement_loss_component(style_of_generations, style_of_style, batch_size:int):\n",
    "    anchor_indexes = np.array([ i* batch_size+i for i in range(batch_size) for _ in range(batch_size-1) ])\n",
    "    pos_indexes = np.array([ batch_size*j + i for i in range(batch_size) for j in range(batch_size) if i !=j ])\n",
    "\n",
    "    es_y = tf.gather(style_of_style, anchor_indexes)\n",
    "    es_x1_y = tf.gather(style_of_generations, anchor_indexes)\n",
    "    es_x2_y = tf.gather(style_of_generations, pos_indexes)\n",
    "\n",
    "    return es_y, es_x1_y, es_x2_y\n",
    "\n",
    "\n",
    "def l2(x:tf.Tensor, y:tf.Tensor):\n",
    "    diff = tf.square(y- x)\n",
    "    _distance = tf.reduce_sum(diff, axis=-1)\n",
    "    return _distance\n",
    "\n",
    "\n",
    "def fixed_point_content(encoded_content_real, encoded_content_fake):\n",
    "    diff = l2(encoded_content_real, encoded_content_fake)\n",
    "    return tf.reduce_mean(tf.square(diff))\n",
    "\n",
    "def fixed_point_triplet_style_loss(anchor_encoded_style, positive_encoded_style, negative_encoded_style):\n",
    "    # shape: [BS, Style_length]\n",
    "    positive_distance = l2(negative_encoded_style, anchor_encoded_style)\n",
    "    negative_distance = l2(positive_encoded_style, anchor_encoded_style)\n",
    "\n",
    "    triplet = TRIPLET_R+ positive_distance- negative_distance\n",
    "    zeros = tf.zeros_like(triplet)\n",
    "    triplet = tf.math.maximum(triplet, zeros)\n",
    "\n",
    "    loss = tf.reduce_mean(triplet)\n",
    "    return loss\n",
    "\n",
    "def fixed_point_disentanglement(\n",
    "        es_x1_y:tf.Tensor, \n",
    "        es_x2_y:tf.Tensor, \n",
    "        es_y:tf.Tensor\n",
    "        ):\n",
    "\n",
    "    diff1 = l2(es_x1_y, es_x2_y)\n",
    "    diff2 = l2(es_x1_y, es_y)\n",
    "\n",
    "    loss = diff1- diff2\n",
    "    zeros = tf.zeros_like(loss)\n",
    "    loss = tf.math.maximum(loss, zeros)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def style_constrain_loss(style_of_style_batch, style_of_generation):\n",
    "    diff = l2(style_of_style_batch, style_of_generation)\n",
    "    diff = tf.reduce_mean(diff)\n",
    "    return diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function\n",
    "def train_step(content_batch:tf.Tensor, style_batch:tf.Tensor):\n",
    "    lambda_style_generations = L_STYLE_GENERATION\n",
    "    lambda_reconstr= L_RECONSTR\n",
    "    lambda_realness= L_REALNESS\n",
    "    lambda_adv= L_ADV\n",
    "    lambda_content= L_CONTENT\n",
    "    lambda_triplet= L_TRIPLET\n",
    "    lambda_dis= L_DIS\n",
    "\n",
    "    with tf.GradientTape() as content_tape, tf.GradientTape() as style_tape, tf.GradientTape() as decoder_tape, tf.GradientTape() as discr_tape:\n",
    "        extended_content_images = tf.gather(content_batch, indexes[:, 0])\n",
    "        extended_style_images = tf.gather(style_batch, indexes[:, 1])\n",
    "        \n",
    "        # Get the content form the content batch\n",
    "        content_of_content = content_encoder(extended_content_images, training=True)\n",
    "        # Get the Style from the style batch\n",
    "        style_of_style = style_encoder(extended_style_images, training=True)\n",
    "\n",
    "        # Generate the time series given the content and the style.\n",
    "        generated_ts = decoder([content_of_content, style_of_style], training=True)\n",
    "\n",
    "        # Get the content and style form generated data\n",
    "        content_of_generations = content_encoder(generated_ts, training=True)\n",
    "        style_of_generations = style_encoder(generated_ts, training=True)\n",
    "\n",
    "        # Get content image stylized with a image from the style batch\n",
    "        # E.g.  (c1, s1), (c2, s2)... (cn, sn)\n",
    "        reduced_stylized = tf.gather(generated_ts, other_index)\n",
    "\n",
    "        crit_on_fake = global_discriminator(reduced_stylized, training=True)\n",
    "        crit_on_real = global_discriminator(content_batch, training=True)\n",
    "\n",
    "\n",
    "\n",
    "        # Compute the generator loss \n",
    "        reconstr_from_content = recontruction_loss(extended_content_images, generated_ts) \n",
    "        realness = generator_loss(reduced_stylized)\n",
    "        content_similarity = fixed_point_content(content_of_content, content_of_generations)\n",
    "        constrain_on_style_generations = style_constrain_loss(style_of_style, style_of_generations)\n",
    "\n",
    "        global_dicriminator_loss = discriminator_loss(crit_on_real, crit_on_fake)\n",
    "        \n",
    "\n",
    "        es_y, es_x1_y, es_x2_y =  get_dissantanglement_loss_component(style_of_generations, style_of_style, BS)\n",
    "        dis_loss = fixed_point_disentanglement(es_y, es_x1_y, es_x2_y)\n",
    "\n",
    "        es_y1, es_x_y1, es_x_y2 = get_anchor_positive_negative_from_batch(style_of_style, style_of_generations, BS)\n",
    "        triplet_style = fixed_point_triplet_style_loss(es_y1, es_x_y1, es_x_y2)\n",
    "\n",
    "        d_loss = lambda_adv* global_dicriminator_loss\n",
    "        content_encoder_loss = lambda_content* content_similarity\n",
    "        style_encoder_loss = lambda_triplet* triplet_style + lambda_dis* dis_loss\n",
    "        g_loss = lambda_reconstr* reconstr_from_content+ lambda_realness* realness# +lambda_style_generations* constrain_on_style_generations\n",
    "\n",
    "    content_grad=content_tape.gradient(content_encoder_loss, content_encoder.trainable_variables)\n",
    "    style_grad = style_tape.gradient(style_encoder_loss, style_encoder.trainable_variables)\n",
    "    decoder_grad = decoder_tape.gradient(g_loss, decoder.trainable_variables)\n",
    "    discr_grads = discr_tape.gradient(d_loss, global_discriminator.trainable_variables)\n",
    "\n",
    "    opt_content_encoder.apply_gradients(zip(content_grad, content_encoder.trainable_variables))\n",
    "    opt_style_encoder.apply_gradients(zip(style_grad, style_encoder.trainable_variables))\n",
    "    opt_decoder.apply_gradients(zip(decoder_grad, decoder.trainable_variables))\n",
    "    opt_discr.apply_gradients(zip(discr_grads, global_discriminator.trainable_variables))\n",
    "\n",
    "    met_generator_train(g_loss)\n",
    "    met_style_encoder_train(style_encoder_loss)\n",
    "    met_content_encoder_train(content_encoder_loss)\n",
    "    met_disc_loss_train(d_loss)\n",
    "    met_style_reconstruction(constrain_on_style_generations)\n",
    "\n",
    "@tf.function\n",
    "def valid_step(content_batch:tf.Tensor, style_batch:tf.Tensor):\n",
    "    lambda_style_generations = L_STYLE_GENERATION\n",
    "    lambda_reconstr= L_RECONSTR\n",
    "    lambda_realness= L_REALNESS\n",
    "    lambda_adv= L_ADV\n",
    "    lambda_content= L_CONTENT\n",
    "    lambda_triplet= L_TRIPLET\n",
    "    lambda_dis= L_DIS\n",
    "\n",
    "    extended_content_images = tf.gather(content_batch, indexes[:, 0])\n",
    "    extended_style_images = tf.gather(style_batch, indexes[:, 1])\n",
    "    \n",
    "    # Get the content form the content batch\n",
    "    content_of_content = content_encoder(extended_content_images, training=False)\n",
    "    # Get the Style from the style batch\n",
    "    style_of_style = style_encoder(extended_style_images, training=False)\n",
    "\n",
    "    # Generate the time series given the content and the style.\n",
    "    generated_ts = decoder([content_of_content, style_of_style], training=False)\n",
    "\n",
    "    # Get the content and style form generated data\n",
    "    content_of_generations = content_encoder(generated_ts, training=False)\n",
    "    style_of_generations = style_encoder(generated_ts, training=False)\n",
    "\n",
    "    # Get content image stylized with a image from the style batch\n",
    "    # E.g.  (c1, s1), (c2, s2)... (cn, sn)\n",
    "    reduced_stylized = tf.gather(generated_ts, other_index)\n",
    "\n",
    "    crit_on_fake = global_discriminator(reduced_stylized, training=False)\n",
    "    crit_on_real = global_discriminator(content_batch, training=False)\n",
    "\n",
    "    reconstr_from_content = recontruction_loss(extended_content_images, generated_ts) \n",
    "    realness = generator_loss(reduced_stylized)\n",
    "    constrain_on_style_generations = style_constrain_loss(style_of_style, style_of_generations)\n",
    "\n",
    "    global_dicriminator_loss = discriminator_loss(crit_on_real, crit_on_fake)\n",
    "    content_similarity = fixed_point_content(content_of_content, content_of_generations)\n",
    "\n",
    "    anchors, positive_vectors, negative_vectors = get_anchor_positive_negative_from_batch(style_of_style, style_of_generations, BS)\n",
    "    es_y, es_x1_y, es_x2_y =  get_dissantanglement_loss_component(style_of_generations, style_of_style, BS)\n",
    "\n",
    "    # sorted_styles = get_style_voctor_for_dis_loss(style_of_style, BS)\n",
    "    triplet_style = fixed_point_triplet_style_loss(anchors, positive_vectors, negative_vectors)\n",
    "\n",
    "    dis_loss = fixed_point_disentanglement(es_y, es_x1_y, es_x2_y)\n",
    "\n",
    "    g_loss = lambda_reconstr* reconstr_from_content+ lambda_realness* realness+ lambda_style_generations* constrain_on_style_generations\n",
    "    d_loss = lambda_adv* global_dicriminator_loss\n",
    "    content_encoder_loss = lambda_content* content_similarity\n",
    "    style_encoder_loss =  lambda_triplet* triplet_style# + lambda_dis* dis_loss \n",
    "\n",
    "    met_generator_valid(g_loss)\n",
    "    met_style_encoder_valid(style_encoder_loss)\n",
    "    met_content_encoder_valid(content_encoder_loss)\n",
    "    met_disc_loss_valid(d_loss)\n",
    "    met_style_reconstruction_valid(constrain_on_style_generations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    total_batch = \"?\"\n",
    "    for e in range(EPOCHS):\n",
    "        reset_metric_states()\n",
    "        reset_valid_states()\n",
    "        \n",
    "        print(\"[+] Train Step...\")\n",
    "        for i, (content_batch, style_batch) in enumerate(zip(dset_d1_train, dset_d2_train)):\n",
    "            train_step(content_batch, style_batch)\n",
    "            print(f\"\\r e:{e}/{EPOCHS}; {i}/{total_batch}. G_loss {met_generator_train.result():0.2f} style loss {met_style_encoder_train.result():0.2f} content loss {met_content_encoder_train.result():0.2f} discr loss {met_disc_loss_train.result():0.2f}        \", end=\"\")\n",
    "            # return\n",
    "\n",
    "        print()\n",
    "        print(\"[+] Validation Step...\")\n",
    "        for vb, (content_batch, style_batch) in enumerate(zip(dset_d1_valid, dset_d2_valid)):\n",
    "            valid_step(content_batch, style_batch)\n",
    "            print(f\"\\r e:{e}/{EPOCHS}; {vb+1}/500. G_loss {met_generator_valid.result():0.2f} style loss {met_style_encoder_valid.result():0.2f} content loss {met_content_encoder_valid.result():0.2f} discr loss {met_disc_loss_valid.result():0.2f}        \", end=\"\")\n",
    "    \n",
    "        # Make Generations\n",
    "        generations = generate(seed_content_batch, seed_style_batch)\n",
    "\n",
    "        vis_fig = plot_generated_sequence2(seed_content_batch, seed_style_batch)\n",
    "        plot_buff = fig_to_buff(vis_fig)\n",
    "        \n",
    "        style_signature = signature_on_batch(seed_style_batch, config.met_params.ins, config.met_params.outs, config.met_params.signature_length)\n",
    "        generated_signature = signature_on_batch(generations, config.met_params.ins, config.met_params.outs, config.met_params.signature_length)\n",
    "        signature_difference = signature_metric(style_signature, generated_signature)\n",
    "        met_correlation_metric_valid(signature_difference)\n",
    "\n",
    "        print(f\"\\n[+] Signature Difference {signature_difference:0.2f}\")\n",
    "\n",
    "        log_train_losses(e, plot_buff)\n",
    "        log_valid_losses(e)\n",
    "        print()\n",
    "\n",
    "        if e == 0:\n",
    "            total_batch = i \n",
    "\n",
    "        \n",
    "train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_on_dataset():\n",
    "    _df_d1= pd.read_hdf(D1_PATH).astype(np.float32)\n",
    "    _df_d2= pd.read_hdf(D2_PATH).astype(np.float32)\n",
    "\n",
    "    _df_d1 = _df_d1.drop(columns=['labels'])\n",
    "    _df_d2 = _df_d2.drop(columns=['labels'])\n",
    "\n",
    "    _, _dset_d1_valid = make_train_valid_dset(\n",
    "        _df_d1, \n",
    "        SEQUENCE_LENGTH, \n",
    "        GRANUARITY, \n",
    "        int(OVERLAP* SEQUENCE_LENGTH),\n",
    "        BS,\n",
    "        valid_set_size=500\n",
    "    )\n",
    "\n",
    "    _, _dset_d2_valid = make_train_valid_dset(\n",
    "        _df_d2, \n",
    "        SEQUENCE_LENGTH, \n",
    "        GRANUARITY, \n",
    "        int(OVERLAP* SEQUENCE_LENGTH),\n",
    "        BS,\n",
    "        valid_set_size=500\n",
    "    )\n",
    "\n",
    "    d1_sequence, d2_sequence = next(iter(_dset_d1_valid)), next(iter(_dset_d2_valid))\n",
    "\n",
    "    generated_batch = generate(d1_sequence, d2_sequence)\n",
    "\n",
    "    real_validation_signature = signature_on_batch(d2_sequence, config.met_params.ins, config.met_params.outs, config.met_params.signature_length)\n",
    "    generated_signature = signature_on_batch(generated_batch, config.met_params.ins, config.met_params.outs, config.met_params.signature_length)\n",
    "\n",
    "    sig_difference = signature_metric(real_validation_signature, generated_signature)\n",
    "\n",
    "    return sig_difference"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TSTR Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.NaiveClassifier import make_naive_discriminator\n",
    "\n",
    "def make_classification_dataset(df:pd.DataFrame, overlap:float=.5):\n",
    "\n",
    "    dset_train, dset_valid = make_train_valid_dset(\n",
    "        df, \n",
    "        SEQUENCE_LENGTH, \n",
    "        GRANUARITY, \n",
    "        int(overlap* SEQUENCE_LENGTH),\n",
    "        BS,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    # Extract labels from dataset.\n",
    "    dset_train = dset_train.map(lambda s: (s[:, :, :-1], s[:, int(SEQUENCE_LENGTH//2), -1]))\n",
    "    dset_valid = dset_valid.map(lambda s: (s[:, :, :-1], s[:, int(SEQUENCE_LENGTH//2), -1]))\n",
    "\n",
    "    return dset_train, dset_valid\n",
    "\n",
    "def stylize_dataset(content_dataset:tf.data.Dataset, style_dataset:tf.data.Dataset):\n",
    "    mixted_dataset = tf.data.Dataset.zip((content_dataset, style_dataset))\n",
    "    stylized_dataset = mixted_dataset.map(lambda _cont, _sty: (generate(_cont[0], _sty[0]), _cont[1]))\n",
    "    return stylized_dataset\n",
    "\n",
    "\n",
    "def train_naive_classifier(train_classif_dataset:tf.data.Dataset, valid_classif_dataset:tf.data.Dataset)->tf.keras.models.Model:\n",
    "    naive_model = make_naive_discriminator((SEQUENCE_LENGTH, N_FEAT), 5)\n",
    "    _epochs = 1\n",
    "\n",
    "    naive_model.fit(train_classif_dataset, validation_data=valid_classif_dataset, epochs=_epochs)\n",
    "\n",
    "    return naive_model\n",
    "\n",
    "def tstr_test():\n",
    "    _df_d1= pd.read_hdf(D1_PATH).astype(np.float32)\n",
    "    _df_d2= pd.read_hdf(D2_PATH).astype(np.float32)\n",
    "\n",
    "    _dset_d1_train, _dset_d1_valid = make_classification_dataset(_df_d1)\n",
    "    _dset_d2_train, _dset_d2_valid = make_classification_dataset(_df_d2)\n",
    "\n",
    "    # Generate the 'stylized' dataset \n",
    "    _dset_12_train = stylize_dataset(_dset_d1_train, _dset_d2_train)\n",
    "    _dset_12_valid = stylize_dataset(_dset_d1_valid, _dset_d2_valid)\n",
    "\n",
    "    # Train a classifier on the stylized dataset.\n",
    "    naive_12_model = train_naive_classifier(_dset_12_train, _dset_12_valid)\n",
    "    eval_on_generated = naive_12_model.evaluate(_dset_12_valid)[1]\n",
    "\n",
    "    # Train on the real Dataset.\n",
    "    naive_on_real = train_naive_classifier(_dset_d2_train, _dset_d2_valid)\n",
    "    eval_on_real = naive_on_real.evaluate(_dset_d2_valid)[1]\n",
    "\n",
    "    return eval_on_generated, eval_on_real"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_results(folder_to_save:str, acc_real, acc_generated, sig_diff):\n",
    "    # Create the folder.\n",
    "    file_path = f\"{folder_to_save}/scores.xlsx\"\n",
    "    df_results = pd.DataFrame([[acc_real, acc_generated, sig_diff]], columns=[\"Acc on Real\", \"Acc on Generated\", \"Signature Difference\"])\n",
    "    df_results.to_excel(file_path)\n",
    "\n",
    "acc_generated, acc_real = tstr_test()\n",
    "signature_diff = correlation_on_dataset()\n",
    "\n",
    "log_results(SAVE_FOLDER, acc_real, acc_generated, signature_diff)\n",
    "save_configuration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
