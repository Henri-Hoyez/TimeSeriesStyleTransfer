{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Style Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from configs.SimulatedData import Proposed\n",
    "from dataset.tf_pipeline import make_train_valid_dset\n",
    "from datetime import datetime\n",
    "import io\n",
    "import json\n",
    "import os\n",
    "from utils.metric import signature_on_batch, signature_metric\n",
    "import mlflow\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs.\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized.\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Proposed()\n",
    "date_str = datetime.now().strftime('%Y-%m-%d_%H_%M_%S')\n",
    "\n",
    "D1_PATH = \"data/simulated_dataset/01 - Source Domain.h5\"\n",
    "D2_PATH = \"data/simulated_dataset/output_noise/0.75.h5\"\n",
    "EXPERIMENT_NAME = f\"{date_str} - Style Transfer Algorithm\"\n",
    "SAVE_FOLDER = f\"experiments_logs/{EXPERIMENT_NAME}\"\n",
    "\n",
    "SEQUENCE_LENGTH = config.sequence_lenght_in_sample\n",
    "GRANUARITY = config.granularity\n",
    "OVERLAP = config.overlap\n",
    "BS = config.batch_size\n",
    "EPOCHS = 15# config.epochs\n",
    "NUM_SEQUENCE_TO_GENERATE = config.met_params.sequence_to_generate\n",
    "\n",
    "STYLE_VECTOR_SIZE = 16\n",
    "FEAT_WIENER = 2\n",
    "N_SAMPLE_WIENER = SEQUENCE_LENGTH//4\n",
    "NOISE_DIM= (N_SAMPLE_WIENER, FEAT_WIENER)\n",
    "N_VALIDATION_SEQUENCE = 500\n",
    "\n",
    "L_STYLE_GENERATION= 0.01\n",
    "L_RECONSTR= 0.1\n",
    "\n",
    "L_CONTENT= 1\n",
    "L_DIS= 0\n",
    "TRIPLET_R = 1\n",
    "L_TRIPLET= 1\n",
    "L_REALNESS= 1\n",
    "L_ADV= 1\n",
    "\n",
    "\n",
    "BASE_DIR = f\"logs/{EXPERIMENT_NAME}\"\n",
    "TRAIN_LOGS_DIR_PATH = f\"{BASE_DIR}/train\"\n",
    "VALID_LOGS_DIR_PATH = f\"{BASE_DIR}/valid\"\n",
    "GENERATION_LOG = f\"{BASE_DIR}/Generations\"\n",
    "os.makedirs(GENERATION_LOG)\n",
    "os.makedirs(SAVE_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# server_ip = \"192.168.210.102\"\n",
    "# mlflow_port_number= \"5001\"\n",
    "\n",
    "# mlflow.set_tracking_uri(f'http://{server_ip}:{mlflow_port_number}') \n",
    "# exp = mlflow.get_experiment_by_name(\"Style Transfer Algorithm\")\n",
    "\n",
    "# run = mlflow.start_run(run_name=date_str) \n",
    "# mlflow.tensorflow.autolog()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_configuration():\n",
    "    d = {\n",
    "        \"d1_path\":D1_PATH,\n",
    "        \"d2_path\":D2_PATH,\n",
    "        \"SEQUENCE_LENGTH\":SEQUENCE_LENGTH,\n",
    "        \"granularity\":GRANUARITY,\n",
    "        \"overlap\":OVERLAP,\n",
    "        \"epochs\":EPOCHS,\n",
    "        \"batch_size\":BS,\n",
    "        \"style_vector_size\":STYLE_VECTOR_SIZE,\n",
    "        \"feat_wiener\":FEAT_WIENER,\n",
    "        \"n_sample_wiener\":N_SAMPLE_WIENER,\n",
    "        \"triplet_r\":TRIPLET_R,\n",
    "        \"n_validation_sequence\":N_VALIDATION_SEQUENCE,\n",
    "        \"l_style\":L_STYLE_GENERATION,\n",
    "        \"l_content\":L_CONTENT,\n",
    "        \"l_triplet\":L_TRIPLET,\n",
    "        \"l_realness\":L_REALNESS,\n",
    "        \"l_adv\":L_ADV,\n",
    "        \"l_reconstr\":L_RECONSTR,    \n",
    "        \"log_dir\":BASE_DIR\n",
    "    }\n",
    "\n",
    "    json_object = json.dumps(d)\n",
    "    mlflow.log_params({\n",
    "        \"SEQUENCE_LENGTH\":SEQUENCE_LENGTH,\n",
    "        \"granularity\":GRANUARITY,\n",
    "        \"overlap\":OVERLAP,\n",
    "        \"epochs\":EPOCHS,\n",
    "        \"batch_size\":BS,\n",
    "        \"style_vector_size\":STYLE_VECTOR_SIZE,\n",
    "        \"feat_wiener\":FEAT_WIENER,\n",
    "        \"n_sample_wiener\":N_SAMPLE_WIENER,\n",
    "        \"triplet_r\":TRIPLET_R,\n",
    "        \"l_style\":L_STYLE_GENERATION,\n",
    "        \"l_content\":L_CONTENT,\n",
    "        \"l_triplet\":L_TRIPLET,\n",
    "        \"l_realness\":L_REALNESS,\n",
    "        \"l_adv\":L_ADV,\n",
    "        \"l_reconstr\":L_RECONSTR,\n",
    "    })\n",
    "\n",
    "    with open(f\"{SAVE_FOLDER}/parameters.json\", \"w\") as outfile:\n",
    "        outfile.write(json_object)\n",
    "\n",
    "\n",
    "save_configuration()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_valid_split(df, train_size:float=.7):\n",
    "    dset_size = df.shape[0]\n",
    "    train_index = int(dset_size* train_size)\n",
    "\n",
    "    train_split = df.loc[:train_index]\n",
    "    valid_split = df.loc[train_index:]\n",
    "\n",
    "    return train_split, valid_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_d1= pd.read_hdf(D1_PATH).astype(np.float32)\n",
    "df_d2= pd.read_hdf(D2_PATH).astype(np.float32)\n",
    "\n",
    "df_d1 = df_d1.drop(columns=['labels'])\n",
    "df_d2 = df_d2.drop(columns=['labels'])\n",
    "\n",
    "N_FEAT = df_d1.shape[1]\n",
    "\n",
    "dset_d1_train, dset_d1_valid = make_train_valid_dset(\n",
    "    df_d1, \n",
    "    SEQUENCE_LENGTH, \n",
    "    GRANUARITY, \n",
    "    int(OVERLAP* SEQUENCE_LENGTH),\n",
    "    BS,\n",
    "    reduce_train_set=True\n",
    ")\n",
    "\n",
    "# Add Style Labels:\n",
    "# dset_d1_train = dset_d1_train.map(lambda batch: (batch, tf.zeros(batch.shape[0])))\n",
    "# dset_d1_valid = dset_d1_valid.map(lambda batch: (batch, tf.zeros(batch.shape[0])))\n",
    "\n",
    "dset_d2_train, dset_d2_valid = make_train_valid_dset(\n",
    "    df_d2, \n",
    "    SEQUENCE_LENGTH, \n",
    "    GRANUARITY, \n",
    "    int(OVERLAP* SEQUENCE_LENGTH),\n",
    "    BS,\n",
    "    reduce_train_set=True\n",
    ")\n",
    "\n",
    "# Add Style Labels:\n",
    "# dset_d2_train = dset_d2_train.map(lambda batch: (batch, tf.zeros(batch.shape[0])))\n",
    "# dset_d2_valid = dset_d2_valid.map(lambda batch: (batch, tf.zeros(batch.shape[0])))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Content Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define AdaIN Layers for Time Series\n",
    "class AdaIN(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(AdaIN, self).__init__()\n",
    "\n",
    "    def get_mean_std(self, x, eps=1e-5):\n",
    "        _mean, _variance = tf.nn.moments(x, axes=[1], keepdims=True)\n",
    "        standard_dev = tf.sqrt(_variance+ eps)\n",
    "        return _mean, standard_dev\n",
    "\n",
    "    def call(self, content_input, style_input):\n",
    "        # print(content_input.shape, style_input.shape)\n",
    "        content_mean, content_std = self.get_mean_std(content_input)\n",
    "        style_mean, style_std = self.get_mean_std(style_input)\n",
    "        adain_res =style_std* (content_input - content_mean) / content_std+ style_mean\n",
    "        return adain_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_content_encoder(seq_length:int, n_feat:int, feat_wiener:int):\n",
    "    _input = tf.keras.Input((seq_length, n_feat,))\n",
    "\n",
    "    x = tf.keras.layers.Conv1D(128, 5, 2, padding='same')(_input)\n",
    "    # x = tf.keras.layers.LeakyReLU()(x)\n",
    "    \n",
    "    x = tf.keras.layers.Conv1D(128, 5, 1, padding='same')(x)\n",
    "    # x = tf.keras.layers.LeakyReLU()(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv1D(128, 5, 2, padding='same')(x)\n",
    "    # x = tf.keras.layers.LeakyReLU()(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv1D(feat_wiener, 5, 1, padding='same')(x)\n",
    "    # x = tf.keras.layers.LeakyReLU()(x)\n",
    "\n",
    "    model = tf.keras.Model(_input, x)\n",
    "\n",
    "    return model\n",
    "\n",
    "def make_style_encoder(seq_length:int, n_feat:int, vector_output_shape:int):\n",
    "    _input = tf.keras.Input((seq_length, n_feat))\n",
    "\n",
    "    x = tf.keras.layers.Conv1D(64, 5, 2, padding='same')(_input)    \n",
    "    x = tf.keras.layers.Conv1D(128, 5, 1, padding='same')(x)\n",
    "    x = tf.keras.layers.Conv1D(128, 5, 1, padding='same')(x)\n",
    "    # x = tf.keras.layers.LeakyReLU()(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv1D(128, 5, 2, padding='same')(x)\n",
    "    x = tf.keras.layers.Conv1D(256, 5, 1, padding='same')(x)\n",
    "    x = tf.keras.layers.Conv1D(256, 5, 1, padding='same')(x)\n",
    "    # x = tf.keras.layers.LeakyReLU()(x)\n",
    "\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dense(100)(x)\n",
    "    x = tf.keras.layers.Dense(50)(x)\n",
    "    x = tf.keras.layers.Dense(vector_output_shape)(x)\n",
    "\n",
    "    model = tf.keras.Model(_input, x)\n",
    "    return model\n",
    "\n",
    "def make_decoder(n_sample_wiener:int, feat_wiener:int, style_vector_size:int, out_feat:int):\n",
    "    _content_input = tf.keras.Input((n_sample_wiener, feat_wiener))\n",
    "    _style_input = tf.keras.Input((style_vector_size, 1)) \n",
    "    _style_input = tf.keras.layers.Flatten()(_style_input)\n",
    "\n",
    "    stage_1_style = tf.keras.layers.Dense(16, name='1')(_style_input)\n",
    "    stage_1_style = tf.keras.layers.Reshape((16, 1))(stage_1_style)\n",
    "\n",
    "    stage_2_style = tf.keras.layers.Dense(32, name='2')(_style_input)\n",
    "    stage_2_style = tf.keras.layers.Reshape((32, 1))(stage_2_style)\n",
    "\n",
    "    x = AdaIN()(_content_input, stage_1_style)\n",
    "    x = tf.keras.layers.Conv1DTranspose(256, 5, 1, padding='same')(x)\n",
    "    # x = AdaIN()(x, stage_1_style)\n",
    "    x = tf.keras.layers.Conv1DTranspose(256, 5, 1, padding='same')(x)\n",
    "    # x = AdaIN()(x, stage_1_style)\n",
    "    x = tf.keras.layers.Conv1DTranspose(256, 5, 2, padding='same')(x)\n",
    "    x = tf.keras.layers.LeakyReLU()(x)\n",
    "\n",
    "    x = AdaIN()(x, stage_2_style)\n",
    "    x = tf.keras.layers.Conv1DTranspose(128, 5, 1, padding='same')(x)\n",
    "    # x = AdaIN()(x, stage_2_style)\n",
    "    x = tf.keras.layers.Conv1DTranspose(128, 5, 1, padding='same')(x)\n",
    "    # x = AdaIN()(x, stage_2_style)\n",
    "    x = tf.keras.layers.Conv1DTranspose(out_feat, 5, 2, padding='same')(x)\n",
    "    x = tf.keras.layers.LeakyReLU()(x)\n",
    "\n",
    "    model = tf.keras.Model([_content_input, _style_input], x)\n",
    "    return model\n",
    "\n",
    "\n",
    "def make_discriminator(seq_length:int, n_feat:int):\n",
    "    _input = tf.keras.Input((seq_length, n_feat))\n",
    "    x = tf.keras.layers.Conv1D(16, 5, 2, padding='same')(_input)\n",
    "    x = tf.keras.layers.LeakyReLU()(x)\n",
    "    # x = tf.keras.layers.Dropout(0.3)(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv1D(16, 5, 2, padding='same')(x)\n",
    "    x = tf.keras.layers.LeakyReLU()(x)\n",
    "    # x = tf.keras.layers.Dropout(0.3)(x)\n",
    "\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    _output = tf.keras.layers.Dense(1)(x)\n",
    "\n",
    "    model = tf.keras.Model(_input, _output)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_encoder = make_content_encoder(SEQUENCE_LENGTH, df_d1.shape[1], FEAT_WIENER)\n",
    "# content_encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_encoder = make_style_encoder(SEQUENCE_LENGTH, df_d1.shape[1], STYLE_VECTOR_SIZE)\n",
    "style_encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = make_decoder(N_SAMPLE_WIENER, FEAT_WIENER, STYLE_VECTOR_SIZE ,df_d1.shape[1])\n",
    "# decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_discriminator = make_discriminator(SEQUENCE_LENGTH, df_d1.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleTransferModel(tf.keras.Model):\n",
    "    def __init__(self, seq_length:int, n_feat:int, style_vector_shape:int, n_sample_wiener:int, feat_wiener:int, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.content_encoder= make_content_encoder(seq_length, n_feat, feat_wiener)\n",
    "        self.style_encoder = make_style_encoder(seq_length, n_feat, style_vector_shape)\n",
    "        self.decoder = make_decoder(n_sample_wiener, feat_wiener, style_vector_shape, n_feat)\n",
    "\n",
    "    def __call__(self, content_sequence, style_sequence):\n",
    "        encoded_content = self.content_encoder(content_sequence)\n",
    "        encoded_style = self.style_encoder(style_sequence)\n",
    "\n",
    "        generated_sequence = self.decoder([encoded_content, encoded_style])\n",
    "\n",
    "        return generated_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(content_batch, style_batch):\n",
    "    content = content_encoder(content_batch, training=False)\n",
    "    style = style_encoder(style_batch, training=False)\n",
    "    generated = decoder([content, style], training=False)\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_seed_content = next(iter(dset_d1_valid))\n",
    "valid_seed_style = next(iter(dset_d2_valid))\n",
    "\n",
    "train_seed_content = next(iter(dset_d1_train))\n",
    "train_seed_style = next(iter(dset_d2_train))\n",
    "\n",
    "generated_sequence = generate(valid_seed_content, valid_seed_style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def plot_generated_sequence2(content_sequences, style_sequences, show=False):\n",
    "\n",
    "    # Compute what we need\n",
    "    content_of_content = content_encoder(content_sequences, training=False)\n",
    "    style_of_content = style_encoder(content_sequences, training=False)\n",
    "\n",
    "    content_of_style= content_encoder(style_sequences, training=False)\n",
    "    style_of_style= style_encoder(style_sequences, training=False)\n",
    "\n",
    "    generated_sequences = decoder([content_of_content, style_of_style], training=False)\n",
    "\n",
    "    content_of_generated = content_encoder(generated_sequences, training=False)\n",
    "    style_of_generated = style_encoder(generated_sequences, training=False)\n",
    "\n",
    "    # Reduce the Style Vector for visualization purposes.\n",
    "    pca = PCA(2)\n",
    "    all_together = np.vstack([style_of_content, style_of_style, style_of_generated])\n",
    "\n",
    "    pca.fit(all_together)\n",
    "\n",
    "    reduced_style_style = pca.transform(style_of_style)\n",
    "    reduced_style_content=pca.transform(style_of_content)\n",
    "    reduced_style_generations=pca.transform(style_of_generated)\n",
    "\n",
    "    all_values = np.array([content_sequences, style_sequences, generated_sequences])\n",
    "    _min, _max = np.min(all_values)-1, np.max(all_values)+ 1\n",
    "\n",
    "    fig= plt.figure(figsize=(18, 8))\n",
    "    spec= fig.add_gridspec(3, 6)\n",
    "\n",
    "    ax00 = fig.add_subplot(spec[0, :2])\n",
    "    ax00.set_title('Content Sequence.')\n",
    "    ax00.plot(content_sequences[0, :, :2], label=df_d1.columns[:2])\n",
    "    ax00.set_ylim(_min, _max)\n",
    "    ax00.grid(True)\n",
    "    ax00.legend()\n",
    "\n",
    "    ax10 = fig.add_subplot(spec[1, :2])\n",
    "    ax10.plot(content_sequences[0, :, 2:], label=df_d1.columns[2:])\n",
    "    ax10.set_ylim(_min, _max)\n",
    "    ax10.grid(True)\n",
    "    ax10.legend()\n",
    "\n",
    "#######\n",
    "    ax01 = fig.add_subplot(spec[0, 2:4])\n",
    "    ax01.set_title('Style Sequence.')\n",
    "    ax01.plot(style_sequences[0, :, :2])\n",
    "    ax01.set_ylim(_min, _max)\n",
    "    ax01.grid(True)\n",
    "\n",
    "    ax11 = fig.add_subplot(spec[1, 2:4])\n",
    "    ax11.plot(style_sequences[0, :, 2:])\n",
    "    ax11.set_ylim(_min, _max)\n",
    "    ax11.grid(True)\n",
    "\n",
    "#######\n",
    "    ax02 = fig.add_subplot(spec[0, 4:])\n",
    "    ax02.set_title('Generated Sequence.')\n",
    "    ax02.plot(generated_sequences[0, :, :2])\n",
    "    ax02.set_ylim(_min, _max)\n",
    "    ax02.grid(True) \n",
    "\n",
    "    ax12 = fig.add_subplot(spec[1, 4:])\n",
    "    ax12.plot(generated_sequences[1, :, 2:])\n",
    "    ax12.set_ylim(_min, _max)\n",
    "    ax12.grid(True) \n",
    "\n",
    "#####\n",
    "    ax10 = fig.add_subplot(spec[2, :3])\n",
    "    ax10.set_title('Content Space.')\n",
    "    ax10.scatter(content_of_content[0, :, 0], content_of_content[0, :, 1],  label='content of content.')\n",
    "    ax10.scatter(content_of_style[0, :, 0], content_of_style[0, :, 1], label='content of style.')\n",
    "    ax10.scatter(content_of_generated[0, :, 0], content_of_generated[0, :, 1],  label='content of generated.')\n",
    "    ax10.grid(True)\n",
    "    ax10.legend()\n",
    "\n",
    "    ax11 = fig.add_subplot(spec[2, 3:])\n",
    "    ax11.set_title('Style Space, Reduced with PCA.')\n",
    "    ax11.scatter(reduced_style_content[:, 0], reduced_style_content[:, 1], label='style of content.')\n",
    "    ax11.scatter(reduced_style_style[:, 0], reduced_style_style[:, 1], label='style of style.')\n",
    "    ax11.scatter(reduced_style_generations[:, 0], reduced_style_generations[:, 1], label='style of genrations.')\n",
    "\n",
    "    ax11.grid(True)\n",
    "    ax11.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    return fig\n",
    "\n",
    "plot_generated_sequence2(valid_seed_content, valid_seed_style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fig_to_buff(fig):\n",
    "    buf = io.BytesIO()\n",
    "    fig.savefig(buf, format='png')\n",
    "    buf.seek(0)\n",
    "    plt.close(fig)\n",
    "    return buf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "def recontruction_loss(true:tf.Tensor, generated:tf.Tensor):\n",
    "    diff = generated- true\n",
    "    result = tf.math.reduce_mean(tf.square(diff))\n",
    "    return result\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "\n",
    "\n",
    "def fixed_point_content(encoded_content_real, encoded_content_fake):\n",
    "    diff = encoded_content_fake- encoded_content_real\n",
    "    return tf.reduce_mean(tf.square(diff))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard Logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Metrics\n",
    "# Generator Losses\n",
    "met_generator_train = tf.keras.metrics.Mean(name=\"Total Generator Loss\")\n",
    "met_generator_reconstr_from_content_train= tf.keras.metrics.Mean(name=\"Reconstruction from Content\")\n",
    "met_generator_realness_train= tf.keras.metrics.Mean(name=\"Realness of Generations\")\n",
    "\n",
    "# Style Encoder Loss\n",
    "met_triplet_train = tf.keras.metrics.Mean(name=\"Total Triplet Loss\")\n",
    "met_disentanglement_train = tf.keras.metrics.Mean(name=\"Disentanglement Loss\")\n",
    "met_style_encoder_train = tf.keras.metrics.Mean(name=\"Style Loss\")\n",
    "\n",
    "# Content encoder Loss\n",
    "met_content_encoder_train= tf.keras.metrics.Mean(name=\"Content Encoder Loss\")\n",
    "\n",
    "# Correlation Metric\n",
    "met_correlation_metric_train = tf.keras.metrics.Mean(name=\"Correlation Metric\")\n",
    "\n",
    "# Discriminator Loss\n",
    "met_disc_loss_train= tf.keras.metrics.Mean(name=\"Discriminator Loss\")\n",
    "\n",
    "# Style of Reconstruction (ajouté ici mais plus un bout de scotch qu'autre chose...).\n",
    "met_style_reconstruction = tf.keras.metrics.Mean(name=\"Style Reconstruction\")\n",
    "\n",
    "# Correlation Metric\n",
    "met_correlation_metric_train = tf.keras.metrics.Mean(name=\"Correlation Metric\")\n",
    "\n",
    "\n",
    "# Valid Metrics\n",
    "# Generator Metric\n",
    "met_generator_valid = tf.keras.metrics.Mean(name=\"Total Generator Loss\")\n",
    "met_generator_reconstr_from_content_valid= tf.keras.metrics.Mean(name=\"Reconstruction from Content\")\n",
    "met_generator_realness_valid= tf.keras.metrics.Mean(name=\"Realness of Generations\")\n",
    "\n",
    "# Style encoder Loss\n",
    "met_triplet_valid = tf.keras.metrics.Mean(name=\"Triplet Loss\")\n",
    "met_disentanglement_valid = tf.keras.metrics.Mean(name=\"Disentanglement Loss\")\n",
    "met_style_encoder_valid = tf.keras.metrics.Mean(name=\"Style Loss\")\n",
    "\n",
    "# Content Encoder Loss\n",
    "met_content_encoder_valid= tf.keras.metrics.Mean(name=\"Content Encoder Loss\")\n",
    "\n",
    "# Discriminator Loss\n",
    "met_disc_loss_valid= tf.keras.metrics.Mean(name=\"Discriminator Loss\")\n",
    "\n",
    "# correlation Metric\n",
    "met_correlation_metric_valid = tf.keras.metrics.Mean(name=\"Correlation Metric\")\n",
    "\n",
    "# Style of Reconstruction (ajouté ici mais plus un bout de scotch qu'autre chose...).\n",
    "met_style_reconstruction_valid = tf.keras.metrics.Mean(name=\"Style Reconstruction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SUMMARY_WRITER = tf.summary.create_file_writer(TRAIN_LOGS_DIR_PATH)\n",
    "VALID_SUMMARY_WRITER = tf.summary.create_file_writer(VALID_LOGS_DIR_PATH)\n",
    "\n",
    "def log_train_losses(epoch, plot_buf):\n",
    "    image = tf.image.decode_png(plot_buf.getvalue(), channels=4)\n",
    "    image = tf.expand_dims(image, 0)\n",
    "\n",
    "    with TRAIN_SUMMARY_WRITER.as_default():\n",
    "        tf.summary.scalar(\"00 - Total Generator Loss\", met_generator_train.result(), step=epoch)\n",
    "        tf.summary.scalar(\"01 - Reconstruction from Content\", met_generator_reconstr_from_content_train.result(), step=epoch)\n",
    "        tf.summary.scalar(\"02 - Realness Loss\", met_generator_realness_train.result(), step=epoch)\n",
    "\n",
    "        tf.summary.scalar(\"10 - Style Loss\", met_style_encoder_train.result(), step=epoch)\n",
    "        tf.summary.scalar(\"11 - Triplet Loss\", met_triplet_train.result(), step=epoch)\n",
    "        tf.summary.scalar(\"12 - Disentanglement Loss\", met_disentanglement_train.result(), step=epoch)\n",
    "\n",
    "        tf.summary.scalar(\"20 - Content Loss\", met_content_encoder_train.result(), step=epoch)\n",
    "        tf.summary.scalar(\"30 - Discriminator Loss\", met_disc_loss_train.result(), step=epoch)\n",
    "        tf.summary.scalar(\"40 - Correlation Metric\", met_correlation_metric_train.result(), step=epoch)\n",
    "        \n",
    "        tf.summary.image(\"Training Generations\", image, step=epoch)\n",
    "\n",
    "\n",
    "def log_valid_losses(epoch):\n",
    "    with VALID_SUMMARY_WRITER.as_default():\n",
    "        tf.summary.scalar(\"00 - Total Generator Loss\", met_generator_valid.result(), step=epoch)\n",
    "        tf.summary.scalar(\"01 - Reconstruction from Content\", met_generator_reconstr_from_content_valid.result(), step=epoch)\n",
    "        tf.summary.scalar(\"02 - Realness Loss\", met_generator_realness_valid.result(), step=epoch)\n",
    "\n",
    "        tf.summary.scalar(\"10 - Style Loss\", met_style_encoder_valid.result(), step=epoch)\n",
    "        tf.summary.scalar(\"11 - Triplet Loss\", met_triplet_valid.result(), step=epoch)\n",
    "        tf.summary.scalar(\"12 - Disentanglement Loss\", met_disentanglement_valid.result(), step=epoch)\n",
    "\n",
    "        tf.summary.scalar(\"20 - Content Loss\", met_content_encoder_valid.result(), step=epoch)\n",
    "        tf.summary.scalar(\"30 - Discriminator Loss\", met_disc_loss_valid.result(), step=epoch)\n",
    "        tf.summary.scalar(\"40 - Correlation Metric\", met_correlation_metric_valid.result(), step=epoch)\n",
    "        \n",
    "\n",
    "\n",
    "def reset_metric_states():\n",
    "    met_generator_train.reset_states()\n",
    "    met_generator_reconstr_from_content_train.reset_states()\n",
    "    met_generator_realness_train.reset_states()\n",
    "    met_triplet_train.reset_states()\n",
    "    met_disentanglement_train.reset_states()\n",
    "    met_style_encoder_train.reset_states()\n",
    "    met_content_encoder_train.reset_states()\n",
    "    met_correlation_metric_train.reset_states()\n",
    "    met_disc_loss_train.reset_states()\n",
    "    met_style_reconstruction.reset_states()\n",
    "    met_correlation_metric_train.reset_states()\n",
    "\n",
    "\n",
    "def reset_valid_states():\n",
    "    met_generator_valid.reset_states()\n",
    "    met_generator_reconstr_from_content_valid.reset_states()\n",
    "    met_generator_realness_valid.reset_states()\n",
    "    met_triplet_valid.reset_states()\n",
    "    met_disentanglement_valid.reset_states()\n",
    "    met_style_encoder_valid.reset_states()\n",
    "    met_content_encoder_valid.reset_states()\n",
    "    met_correlation_metric_valid.reset_states()\n",
    "    met_disc_loss_valid.reset_states()\n",
    "    met_style_reconstruction.reset_states()\n",
    "    met_correlation_metric_valid.reset_states()\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate model for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_encoder = make_content_encoder(SEQUENCE_LENGTH, df_d1.shape[1], FEAT_WIENER)\n",
    "style_encoder = make_style_encoder(SEQUENCE_LENGTH, df_d1.shape[1], STYLE_VECTOR_SIZE)\n",
    "decoder = make_decoder(N_SAMPLE_WIENER, FEAT_WIENER, STYLE_VECTOR_SIZE ,df_d1.shape[1])\n",
    "\n",
    "\n",
    "opt_content_encoder = tf.keras.optimizers.Adam()\n",
    "opt_style_encoder = tf.keras.optimizers.Adam()\n",
    "opt_decoder = tf.keras.optimizers.Adam()\n",
    "opt_discr = tf.keras.optimizers.Adam()\n",
    "\n",
    "# opt_content_encoder = tf.keras.optimizers.RMSprop(learning_rate=1e-6)\n",
    "# opt_style_encoder = tf.keras.optimizers.RMSprop(learning_rate=2e-6)\n",
    "# opt_decoder = tf.keras.optimizers.RMSprop(learning_rate=1e-3)\n",
    "# opt_discr = tf.keras.optimizers.RMSprop(learning_rate=1e-6)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "indexes = np.array(list(product(range(BS), range(BS))))\n",
    "other_index = np.arange(BS)* BS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_indexes = np.array([ i* BS+i for i in range(BS) for _ in range(BS) ])\n",
    "pos_indexes = np.array([ BS*j + i for i in range(BS) for j in range(BS)])\n",
    "neg_indexes = np.array([ (j*BS + (i+1)%BS) for i in range(BS) for j in range(BS)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_style_voctor_for_dis_loss(style_vector:tf.Tensor, batch_size:int):\n",
    "    anchor_indexes = np.array([ i* batch_size+i for i in range(batch_size) for _ in range(batch_size-1) ])\n",
    "    return tf.gather(style_vector, anchor_indexes)\n",
    "\n",
    "def get_anchor_positive_negative_from_batch(style_from_style_ts:tf.Tensor, style_of_generations:tf.Tensor, batch_size:int):\n",
    "    anchor_indexes = np.array([ i* batch_size+i for i in range(batch_size) for _ in range(batch_size) ])\n",
    "    pos_indexes = np.array([ batch_size*j + i for i in range(batch_size) for j in range(batch_size)])\n",
    "    neg_indexes = np.array([ (j*batch_size + (i+1)%batch_size) for i in range(batch_size) for j in range(batch_size)])\n",
    "\n",
    "    # Anchor are for example:\n",
    "    # [(c0, s0), (c0, s0), \n",
    "    # (c1, s1), (c1, s1), ...]\n",
    "    anchors = tf.gather(style_from_style_ts, anchor_indexes)\n",
    "    # Different content, Same Style:\n",
    "    # [(c1, s0), (c2, s0),(c2, s0),\n",
    "    #  (c1, s1), (c2, s1), (c3, s1)...]\n",
    "    # E_s(G(x, y1))\n",
    "    pos_vector= tf.gather(style_of_generations, pos_indexes)\n",
    "    # Same content but different style\n",
    "    # [(c1, s1), (c2, s1),(c2, s1),\n",
    "    #  (c1, s2), (c2, s2), (c3, s2)...]\n",
    "    # # E_s(G(x, y2)) \n",
    "    neg_vector = tf.gather(style_of_generations, neg_indexes)\n",
    "\n",
    "    return anchors, pos_vector, neg_vector\n",
    "\n",
    "def get_dissantanglement_loss_component(style_of_generations, style_of_style, batch_size:int):\n",
    "    anchor_indexes = np.array([ i* batch_size+i for i in range(batch_size) for _ in range(batch_size-1) ])\n",
    "    pos_indexes = np.array([ batch_size*j + i for i in range(batch_size) for j in range(batch_size) if i !=j ])\n",
    "\n",
    "    es_y = tf.gather(style_of_style, anchor_indexes)\n",
    "    es_x1_y = tf.gather(style_of_generations, anchor_indexes)\n",
    "    es_x2_y = tf.gather(style_of_generations, pos_indexes)\n",
    "\n",
    "    return es_y, es_x1_y, es_x2_y\n",
    "\n",
    "\n",
    "def l2(x:tf.Tensor, y:tf.Tensor):\n",
    "    diff = tf.square(y- x)\n",
    "    _distance = tf.reduce_sum(diff, axis=-1)\n",
    "    return _distance\n",
    "\n",
    "\n",
    "def fixed_point_content(encoded_content_real, encoded_content_fake):\n",
    "    diff = l2(encoded_content_real, encoded_content_fake)\n",
    "    return tf.reduce_mean(tf.square(diff))\n",
    "\n",
    "def fixed_point_triplet_style_loss(anchor_encoded_style, positive_encoded_style, negative_encoded_style):\n",
    "    # shape: [BS, Style_length]\n",
    "    negative_distance = l2(negative_encoded_style, anchor_encoded_style)\n",
    "    positive_distance = l2(positive_encoded_style, anchor_encoded_style)\n",
    "\n",
    "    triplet = TRIPLET_R+ positive_distance- negative_distance\n",
    "    zeros = tf.zeros_like(triplet)\n",
    "    triplet = tf.math.maximum(triplet, zeros)\n",
    "\n",
    "    loss = tf.reduce_mean(triplet)\n",
    "    return loss\n",
    "\n",
    "def fixed_point_disentanglement(\n",
    "        es_x1_y:tf.Tensor, \n",
    "        es_x2_y:tf.Tensor, \n",
    "        es_y:tf.Tensor\n",
    "        ):\n",
    "\n",
    "    diff1 = l2(es_x1_y, es_x2_y)\n",
    "    diff2 = l2(es_x1_y, es_y)\n",
    "\n",
    "    loss = diff1- diff2\n",
    "    zeros = tf.zeros_like(loss)\n",
    "    loss = tf.math.maximum(loss, zeros)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def style_constrain_loss(style_of_style_batch, style_of_generation):\n",
    "    diff = l2(style_of_style_batch, style_of_generation)\n",
    "    diff = tf.reduce_mean(diff)\n",
    "    return diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _pairwise_distance(a_embeddings, b_embeddings):\n",
    "    # Get the dot product between all embeddings\n",
    "    # shape (batch_size, batch_size)\n",
    "    dot_product = tf.matmul(a_embeddings, tf.transpose(b_embeddings))\n",
    "\n",
    "    # Get squared L2 norm for each embedding. We can just take the diagonal of `dot_product`.\n",
    "    # This also provides more numerical stability (the diagonal of the result will be exactly 0).\n",
    "    # shape (batch_size,)\n",
    "    square_norm = tf.linalg.diag_part(dot_product)\n",
    "\n",
    "    # Compute the pairwise distance matrix as we have:\n",
    "    # ||a - b||^2 = ||a||^2  - 2 <a, b> + ||b||^2\n",
    "    # shape (batch_size, batch_size)\n",
    "    distances = tf.expand_dims(square_norm, 0) - 2.0 * dot_product + tf.expand_dims(square_norm, 1)\n",
    "\n",
    "    # Because of computation errors, some distances might be negative so we put everything >= 0.0\n",
    "    distances = tf.maximum(distances, 0.0)\n",
    "\n",
    "    return distances\n",
    "\n",
    "def get_positives(labels):\n",
    "    # will assign 1 where this is a positive \n",
    "    positive_mask = np.array([ tf.cast(labels == l, tf.float32) for l in labels])\n",
    "    positive_mask = positive_mask - np.identity(positive_mask.shape[0])\n",
    "    return positive_mask\n",
    "\n",
    "def get_negative(labels):\n",
    "    neg_labels= np.array([ tf.cast(labels != l, tf.float32) for l in labels])\n",
    "    return neg_labels\n",
    "\n",
    "\n",
    "def get_triplet_loss(anchor_embedding, positive_embedding, negative_embedding):\n",
    "    positive_distance= _pairwise_distance(anchor_embedding, positive_embedding)\n",
    "    negative_distance= _pairwise_distance(anchor_embedding, negative_embedding)\n",
    "\n",
    "    positive_index= tf.argmax(positive_distance, axis=1)\n",
    "\n",
    "    pos_embedding = tf.gather(positive_embedding, positive_index)\n",
    " \n",
    "    neg_indexes = tf.argmin(negative_distance, axis=1)\n",
    "    \n",
    "    neg_embeddings= tf.gather(negative_embedding, neg_indexes)\n",
    "\n",
    "    positive_distances= l2(anchor_embedding, pos_embedding)\n",
    "    negative_distances= l2(anchor_embedding, neg_embeddings)\n",
    "\n",
    "    loss = tf.reduce_mean(tf.maximum(TRIPLET_R+ positive_distances - negative_distances, 0))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function\n",
    "def train_step(content_batch:tf.Tensor, style_batch:tf.Tensor):\n",
    "    lambda_style_generations = L_STYLE_GENERATION\n",
    "    lambda_reconstr= L_RECONSTR\n",
    "    lambda_realness= L_REALNESS\n",
    "    lambda_adv= L_ADV\n",
    "    lambda_content= L_CONTENT\n",
    "    lambda_triplet= L_TRIPLET\n",
    "    lambda_dis= L_DIS\n",
    "\n",
    "    with tf.GradientTape() as content_tape, tf.GradientTape() as style_tape, tf.GradientTape() as decoder_tape, tf.GradientTape() as discr_tape:\n",
    "        # extended_content_images = tf.gather(content_batch, indexes[:, 0])\n",
    "        # extended_style_images = tf.gather(style_batch, indexes[:, 1])\n",
    "        \n",
    "        # Get the content form the content batch\n",
    "        c_0 = content_encoder(content_batch, training=True)\n",
    "        s_0 = style_encoder(content_batch, training=True)\n",
    "        # Get the Style from the style batch\n",
    "        c_1 = content_encoder(style_batch, training=True)\n",
    "        s_1 = style_encoder(style_batch, training=True)\n",
    "\n",
    "        # Generate the time series given the content and the style.\n",
    "        c0_s0 = decoder([c_0, s_0], training=True)\n",
    "        c1_s1 = decoder([c_1, s_1], training=True)\n",
    "        c0_s1 = decoder([c_0, s_1], training=True)\n",
    "        c1_s0 = decoder([c_1, s_0], training=True)\n",
    "\n",
    "        # Get the content and style form generated data\n",
    "        c_c0s1 = content_encoder(c0_s1, training=True)\n",
    "        s_c0s1 = style_encoder(c0_s1, training=True)\n",
    "\n",
    "        # c_c0s0 = content_encoder(c0_s0, training=True)\n",
    "        s_c0s0 = style_encoder(c0_s0, training=True)\n",
    "\n",
    "        # c_c1s1 = content_encoder(c1_s1, training=True)\n",
    "        s_c1s1 = style_encoder(c1_s1, training=True)\n",
    "\n",
    "        s_c1s0 = style_encoder(c1_s0, training=True)\n",
    "\n",
    "        # Get content image stylized with a image from the style batch\n",
    "        # E.g.  (c1, s1), (c2, s2)... (cn, sn)\n",
    "        crit_on_fake = global_discriminator(c0_s1, training=True)\n",
    "        crit_on_real = global_discriminator(content_batch, training=True)\n",
    "        \n",
    "        # Compute the generator loss \n",
    "        reconstr_from_content = recontruction_loss(content_batch, c0_s0)\n",
    "        realness = generator_loss(c0_s1)\n",
    "        content_similarity = fixed_point_content(c_0, c_c0s1)\n",
    "\n",
    "        global_dicriminator_loss = discriminator_loss(crit_on_real, crit_on_fake)\n",
    "\n",
    "        dis_loss = fixed_point_disentanglement(s_1, s_c0s1, s_c1s1)\n",
    "\n",
    "        test = s_1.shape[0]\n",
    "        triplet_style =  get_triplet_loss(s_1, s_c0s1, s_c0s0) + get_triplet_loss(s_0, s_c1s0, s_c1s1) # +get_triplet_loss(s_1[:test//2], s_1[test//2:], s_0[test//2:])\n",
    "        triplet_style = triplet_style/2\n",
    "\n",
    "        d_loss = lambda_adv* global_dicriminator_loss\n",
    "        content_encoder_loss = lambda_content* content_similarity\n",
    "        style_encoder_loss = lambda_triplet* triplet_style + lambda_dis* dis_loss\n",
    "        g_loss = lambda_reconstr* reconstr_from_content+ lambda_realness* realness#+ lambda_style_generations* constrain_on_style_generations\n",
    "\n",
    "    content_grad=content_tape.gradient(content_encoder_loss, content_encoder.trainable_variables)\n",
    "    style_grad = style_tape.gradient(style_encoder_loss, style_encoder.trainable_variables)\n",
    "    decoder_grad = decoder_tape.gradient(g_loss, decoder.trainable_variables)\n",
    "    discr_grads = discr_tape.gradient(d_loss, global_discriminator.trainable_variables)\n",
    "\n",
    "    opt_content_encoder.apply_gradients(zip(content_grad, content_encoder.trainable_variables))\n",
    "    opt_style_encoder.apply_gradients(zip(style_grad, style_encoder.trainable_variables))\n",
    "    opt_decoder.apply_gradients(zip(decoder_grad, decoder.trainable_variables))\n",
    "    opt_discr.apply_gradients(zip(discr_grads, global_discriminator.trainable_variables))\n",
    "\n",
    "    met_generator_train(g_loss)\n",
    "    met_generator_reconstr_from_content_train(reconstr_from_content)\n",
    "    met_generator_realness_train(realness)\n",
    "\n",
    "    met_style_encoder_train(style_encoder_loss)\n",
    "    met_triplet_train(triplet_style)\n",
    "    met_disentanglement_train(dis_loss)\n",
    "\n",
    "    met_content_encoder_train(content_similarity)\n",
    "    met_disc_loss_train(global_dicriminator_loss)\n",
    "    # met_style_reconstruction(constrain_on_style_generations)\n",
    "\n",
    "@tf.function\n",
    "def valid_step(content_batch:tf.Tensor, style_batch:tf.Tensor):\n",
    "    lambda_style_generations = L_STYLE_GENERATION\n",
    "    lambda_reconstr= L_RECONSTR\n",
    "    lambda_realness= L_REALNESS\n",
    "    lambda_adv= L_ADV\n",
    "    lambda_content= L_CONTENT\n",
    "    lambda_triplet= L_TRIPLET\n",
    "    lambda_dis= L_DIS\n",
    "    \n",
    "    # extended_content_images = tf.gather(content_batch, indexes[:, 0])\n",
    "    # extended_style_images = tf.gather(style_batch, indexes[:, 1])\n",
    "    \n",
    "    # Get the content form the content batch\n",
    "    c_0 = content_encoder(content_batch, training=False)\n",
    "    s_0 = style_encoder(content_batch, training=False)\n",
    "    # Get the Style from the style batch\n",
    "    c_1 = content_encoder(style_batch, training=False)\n",
    "    s_1 = style_encoder(style_batch, training=False)\n",
    "\n",
    "    # Generate the time series given the content and the style.\n",
    "    c0_s1 = decoder([c_0, s_1], training=False)\n",
    "    c0_s0 = decoder([c_0, s_0], training=False)\n",
    "    c1_s1 = decoder([c_1, s_1], training=False)\n",
    "\n",
    "    # Get the content and style form generated data\n",
    "    c_c0s1 = content_encoder(c0_s1, training=False)\n",
    "    s_c0s1 = style_encoder(c0_s1, training=False)\n",
    "\n",
    "    # c_c0s0 = content_encoder(c0_s0, training=False)\n",
    "    s_c0s0 = style_encoder(c0_s0, training=False)\n",
    "\n",
    "    # c_c1s1 = content_encoder(c1_s1, training=False)\n",
    "    s_c1s1 = style_encoder(c1_s1, training=False)\n",
    "\n",
    "    # Get content image stylized with a image from the style batch\n",
    "    # E.g.  (c1, s1), (c2, s2)... (cn, sn)\n",
    "    crit_on_fake = global_discriminator(c0_s1, training=False)\n",
    "    crit_on_real = global_discriminator(content_batch, training=False)\n",
    "    \n",
    "    # Compute the generator loss \n",
    "    reconstr_from_content = (recontruction_loss(content_batch, c0_s0) + recontruction_loss(style_batch, c1_s1))/2\n",
    "    realness = generator_loss(c0_s1)\n",
    "    content_similarity = fixed_point_content(c_0, c_c0s1)\n",
    "\n",
    "    global_dicriminator_loss = discriminator_loss(crit_on_real, crit_on_fake)\n",
    "\n",
    "    dis_loss = fixed_point_disentanglement(s_1, s_c0s1, s_c1s1)\n",
    "\n",
    "    test = s_1.shape[0]\n",
    "    triplet_style = get_triplet_loss(s_1, s_c0s1, s_c0s0)# + get_triplet_loss(s_1[:test//2], s_1[test//2:], s_0[test//2:])\n",
    "    # triplet_style = triplet_style/2\n",
    "\n",
    "    d_loss = lambda_adv* global_dicriminator_loss\n",
    "    content_encoder_loss = lambda_content* content_similarity\n",
    "    style_encoder_loss = lambda_triplet* triplet_style + lambda_dis* dis_loss\n",
    "    g_loss = lambda_reconstr* reconstr_from_content+ lambda_realness* realness#+ lambda_style_generations* constrain_on_style_generations\n",
    "\n",
    "    met_generator_valid(g_loss)\n",
    "    met_generator_reconstr_from_content_valid(reconstr_from_content)\n",
    "    met_generator_realness_valid(realness)\n",
    "\n",
    "    met_triplet_valid(triplet_style)\n",
    "    met_disentanglement_valid(dis_loss)\n",
    "    met_style_encoder_valid(style_encoder_loss)\n",
    "\n",
    "    met_content_encoder_valid(content_encoder_loss)\n",
    "\n",
    "    met_disc_loss_valid(d_loss)\n",
    "    # met_style_reconstruction_valid(constrain_on_style_generations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    total_batch = \"?\"\n",
    "    for e in range(EPOCHS):\n",
    "        reset_metric_states()\n",
    "        reset_valid_states()\n",
    "        \n",
    "        print(\"[+] Train Step...\")\n",
    "        for i, (content_batch, style_batch) in enumerate(zip(dset_d1_train, dset_d2_train)):\n",
    "            train_step(content_batch, style_batch)\n",
    "            print(f\"\\r e:{e}/{EPOCHS}; {i}/{total_batch}. G_loss {met_generator_train.result():0.2f} Triplet Loss {met_triplet_train.result():0.2f}; Disentanglement Loss: {met_disc_loss_train.result():0.2f}; Content Loss {met_content_encoder_train.result():0.2f} D Loss {met_disc_loss_train.result():0.2f}        \", end=\"\")\n",
    "            # return\n",
    "\n",
    "        print()\n",
    "        print(\"[+] Validation Step...\")\n",
    "        for vb, (content_batch, style_batch) in enumerate(zip(dset_d1_valid, dset_d2_valid)):\n",
    "            valid_step(content_batch, style_batch)\n",
    "            print(f\"\\r e:{e}/{EPOCHS}; {i}/{total_batch}. G_loss {met_generator_valid.result():0.2f} Triplet Loss {met_triplet_valid.result():0.2f}; Disentanglement Loss: {met_disc_loss_train.result():0.2f}; Content Loss {met_content_encoder_train.result():0.2f} D Loss {met_disc_loss_train.result():0.2f}        \", end=\"\")\n",
    "    \n",
    "        # Make Generations Train Set\n",
    "        train_generations = generate(train_seed_content, train_seed_style)\n",
    "\n",
    "        train_style_signature = signature_on_batch(train_seed_style, config.met_params.ins, config.met_params.outs, config.met_params.signature_length)\n",
    "        train_generated_signature = signature_on_batch(train_generations, config.met_params.ins, config.met_params.outs, config.met_params.signature_length)\n",
    "        train_signature_difference = signature_metric(train_style_signature, train_generated_signature)\n",
    "        met_correlation_metric_train(train_signature_difference)\n",
    "\n",
    "\n",
    "        # Make Generations Valid Set \n",
    "        valid_generations = generate(valid_seed_content, valid_seed_style)\n",
    "\n",
    "        vis_fig = plot_generated_sequence2(valid_seed_content, valid_seed_style)\n",
    "        plot_buff = fig_to_buff(vis_fig)\n",
    "        \n",
    "        # Compute the Correlation Metric on Valid set.\n",
    "        valid_style_signature = signature_on_batch(valid_seed_style, config.met_params.ins, config.met_params.outs, config.met_params.signature_length)\n",
    "        valid_generated_signature = signature_on_batch(valid_generations, config.met_params.ins, config.met_params.outs, config.met_params.signature_length)\n",
    "        valid_signature_difference = signature_metric(valid_style_signature, valid_generated_signature)\n",
    "        met_correlation_metric_valid(valid_signature_difference)\n",
    "\n",
    "        print(f\"\\n[+] Signature Difference {valid_signature_difference:0.2f}\")\n",
    "\n",
    "        log_train_losses(e, plot_buff)\n",
    "        log_valid_losses(e)\n",
    "        print()\n",
    "\n",
    "        if e == 0:\n",
    "            total_batch = i \n",
    "\n",
    "train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_on_dataset():\n",
    "    _df_d1= pd.read_hdf(D1_PATH).astype(np.float32)\n",
    "    _df_d2= pd.read_hdf(D2_PATH).astype(np.float32)\n",
    "\n",
    "    _df_d1 = _df_d1.drop(columns=['labels'])\n",
    "    _df_d2 = _df_d2.drop(columns=['labels'])\n",
    "\n",
    "    _, _dset_d1_valid = make_train_valid_dset(\n",
    "        _df_d1, \n",
    "        SEQUENCE_LENGTH, \n",
    "        GRANUARITY, \n",
    "        int(OVERLAP* SEQUENCE_LENGTH),\n",
    "        BS,\n",
    "        valid_set_size=500\n",
    "    )\n",
    "\n",
    "    _, _dset_d2_valid = make_train_valid_dset(\n",
    "        _df_d2, \n",
    "        SEQUENCE_LENGTH, \n",
    "        GRANUARITY, \n",
    "        int(OVERLAP* SEQUENCE_LENGTH),\n",
    "        BS,\n",
    "        valid_set_size=500\n",
    "    )\n",
    "\n",
    "    d1_sequence, d2_sequence = next(iter(_dset_d1_valid)), next(iter(_dset_d2_valid))\n",
    "\n",
    "    generated_batch = generate(d1_sequence, d2_sequence)\n",
    "\n",
    "    real_validation_signature = signature_on_batch(d2_sequence, config.met_params.ins, config.met_params.outs, config.met_params.signature_length)\n",
    "    generated_signature = signature_on_batch(generated_batch, config.met_params.ins, config.met_params.outs, config.met_params.signature_length)\n",
    "\n",
    "    sig_difference = signature_metric(real_validation_signature, generated_signature)\n",
    "\n",
    "    return sig_difference"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TSTR Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.NaiveClassifier import make_naive_discriminator\n",
    "\n",
    "def make_classification_dataset(df:pd.DataFrame, overlap:float=.5):\n",
    "\n",
    "    dset_train, dset_valid = make_train_valid_dset(\n",
    "        df, \n",
    "        SEQUENCE_LENGTH, \n",
    "        GRANUARITY, \n",
    "        int(overlap* SEQUENCE_LENGTH),\n",
    "        BS,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    # Extract labels from dataset.\n",
    "    dset_train = dset_train.map(lambda s: (s[:, :, :-1], s[:, int(SEQUENCE_LENGTH//2), -1]))\n",
    "    dset_valid = dset_valid.map(lambda s: (s[:, :, :-1], s[:, int(SEQUENCE_LENGTH//2), -1]))\n",
    "\n",
    "    return dset_train, dset_valid\n",
    "\n",
    "def stylize_dataset(content_dataset:tf.data.Dataset, style_dataset:tf.data.Dataset):\n",
    "    mixted_dataset = tf.data.Dataset.zip((content_dataset, style_dataset))\n",
    "    stylized_dataset = mixted_dataset.map(lambda _cont, _sty: (generate(_cont[0], _sty[0]), _cont[1]))\n",
    "    return stylized_dataset\n",
    "\n",
    "\n",
    "def train_naive_classifier(train_classif_dataset:tf.data.Dataset, valid_classif_dataset:tf.data.Dataset)->tf.keras.models.Model:\n",
    "    naive_model = make_naive_discriminator((SEQUENCE_LENGTH, N_FEAT), 5)\n",
    "    _epochs = 1\n",
    "\n",
    "    naive_model.fit(train_classif_dataset, validation_data=valid_classif_dataset, epochs=_epochs)\n",
    "\n",
    "    return naive_model\n",
    "\n",
    "def tstr_test():\n",
    "    _df_d1= pd.read_hdf(D1_PATH).astype(np.float32)\n",
    "    _df_d2= pd.read_hdf(D2_PATH).astype(np.float32)\n",
    "\n",
    "    _dset_d1_train, _dset_d1_valid = make_classification_dataset(_df_d1)\n",
    "    _dset_d2_train, _dset_d2_valid = make_classification_dataset(_df_d2)\n",
    "\n",
    "    # Generate the 'stylized' dataset \n",
    "    _dset_12_train = stylize_dataset(_dset_d1_train, _dset_d2_train)\n",
    "    _dset_12_valid = stylize_dataset(_dset_d1_valid, _dset_d2_valid)\n",
    "\n",
    "    # Train a classifier on the stylized dataset.\n",
    "    naive_12_model = train_naive_classifier(_dset_12_train, _dset_12_valid)\n",
    "    eval_on_generated = naive_12_model.evaluate(_dset_12_valid)[1]\n",
    "\n",
    "    # Train on the real Dataset.\n",
    "    naive_on_real = train_naive_classifier(_dset_d2_train, _dset_d2_valid)\n",
    "    eval_on_real = naive_on_real.evaluate(_dset_d2_valid)[1]\n",
    "\n",
    "    return eval_on_generated, eval_on_real"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_results(folder_to_save:str, acc_real, acc_generated, sig_diff):\n",
    "    # Create the folder.\n",
    "    file_path = f\"{folder_to_save}/scores.xlsx\"\n",
    "    df_results = pd.DataFrame([[acc_real, acc_generated, sig_diff]], columns=[\"Acc on Real\", \"Acc on Generated\", \"Signature Difference\"])\n",
    "    df_results.to_excel(file_path)\n",
    "\n",
    "acc_generated, acc_real = tstr_test()\n",
    "signature_diff = correlation_on_dataset()\n",
    "\n",
    "log_results(SAVE_FOLDER, acc_real, acc_generated, signature_diff)\n",
    "save_configuration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
