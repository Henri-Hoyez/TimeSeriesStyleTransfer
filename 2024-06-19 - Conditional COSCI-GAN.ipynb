{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MTS Generation with COSCI-GAN.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from configs.Metric import Metric\n",
    "from configs.SimulatedData import Proposed\n",
    "from utils.metric import signature_on_batch, signature_metric\n",
    "import mlflow\n",
    "from sklearn.decomposition import PCA\n",
    "from datetime import datetime\n",
    "import io\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs.\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized.\n",
    "    print(e)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config= Proposed()\n",
    "\n",
    "SEQUENCE_LENGTH = config.sequence_lenght_in_sample\n",
    "GRANUARITY = config.granularity\n",
    "OVERLAP = config.overlap\n",
    "BS = config.batch_size\n",
    "EPOCHS = 20 #config.epochs\n",
    "NUM_SEQUENCE_TO_GENERATE = config.met_params.sequence_to_generate\n",
    "STEP_LIMITATION = 1000\n",
    "UPDATE_DISCRIMINATOR = 20\n",
    "\n",
    "SIMULATED_DATA_PATH = \"data/simulated_dataset/01 - Source Domain.h5\"\n",
    "SIMULATED_DATA_PATH2= \"data/simulated_dataset/output_noise/0.25.h5\"\n",
    "\n",
    "N_SAMPLE_WIENER = SEQUENCE_LENGTH//4\n",
    "FEAT_WIENER = 2\n",
    "NOISE_DIM= (N_SAMPLE_WIENER, FEAT_WIENER)\n",
    "N_STYLE = 2\n",
    "STYLE_VECTOR = 1\n",
    "\n",
    "LAMBDA_GLOBAL = 0.001\n",
    "LAMBDA_LOCAL = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dataframe_to_tensorflow_sequences(df:pd.DataFrame, sequence_lenght_in_sample, granularity, shift_between_sequences, batch_size, shuffle=True):\n",
    "    sequence_lenght = int(sequence_lenght_in_sample*granularity)\n",
    "\n",
    "    dset = tf.data.Dataset.from_tensor_slices(df.values)\n",
    "    dset = dset.window(sequence_lenght , shift=shift_between_sequences, stride=granularity).flat_map(lambda x: x.batch(sequence_lenght_in_sample, drop_remainder=True))\n",
    "\n",
    "    if shuffle:\n",
    "        dset= dset.shuffle(256)\n",
    "\n",
    "    dset = dset.batch(batch_size, drop_remainder=True)\n",
    "\n",
    "    dset = dset.cache().prefetch(10)\n",
    "\n",
    "    return dset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_simulated = pd.read_hdf(SIMULATED_DATA_PATH)\n",
    "df_simulated = df_simulated.drop(columns='labels')\n",
    "\n",
    "df_simulated2 = pd.read_hdf(SIMULATED_DATA_PATH2)\n",
    "df_simulated2 = df_simulated2.drop(columns='labels')\n",
    "\n",
    "dset_simulated = convert_dataframe_to_tensorflow_sequences(\n",
    "    df_simulated, \n",
    "    SEQUENCE_LENGTH, \n",
    "    GRANUARITY, \n",
    "    int(OVERLAP* SEQUENCE_LENGTH),\n",
    "    BS\n",
    ")\n",
    "\n",
    "dset_simulated2 = convert_dataframe_to_tensorflow_sequences(\n",
    "    df_simulated2, \n",
    "    SEQUENCE_LENGTH, \n",
    "    GRANUARITY, \n",
    "    int(OVERLAP* SEQUENCE_LENGTH),\n",
    "    BS\n",
    ")\n",
    "\n",
    "dset_simulated = dset_simulated.take(STEP_LIMITATION)\n",
    "dset_simulated2 = dset_simulated2.take(STEP_LIMITATION)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify if it is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = next(iter(dset_simulated))[0]\n",
    "sequence2= next(iter(dset_simulated2))[0]\n",
    "\n",
    "plt.figure(figsize=(18, 5))\n",
    "\n",
    "ax = plt.subplot(211)\n",
    "plt.title(\"Simulated Dataset 1.\")\n",
    "for i in range(sequence.shape[1]):\n",
    "    plt.plot(sequence[:, i], label=f'feat {i+1}')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "\n",
    "ax = plt.subplot(212)\n",
    "ax.set_title(\"Simulated Dataset 2.\")\n",
    "\n",
    "for i in range(sequence.shape[1]):\n",
    "    plt.plot(sequence2[:, i], label=f'feat {i+1}')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make some Wiener Noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wiener_process(batch:int, n_sample_wiener:int, n_feat_wiener:int):\n",
    "    d_noise = tf.random.normal([batch, n_sample_wiener, n_feat_wiener])\n",
    "    wiener_noise = tf.math.cumsum(d_noise, axis=1)\n",
    "    return wiener_noise\n",
    "\n",
    "\n",
    "seed = wiener_process(NUM_SEQUENCE_TO_GENERATE, N_SAMPLE_WIENER, FEAT_WIENER)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_arrow(A, B, color=\"b\"):\n",
    "    plt.arrow(A[0], A[1], B[0] - A[0], B[1] - A[1],\n",
    "              length_includes_head=True, color=color)\n",
    "    \n",
    "def draw_arrows(xs, ys, color=\"b\"):\n",
    "    for i in range(xs.shape[0]-1):\n",
    "        point0 = [xs[i], ys[i]]\n",
    "        point1 = [xs[i+1], ys[i+1]]\n",
    "        draw_arrow(point0, point1, color=color)\n",
    "\n",
    "plt.figure(figsize=(18, 5))\n",
    "plt.title(\"Example of the wiener process.\")\n",
    "\n",
    "draw_arrows(seed[0,:,0], seed[0,:,1], color=\"tab:blue\")\n",
    "plt.scatter(seed[0,:,0], seed[0,:,1], label='Wiener Process.', color='tab:blue')\n",
    "\n",
    "plt.grid()\n",
    "plt.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Model Architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define AdaIN Layers for Time Series\n",
    "class AdaIN(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(AdaIN, self).__init__()\n",
    "\n",
    "    def get_mean_std(self, x, eps=1e-5):\n",
    "        _mean, _variance = tf.nn.moments(x, axes=[1], keepdims=True)\n",
    "        standard_dev = tf.sqrt(_variance+ eps)\n",
    "        return _mean, standard_dev\n",
    "\n",
    "    def call(self, content_input, style_input):\n",
    "        # print(content_input.shape, style_input.shape)\n",
    "        content_mean, content_std = self.get_mean_std(content_input)\n",
    "        style_mean, style_std = self.get_mean_std(style_input)\n",
    "        adain_res =style_std* (content_input - content_mean) / content_std+ style_mean\n",
    "        return adain_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_part(content_input, n_sample_wiener:int, feat_wiener:int, style_input:int):\n",
    "    init = tf.keras.initializers.RandomNormal()\n",
    "\n",
    "    # Make a small projection...\n",
    "    _content_input = tf.keras.layers.Flatten()(content_input)\n",
    "    _content_input = tf.keras.layers.Dense(n_sample_wiener* feat_wiener, kernel_initializer=init)(_content_input)\n",
    "    _content_input = tf.keras.layers.Reshape((n_sample_wiener, feat_wiener))(_content_input)\n",
    "\n",
    "    # Make the style input \n",
    "    _style_input = tf.keras.layers.Dense(16)(style_input)\n",
    "    _style_input = tf.keras.layers.Reshape((16, 1))(_style_input)\n",
    "\n",
    "    x = AdaIN()(_content_input, _style_input)\n",
    "    x = tf.keras.layers.Conv1DTranspose(256, 5, 2, padding='same', kernel_initializer=init)(x)\n",
    "    x = tf.keras.layers.LeakyReLU()(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv1DTranspose(1, 5, 2, padding='same', kernel_initializer=init)(x)\n",
    "    x = tf.keras.layers.LeakyReLU()(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "def make_generator(n_sample_wiener:int, feat_wiener:int, style_vector_size:int, n_generators:int):\n",
    "    \n",
    "    input = tf.keras.Input((n_sample_wiener, feat_wiener), name=f\"Content_Input_{i}\")\n",
    "    style_input = tf.keras.Input((style_vector_size,), name=\"Style_Input\") \n",
    "    gens_outputs = []\n",
    "\n",
    "    for _ in range(n_generators):\n",
    "        gens_outputs.append(generator_part(input, n_sample_wiener, feat_wiener, style_input))\n",
    "\n",
    "\n",
    "    test = tf.keras.layers.concatenate(gens_outputs, axis=-1)\n",
    "\n",
    "    model = tf.keras.Model([input, style_input], test)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_global_discriminator(seq_length:int, n_feat:int, n_classes:int):\n",
    "    _input = tf.keras.Input((seq_length, n_feat))\n",
    "\n",
    "    x = tf.keras.layers.Conv1D(32, 5, 2, padding='same')(_input)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "\n",
    "    x = layers.Flatten()(x)\n",
    "\n",
    "    _output = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    _class_output = layers.Dense(n_classes, activation=\"sigmoid\")(x)\n",
    "\n",
    "    model = tf.keras.Model(_input, [_output, _class_output], name=\"global_discriminator\")\n",
    "    early_predictor = tf.keras.Model(_input, x, name=\"early_discriminator\")\n",
    "\n",
    "    return model, early_predictor\n",
    "\n",
    "\n",
    "def local_discriminator_part(_input, n_classes:int):\n",
    "\n",
    "    x = tf.keras.layers.Conv1D(16, 5, 2, padding='same')(_input)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv1D(16, 5, 2, padding='same')(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "\n",
    "    x = layers.Flatten()(x)\n",
    "    _output = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    _class_output = layers.Dense(n_classes, activation=\"sigmoid\")(x)\n",
    "\n",
    "    return _output, _class_output\n",
    "\n",
    "\n",
    "def create_local_discriminator(n_signals:int, sequence_length:int, n_styles:int):\n",
    "    sig_inputs = tf.keras.Input((sequence_length, n_signals))\n",
    "    splited_inputs = tf.split(sig_inputs, n_signals, axis=-1)\n",
    "\n",
    "    crit_outputs = []\n",
    "    style_outputs = []\n",
    "\n",
    "    for sig_input in splited_inputs:\n",
    "        crit_output, style_output = local_discriminator_part(sig_input, n_styles)\n",
    "        crit_outputs.append(crit_output)\n",
    "        style_outputs.append(style_output)\n",
    "\n",
    "    crit_outputs = tf.keras.layers.concatenate(crit_outputs, axis=-1, name=\"crit_output\")\n",
    "\n",
    "    model_output = [crit_outputs]\n",
    "    model_output.extend(style_outputs)\n",
    "\n",
    "    return tf.keras.Model(sig_inputs, model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generators = make_generator(N_SAMPLE_WIENER, FEAT_WIENER, STYLE_VECTOR, df_simulated.shape[-1])\n",
    "\n",
    "local_discriminators = create_local_discriminator(df_simulated.shape[-1], SEQUENCE_LENGTH, N_STYLE)\n",
    "\n",
    "global_discriminator, early_predictor = make_global_discriminator(SEQUENCE_LENGTH, df_simulated.shape[1], N_STYLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(local_discriminators, show_shapes=True)\n",
    "# local_discriminators.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot a Sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(content_wp, style, training=True):\n",
    "    style_labels = tf.zeros((NUM_SEQUENCE_TO_GENERATE, 1))+ style\n",
    "\n",
    "    signals = generators([content_wp, style_labels])\n",
    "\n",
    "    # signals = tf.transpose(signals, (1, 2, 0, 3))\n",
    "    # signals = tf.reshape(signals, signals.shape[:-1])\n",
    "    return signals\n",
    "\n",
    "def local_discrimination(sequences, training=True):\n",
    "    crit = np.array([d(sequences[:, :, i], training=training) for i, d in enumerate(local_discriminators)])\n",
    "    # crit = tf.transpose(crit, (1, 0, 2))\n",
    "    return crit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate the reference signature.\n",
    "real_sequence_style1 = next(iter(dset_simulated))\n",
    "real_sequence_style2 = next(iter(dset_simulated2))\n",
    "\n",
    "real_batch_signature_style1= signature_on_batch(real_sequence_style1, config.met_params.ins, config.met_params.outs, config.met_params.signature_length)\n",
    "real_batch_signature_style2= signature_on_batch(real_sequence_style2, config.met_params.ins, config.met_params.outs, config.met_params.signature_length)\n",
    "\n",
    "# Generate Sequences...\n",
    "generations_style1 = generate(seed, 0.)\n",
    "generations_style2 = generate(seed, 1.)\n",
    "\n",
    "generated_batch_signature_style1 = signature_on_batch(generations_style1, config.met_params.ins, config.met_params.outs, config.met_params.signature_length)\n",
    "generated_batch_signature_style2 = signature_on_batch(generations_style2, config.met_params.ins, config.met_params.outs, config.met_params.signature_length)\n",
    "\n",
    "style_1_metric = signature_metric(real_batch_signature_style1, generated_batch_signature_style1)\n",
    "style_2_metric = signature_metric(real_batch_signature_style2, generated_batch_signature_style2)\n",
    "\n",
    "print(f'Metric Style 1: {style_1_metric:0.2f}')\n",
    "print(f'Metric Style 2: {style_2_metric:0.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def integrate_signatures(ax:plt.Axes, real_signature:np.ndarray, fake_signature:np.ndarray):\n",
    "    t = np.arange(real_signature.shape[1])\n",
    "\n",
    "    _real_min, _real_max, _real_mean = np.mean(real_signature[:, : , 0], 0), np.mean(real_signature[:, : , 1], 0), np.mean(real_signature[:, : , 2], 0)\n",
    "    _fake_min, _fake_max, _fake_mean = np.mean(fake_signature[:, : , 0], 0),  np.mean(fake_signature[:, : , 1], 0),  np.mean(fake_signature[:, : , 2], 0)\n",
    "\n",
    "    plt.plot(_real_min, 'g')\n",
    "    plt.plot(_real_max, 'g')\n",
    "    plt.plot(_real_mean, 'g')\n",
    "\n",
    "    ax.fill_between(t, _real_min, _real_max, color=\"g\", label=\"Signature From Real data.\", alpha=0.25)\n",
    "\n",
    "    plt.plot(_fake_min, 'b')\n",
    "    plt.plot(_fake_max, 'b')\n",
    "    plt.plot(_fake_mean, 'b')\n",
    "\n",
    "    ax.fill_between(t, _fake_min, _fake_max, color=\"b\", label=\"Signature From Fake data.\", alpha=0.25)\n",
    "\n",
    "    ax.grid(True)\n",
    "\n",
    "    return ax\n",
    "\n",
    "\n",
    "# after_training_generations\n",
    "def plot_several_generations(\n",
    "        real_sequence1,\n",
    "        real_sequence2, \n",
    "        real_signature1, \n",
    "        real_signature2, \n",
    "        generated_signature1,\n",
    "        generated_signature2,\n",
    "        generations_style1:np.ndarray,\n",
    "        generations_style2:np.ndarray, \n",
    "        return_fig=False):\n",
    "    nvertical=3\n",
    "    nhoriz=4\n",
    "\n",
    "    legend = [f\"feat {j}\" for j in range(generations_style1.shape[-1])]\n",
    "\n",
    "    fig = plt.figure(figsize=(18, 10))\n",
    "    plt.suptitle(\"Several Generations\")\n",
    "\n",
    "    # Plot Some Usefull Things.\n",
    "\n",
    "    ax = plt.subplot(nvertical, nhoriz, 1)\n",
    "    ax.set_title(\"Real Sequence, Style 1.\")\n",
    "    plt.plot(real_sequence1[0])\n",
    "    ax.grid(True)\n",
    "    plt.legend(legend)\n",
    "\n",
    "\n",
    "    ax = plt.subplot(nvertical, nhoriz, 2)\n",
    "    ax.set_title(\"Real Sequence, Style 2.\")\n",
    "    plt.plot(real_sequence2[0])\n",
    "    ax.grid(True)\n",
    "    plt.legend(legend)\n",
    "\n",
    "\n",
    "    ax = plt.subplot(nvertical, nhoriz, 3)\n",
    "    ax.set_title(\"Signature Difference, Style 1.\")\n",
    "    ax = integrate_signatures(ax, real_signature1, generated_signature1)\n",
    "    \n",
    "    ax.legend()\n",
    "\n",
    "    \n",
    "\n",
    "    ax = plt.subplot(nvertical, nhoriz, 4)\n",
    "    ax.set_title(\"Signature Difference, Style 2.\")\n",
    "    ax = integrate_signatures(ax, real_signature2, generated_signature2)\n",
    "    ax.legend()\n",
    "\n",
    "\n",
    "\n",
    "    for i in range(nhoriz, nvertical* nhoriz):\n",
    "        ax = plt.subplot(nvertical, nhoriz, i+ 1)\n",
    "\n",
    "        if i%2 == 0:\n",
    "            ax.set_title(f\"sequence {i+1}, Style 1.\")\n",
    "            plt.plot(generations_style1[i])\n",
    "        else:\n",
    "            ax.set_title(f\"sequence {i+1}, Style 2.\")\n",
    "            plt.plot(generations_style2[i])\n",
    "        ax.grid(True)\n",
    "        plt.legend(legend)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if return_fig == True:\n",
    "        return fig\n",
    "    \n",
    "    # plt.show()\n",
    "\n",
    "plot_several_generations(real_sequence_style1,\n",
    "                         real_sequence_style2,\n",
    "                         real_batch_signature_style1,\n",
    "                         real_batch_signature_style2,\n",
    "                         generated_batch_signature_style1,\n",
    "                         generated_batch_signature_style2,\n",
    "                         generations_style1,\n",
    "                         generations_style2,\n",
    "                         return_fig=True\n",
    "                         ) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "\n",
    "def similarity_loss(extracted_features:np.ndarray):\n",
    "    anchor = extracted_features[0]\n",
    "    return tf.exp(-(tf.norm(extracted_features[1:]- anchor)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_opt = tf.keras.optimizers.RMSprop(1e-2)\n",
    "discriminator_opt = tf.keras.optimizers.legacy.RMSprop(2e-3)\n",
    "global_discriminator_opt = tf.keras.optimizers.RMSprop(2e-3)\n",
    "\n",
    "# generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "# discriminator_optimizer = tf.keras.optimizers.Adam(1e-6)    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_metric = tf.keras.metrics.Mean()\n",
    "local_discriminator_metric = tf.keras.metrics.Mean()\n",
    "global_discriminator_metric = tf.keras.metrics.Mean()\n",
    "similarity_metric = tf.keras.metrics.Mean()\n",
    "\n",
    "correlation_metric_style1 = tf.keras.metrics.Mean()\n",
    "correlation_metric_style2 = tf.keras.metrics.Mean()\n",
    "\n",
    "local_classification_loss = tf.keras.metrics.Mean()\n",
    "global_classification_loss= tf.keras.metrics.Mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_str = datetime.now().strftime('%Y-%m-%d_%H_%M_%S')\n",
    "\n",
    "BASE_DIR = f\"logs - conditional COSCI-GAN/{date_str} - Conditional COSCI-GAN other calc and no ppc\"\n",
    "TRAIN_LOGS_DIR_PATH = f\"{BASE_DIR}/fit\"\n",
    "GENERATION_LOG = f\"{BASE_DIR}/Generations\"\n",
    "\n",
    "TRAIN_SUMMARY_WRITER = tf.summary.create_file_writer(TRAIN_LOGS_DIR_PATH)\n",
    "\n",
    "\n",
    "def plot_to_buff(real_sequence1,\n",
    "                 real_sequence2,\n",
    "                 real_signature1,\n",
    "                 real_signature2,\n",
    "                 generated_signature1,\n",
    "                 generated_signature2,\n",
    "                 generations_style1,\n",
    "                 generations_style2):\n",
    "\n",
    "    fig = plot_several_generations(real_sequence1,\n",
    "                                   real_sequence2,\n",
    "                                   real_signature1,\n",
    "                                   real_signature2,\n",
    "                                   generated_signature1,\n",
    "                                   generated_signature2,\n",
    "                                   generations_style1,\n",
    "                                   generations_style2, \n",
    "                                   return_fig=True)\n",
    "\n",
    "\n",
    "    buf = io.BytesIO()\n",
    "    fig.savefig(buf, format='png')\n",
    "    buf.seek(0)\n",
    "    plt.close(fig)\n",
    "    return buf\n",
    "\n",
    "\n",
    "def log_losses(epoch, plot_buf):\n",
    "    image = tf.image.decode_png(plot_buf.getvalue(), channels=4)\n",
    "    image = tf.expand_dims(image, 0)\n",
    "\n",
    "    with TRAIN_SUMMARY_WRITER.as_default():\n",
    "        tf.summary.scalar(\"Generator Loss\", generator_metric.result(), step=epoch)\n",
    "\n",
    "        tf.summary.scalar(\"Local D loss\", local_discriminator_metric.result(), step=epoch)\n",
    "        tf.summary.scalar(\"Gobal D loss\", global_discriminator_metric.result(), step=epoch)\n",
    "        \n",
    "        tf.summary.scalar(\"Mode Colapsing ?\", similarity_metric.result(), step=epoch)\n",
    "        tf.summary.scalar(\"Correlation Metric Style 1\", correlation_metric_style1.result(), step=epoch)\n",
    "        tf.summary.scalar(\"Correlation Metric Style 2\", correlation_metric_style2.result(), step=epoch)\n",
    "\n",
    "        tf.summary.image(\"Training Generations\", image, step=epoch)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_plots(noise, save_to):\n",
    "    generated = generate(seed)\n",
    "\n",
    "    fig =plt.figure(figsize=(18, 5))\n",
    "    plt.title(\"Generation of the GAN during Training.\")\n",
    "    for i in range(generated.shape[-1]):\n",
    "        plt.plot(generated[0, :, i], label=f'feat {i+1}')\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "\n",
    "    plt.savefig(save_to)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc = tf.keras.losses.BinaryCrossentropy()\n",
    "error_classif = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "def local_discriminator_loss(crits_on_real, crits_on_fake):\n",
    "    individual_losses = []\n",
    "\n",
    "    for i in range(crits_on_real.shape[0]):\n",
    "        l1 = bc(tf.zeros_like(crits_on_real), crits_on_fake[i])\n",
    "        l2 = bc(tf.ones_like(crits_on_real), crits_on_real[i])\n",
    "        loss = (l1+ l2)/2\n",
    "        individual_losses.append(loss)\n",
    "        \n",
    "    return individual_losses\n",
    "\n",
    "\n",
    "def local_generator_loss(crit_on_fake):\n",
    "    individual_losses = []\n",
    "    true_label = tf.zeros(crit_on_fake[0].shape)\n",
    "\n",
    "    for i in range(crit_on_fake.shape[0]):\n",
    "        individual_losses.append(bc(true_label, crit_on_fake[i]))\n",
    "        \n",
    "    return individual_losses\n",
    "\n",
    "def style_classsification_loss(y_pred, y_true):\n",
    "    return error_classif(y_true, y_pred)\n",
    "\n",
    "def global_discriminator_loss(crit_on_real, crit_on_fake):\n",
    "    l1 = bc(tf.zeros_like(crit_on_fake), crit_on_fake)\n",
    "    l2 = bc(tf.ones_like(crit_on_real), crit_on_real)\n",
    "    loss = (l1+ l2)/2\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def global_generator_loss(crit_on_fake):\n",
    "    loss = bc(tf.ones_like(crit_on_fake), crit_on_fake)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_local_predictions(sequences, training=True):\n",
    "    _crit_classif = []\n",
    "    _crit_styles = []\n",
    "\n",
    "    for i in range(len(local_discriminators)):\n",
    "        crit, styles = local_discriminators[i](sequences[:,:, i], training=training)\n",
    "        _crit_classif.append(crit)\n",
    "        _crit_styles.append(styles)\n",
    "\n",
    "    return np.array(_crit_classif), np.array(_crit_styles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def discriminators_step(style1_ts, style2_ts, generators_, local_discriminators_, global_discriminator_):\n",
    "    noise= wiener_process(BS, N_SAMPLE_WIENER, FEAT_WIENER)\n",
    "    style_labels = tf.zeros((BS,))\n",
    "\n",
    "    with tf.GradientTape(persistent=True) as d_tape:\n",
    "        style1_generated = generators_([noise, style_labels], training=False)\n",
    "        style2_generated = generators_([noise, style_labels+ 1.], training=False)\n",
    "\n",
    "        local_gen1 = local_discriminators_(style1_generated, training=True)\n",
    "        local_g1_crit = local_gen1[0]  #We DO NOT care about style of generation of the discriminator step.\n",
    "\n",
    "        local_gen2 = local_discriminators_(style2_generated, training=True)\n",
    "        local_g2_crit = local_gen2[0] #We DO NOT care about style of generation of the discriminator step.\n",
    "\n",
    "        local_real1 = local_discriminators_(style1_ts, training=True)\n",
    "        local_r1_crit, local_r1_styles = local_real1[0], local_real1[1:]\n",
    "\n",
    "        local_real2 = local_discriminators_(style2_ts, training=True)\n",
    "        local_r2_crit, local_r2_styles =  local_real2[0], local_real2[1:]\n",
    "\n",
    "        g_crit_gen1, _ = global_discriminator_(style1_generated, training=True)\n",
    "        g_crit_gen2, _ = global_discriminator_(style2_generated, training=True)\n",
    "\n",
    "        g_crit_real1, g_style_real1= global_discriminator_(style1_ts, training=True)\n",
    "        g_crit_real2, g_style_real2= global_discriminator_(style2_ts, training=True)\n",
    "\n",
    "        # (LOCAL DISCRIMINATOR) Real / Fake Loss \n",
    "        local_d_losses1 = local_discriminator_loss(local_g1_crit, local_r1_crit)\n",
    "        local_d_losses2 = local_discriminator_loss(local_g2_crit, local_r2_crit)\n",
    "        local_d_losses = tf.add(local_d_losses1, local_d_losses2)\n",
    "\n",
    "        #(LOCAL DISCRIMINATOR) Style CLassification:\n",
    "        style_labels = tf.zeros((BS, 1))\n",
    "        local_s1_loss = [style_classsification_loss(local_r1_styles[i], style_labels) for i in range(style1_ts.shape[-1])]\n",
    "        local_s2_loss = [style_classsification_loss(local_r2_styles[i], style_labels+ 1.) for i in range(style1_ts.shape[-1])]\n",
    "        local_style_loss = tf.add(local_s1_loss, local_s2_loss)\n",
    "        local_style_loss = tf.concat((local_d_losses, local_style_loss), 0)\n",
    "\n",
    "        # (GLOBAL DISCRIMINATOR) Real/ Fake Loss\n",
    "        global_d_loss1 = global_discriminator_loss(g_crit_real1, g_crit_gen1)\n",
    "        global_d_loss2 = global_discriminator_loss(g_crit_real2, g_crit_gen2)\n",
    "        global_discrimination_loss = tf.add(global_d_loss1, global_d_loss2)\n",
    "\n",
    "        # (GLOBAL DISCRIMINATOR) Style Classification\n",
    "        global_classif1 = style_classsification_loss(g_style_real1, style_labels)\n",
    "        global_classif2 = style_classsification_loss(g_style_real2, style_labels+ 1.)\n",
    "        global_style_loss = tf.add(global_classif1, global_classif2)\n",
    "\n",
    "    # start = datetime.now()\n",
    "    # (GOBAL DISCRIMINATOR): Real / Fake and style\n",
    "    global_discr_gradient = d_tape.gradient([global_discrimination_loss, global_style_loss], global_discriminator_.trainable_variables)\n",
    "    global_discriminator_opt.apply_gradients(zip(global_discr_gradient, global_discriminator_.trainable_variables)) \n",
    "    \n",
    "    grads = d_tape.gradient(local_style_loss, local_discriminators_.trainable_variables)\n",
    "    discriminator_opt.apply_gradients(zip(grads, local_discriminators_.trainable_variables))\n",
    "\n",
    "    local_discriminator_metric(tf.reduce_mean(local_d_losses))\n",
    "    global_discriminator_metric(global_discrimination_loss)\n",
    "    # print(\" - \", datetime.now() - start)\n",
    "    \n",
    "@tf.function\n",
    "def generator_step(generators_, local_discriminators_, _global_discriminator):\n",
    "    noise= wiener_process(BS, N_SAMPLE_WIENER, FEAT_WIENER)\n",
    "    style_labels = tf.zeros((BS,))\n",
    "    \n",
    "    with tf.GradientTape(persistent=True) as g_tape:\n",
    "        style1_generated = generators_([noise, style_labels], training=False)\n",
    "        style2_generated = generators_([noise, style_labels+ 1.], training=False)\n",
    "\n",
    "        # (LOCAL DISCRIMINATOR): Fakeness Predictions\n",
    "        local_gen1 = local_discriminators_(style1_generated, training=True)\n",
    "        local_d1_crit, local_d1_styles =  local_gen1[0], local_gen1[1:]\n",
    "\n",
    "        local_gen2 = local_discriminators_(style2_generated, training=True)\n",
    "        local_d2_crit, local_d2_styles =  local_gen2[0], local_gen2[1:]\n",
    "\n",
    "        # (GLOBAL DISCRIMINATOR): Predictions.\n",
    "        g_crit_gen1, g_style1 = _global_discriminator(style1_generated, training=False)\n",
    "        g_crit_gen2, g_style2 = _global_discriminator(style2_generated, training=False)\n",
    "\n",
    "        # (LOCAL DISCRIMINATOR): Real/ Fake.\n",
    "        local_g_loss1 = local_generator_loss(local_d1_crit)\n",
    "        local_g_loss2 = local_generator_loss(local_d2_crit)\n",
    "        local_g_loss = tf.add(local_g_loss1, local_g_loss2)\n",
    "\n",
    "        # (LOCAL DISCRIMINATOR): Style.\n",
    "        style_labels = tf.zeros((BS, 1))\n",
    "        local_s1_loss = [style_classsification_loss(local_d1_styles[i], style_labels) for i in range(style1_generated.shape[-1])]\n",
    "        local_s2_loss = [style_classsification_loss(local_d2_styles[i], style_labels+ 1.) for i in range(style1_generated.shape[-1])]\n",
    "        local_style_preservation = tf.add(local_s1_loss, local_s2_loss)\n",
    "\n",
    "        # (GLOBAL DISCRIMINATOR): Real / Fake.\n",
    "        global_g_loss1= global_generator_loss(g_crit_gen1)\n",
    "        global_g_loss2= global_generator_loss(g_crit_gen2)\n",
    "        global_g_loss = tf.add(global_g_loss1, global_g_loss2)\n",
    "\n",
    "        # (GLOBAL DISCRIMINATOR): Style Classification.\n",
    "        global_style1_preservation = style_classsification_loss(g_style1, style_labels)\n",
    "        global_style2_preservation = style_classsification_loss(g_style2, style_labels+ 1.)\n",
    "        global_style_preservation = tf.add(global_style1_preservation, global_style2_preservation)\n",
    "\n",
    "        # LAMBDA_GLOBAL* (global_g_loss+ global_style_preservation)+\n",
    "        g_style = tf.reduce_mean(LAMBDA_LOCAL* (global_style_preservation+ local_style_preservation))\n",
    "        c_realness=tf.reduce_mean(LAMBDA_GLOBAL* (global_g_loss+ local_g_loss))\n",
    "        g_loss = tf.reduce_mean(g_style+ c_realness)\n",
    "        \n",
    "    grads = g_tape.gradient(g_loss, generators.trainable_variables)\n",
    "    generator_opt.apply_gradients(zip(grads, generators.trainable_variables))\n",
    "\n",
    "    generator_metric(tf.reduce_mean(local_g_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset1, dataset2, epochs):\n",
    "  losses = []\n",
    "  total_steps = \"?\"\n",
    "  for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "\n",
    "    generator_metric.reset_states()\n",
    "    global_discriminator_metric.reset_states()\n",
    "    local_discriminator_metric.reset_states()\n",
    "\n",
    "    for s, (sequence1, sequence2) in enumerate(zip(dataset1, dataset2)):\n",
    "      if s % UPDATE_DISCRIMINATOR == 0:\n",
    "        discriminators_step(sequence1, sequence2, generators, local_discriminators, global_discriminator)\n",
    "        \n",
    "      generator_step(generators, local_discriminators, global_discriminator)\n",
    "\n",
    "      print(f\"\\r e {epoch}/{epochs}, s {s}/{total_steps}: Gen {generator_metric.result():0.4f}; Global discriminator: {global_discriminator_metric.result():0.4f}; Local discriminator: {local_discriminator_metric.result():0.4f}; Sim loss: {similarity_metric.result():0.4f}\", end=\"\")\n",
    "      # break\n",
    "    if epoch == 0:\n",
    "      total_steps = s\n",
    "\n",
    "    # stop = time.time()\n",
    "    # print()\n",
    "    # print(f\"\\r[+] Epoch {epoch}/{epochs} in {(stop-start):0.4f} seconds. ({(stop-start)/total_steps:0.4f} s/step)\")\n",
    "\n",
    "    # generate_plots(seed, f\"imgs/GAN_generations/{epoch}.png\")\n",
    "    # Make generations on seed\n",
    "    seed_generation1 = generate(seed, 0., training=False)\n",
    "    seed_generation2 = generate(seed, 1., training=False)\n",
    "\n",
    "    batch_signature_style1 = signature_on_batch(seed_generation1, [0, 1], [2, 3, 4, 5], config.met_params.signature_length)\n",
    "    signature_difference1 = signature_metric(real_batch_signature_style1, batch_signature_style1)\n",
    "\n",
    "    batch_signature_style2 = signature_on_batch(seed_generation2, [0, 1], [2, 3, 4, 5], config.met_params.signature_length)\n",
    "    signature_difference2 = signature_metric(real_batch_signature_style2, batch_signature_style2)\n",
    "\n",
    "    buff = plot_to_buff(real_sequence_style1, real_sequence_style2, real_batch_signature_style1, real_batch_signature_style2, batch_signature_style1, batch_signature_style2, seed_generation1, seed_generation2)\n",
    "\n",
    "    correlation_metric_style1(signature_difference1)\n",
    "    correlation_metric_style2(signature_difference2)\n",
    "\n",
    "    l = [generator_metric.result(), global_discriminator_metric.result(), local_discriminator_metric.result()]\n",
    "    losses.append(l)\n",
    "    log_losses(epoch, buff)\n",
    "\n",
    "  return np.array(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_losses = train(dset_simulated, dset_simulated2, EPOCHS)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 5))\n",
    "plt.title(\"Training Losses.\")\n",
    "plt.plot(training_losses[:, 0], \".-\", label=\"Generator Loss\")\n",
    "plt.plot(training_losses[:, 1], \".-\", label=\"Discriminator Loss\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's Test it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "after_training_generations = generator(seed, training=False)\n",
    "\n",
    "plt.figure(figsize=(18, 5))\n",
    "plt.title(\"Generation of the GAN whitout Training.\")\n",
    "for i in range(after_training_generations.shape[-1]):\n",
    "    plt.plot(after_training_generations[0, :, i], label=f'feat {i+1}')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "after_training_decision = discriminator(after_training_generations)\n",
    "after_training_decision[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test if Mode Colapsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after_training_generations\n",
    "def plot_several_generations(generations:np.ndarray, nvertical:int=3, nhoriz:int=3):\n",
    "\n",
    "    legend = [f\"feat {j}\" for j in range(generations.shape[-1])]\n",
    "\n",
    "    plt.figure(figsize=(18, 10))\n",
    "    plt.suptitle(\"Generations After GAN Training.\")\n",
    "\n",
    "    for i in range(nvertical* nhoriz):\n",
    "        ax = plt.subplot(nvertical, nhoriz, i+ 1)\n",
    "        ax.set_title(f\"sequence {i+1}\")\n",
    "\n",
    "        plt.plot(generations[i])\n",
    "        ax.grid(True)\n",
    "        plt.legend(legend)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_several_generations(after_training_generations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
