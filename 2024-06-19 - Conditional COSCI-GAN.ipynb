{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MTS Generation with COSCI-GAN.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from configs.Metric import Metric\n",
    "from configs.SimulatedData import Proposed\n",
    "from utils.metric import signature_on_batch, signature_metric\n",
    "import mlflow\n",
    "from sklearn.decomposition import PCA\n",
    "from datetime import datetime\n",
    "import io\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs.\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized.\n",
    "    print(e)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config= Proposed()\n",
    "\n",
    "SEQUENCE_LENGTH = config.sequence_lenght_in_sample\n",
    "GRANUARITY = config.granularity\n",
    "OVERLAP = config.overlap\n",
    "BS = config.batch_size\n",
    "EPOCHS = config.epochs\n",
    "NUM_SEQUENCE_TO_GENERATE = config.met_params.sequence_to_generate\n",
    "STEP_LIMITATION = 1000\n",
    "UPDATE_DISCRIMINATOR = 20\n",
    "\n",
    "SIMULATED_DATA_PATH = \"data/simulated_dataset/01 - Source Domain.h5\"\n",
    "SIMULATED_DATA_PATH2= \"data/simulated_dataset/output_noise/0.25.h5\"\n",
    "\n",
    "N_SAMPLE_WIENER = SEQUENCE_LENGTH//4\n",
    "FEAT_WIENER = 2\n",
    "NOISE_DIM= (N_SAMPLE_WIENER, FEAT_WIENER)\n",
    "\n",
    "LAMBDA_GLOBAL = 0.001\n",
    "LAMBDA_LOCAL = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dataframe_to_tensorflow_sequences(df:pd.DataFrame, sequence_lenght_in_sample, granularity, shift_between_sequences, batch_size, shuffle=True):\n",
    "    sequence_lenght = int(sequence_lenght_in_sample*granularity)\n",
    "\n",
    "    dset = tf.data.Dataset.from_tensor_slices(df.values)\n",
    "    dset = dset.window(sequence_lenght , shift=shift_between_sequences, stride=granularity).flat_map(lambda x: x.batch(sequence_lenght_in_sample, drop_remainder=True))\n",
    "\n",
    "    if shuffle:\n",
    "        dset= dset.shuffle(256)\n",
    "\n",
    "    dset = dset.batch(batch_size, drop_remainder=True)\n",
    "\n",
    "    dset = dset.cache().prefetch(10)\n",
    "\n",
    "    return dset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_simulated = pd.read_hdf(SIMULATED_DATA_PATH)\n",
    "df_simulated = df_simulated.drop(columns='labels')\n",
    "\n",
    "df_simulated2 = pd.read_hdf(SIMULATED_DATA_PATH2)\n",
    "df_simulated2 = df_simulated2.drop(columns='labels')\n",
    "\n",
    "dset_simulated = convert_dataframe_to_tensorflow_sequences(\n",
    "    df_simulated, \n",
    "    SEQUENCE_LENGTH, \n",
    "    GRANUARITY, \n",
    "    int(OVERLAP* SEQUENCE_LENGTH),\n",
    "    BS\n",
    ")\n",
    "\n",
    "dset_simulated2 = convert_dataframe_to_tensorflow_sequences(\n",
    "    df_simulated2, \n",
    "    SEQUENCE_LENGTH, \n",
    "    GRANUARITY, \n",
    "    int(OVERLAP* SEQUENCE_LENGTH),\n",
    "    BS\n",
    ")\n",
    "\n",
    "dset_simulated = dset_simulated.take(STEP_LIMITATION)\n",
    "dset_simulated2 = dset_simulated2.take(STEP_LIMITATION)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify if it is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = next(iter(dset_simulated))[0]\n",
    "sequence2= next(iter(dset_simulated2))[0]\n",
    "\n",
    "plt.figure(figsize=(18, 5))\n",
    "\n",
    "ax = plt.subplot(211)\n",
    "plt.title(\"Simulated Dataset 1.\")\n",
    "for i in range(sequence.shape[1]):\n",
    "    plt.plot(sequence[:, i], label=f'feat {i+1}')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "\n",
    "ax = plt.subplot(212)\n",
    "ax.set_title(\"Simulated Dataset 2.\")\n",
    "\n",
    "for i in range(sequence.shape[1]):\n",
    "    plt.plot(sequence2[:, i], label=f'feat {i+1}')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make some Wiener Noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wiener_process(batch:int, n_sample_wiener:int, n_feat_wiener:int):\n",
    "    d_noise = tf.random.normal([batch, n_sample_wiener, n_feat_wiener])\n",
    "    wiener_noise = tf.math.cumsum(d_noise, axis=1)\n",
    "    return wiener_noise\n",
    "\n",
    "\n",
    "seed = wiener_process(NUM_SEQUENCE_TO_GENERATE, N_SAMPLE_WIENER, FEAT_WIENER)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_arrow(A, B, color=\"b\"):\n",
    "    plt.arrow(A[0], A[1], B[0] - A[0], B[1] - A[1],\n",
    "              length_includes_head=True, color=color)\n",
    "    \n",
    "def draw_arrows(xs, ys, color=\"b\"):\n",
    "    for i in range(xs.shape[0]-1):\n",
    "        point0 = [xs[i], ys[i]]\n",
    "        point1 = [xs[i+1], ys[i+1]]\n",
    "        draw_arrow(point0, point1, color=color)\n",
    "\n",
    "plt.figure(figsize=(18, 5))\n",
    "plt.title(\"Example of the wiener process.\")\n",
    "\n",
    "draw_arrows(seed[0,:,0], seed[0,:,1], color=\"tab:blue\")\n",
    "plt.scatter(seed[0,:,0], seed[0,:,1], label='Wiener Process.', color='tab:blue')\n",
    "\n",
    "plt.grid()\n",
    "plt.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Model Architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define AdaIN Layers for Time Series\n",
    "class AdaIN(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(AdaIN, self).__init__()\n",
    "\n",
    "    def get_mean_std(self, x, eps=1e-5):\n",
    "        _mean, _variance = tf.nn.moments(x, axes=[1], keepdims=True)\n",
    "        standard_dev = tf.sqrt(_variance+ eps)\n",
    "        return _mean, standard_dev\n",
    "\n",
    "    def call(self, content_input, style_input):\n",
    "        # print(content_input.shape, style_input.shape)\n",
    "        content_mean, content_std = self.get_mean_std(content_input)\n",
    "        style_mean, style_std = self.get_mean_std(style_input)\n",
    "        adain_res =style_std* (content_input - content_mean) / content_std+ style_mean\n",
    "        return adain_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generator(n_sample_wiener:int, feat_wiener:int, style_vector_size:int):\n",
    "    init = tf.keras.initializers.RandomNormal()\n",
    "\n",
    "    content_input = tf.keras.Input((n_sample_wiener, feat_wiener), name=\"Content Input\")\n",
    "    style_input = tf.keras.Input((style_vector_size,), name=\"Style Input\") \n",
    "\n",
    "    # Make a small projection...\n",
    "    _content_input = tf.keras.layers.Flatten()(content_input)\n",
    "    _content_input = tf.keras.layers.Dense(n_sample_wiener* feat_wiener, kernel_initializer=init)(_content_input)\n",
    "    _content_input = tf.keras.layers.Reshape((n_sample_wiener, feat_wiener))(_content_input)\n",
    "\n",
    "    # Make the style input \n",
    "    _style_input = tf.keras.layers.Dense(16, name='1')(style_input)\n",
    "    _style_input = tf.keras.layers.Reshape((16, 1))(_style_input)\n",
    "\n",
    "    x = AdaIN()(_content_input, _style_input)\n",
    "    x = tf.keras.layers.Conv1DTranspose(256, 5, 2, padding='same', kernel_initializer=init)(x)\n",
    "    x = tf.keras.layers.LeakyReLU()(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv1DTranspose(1, 5, 2, padding='same', kernel_initializer=init)(x)\n",
    "    x = tf.keras.layers.LeakyReLU()(x)\n",
    "\n",
    "    model = tf.keras.Model([content_input, style_input], x)\n",
    "    return model\n",
    "\n",
    "\n",
    "generator = make_generator(16, 2, 1)\n",
    "generator.summary()\n",
    "NOISE_DIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_global_discriminator(seq_length:int, n_feat:int, n_classes:int):\n",
    "    _input = tf.keras.Input((seq_length, n_feat))\n",
    "\n",
    "    x = tf.keras.layers.Conv1D(32, 5, 2, padding='same')(_input)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "\n",
    "    x = layers.Flatten()(x)\n",
    "\n",
    "    _output = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    _class_output = layers.Dense(n_classes, activation=\"sigmoid\")(x)\n",
    "\n",
    "    model = tf.keras.Model(_input, [_output, _class_output])\n",
    "    early_predictor = tf.keras.Model(_input, x, name=\"Local Discriminator\")\n",
    "\n",
    "    return model, early_predictor\n",
    "\n",
    "\n",
    "def local_discriminator(seq_length:int, n_classes:int):\n",
    "    _input = tf.keras.Input((seq_length, 1))\n",
    "\n",
    "    x = tf.keras.layers.Conv1D(32, 5, 2, padding='same')(_input)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv1D(32, 5, 2, padding='same')(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "\n",
    "    x = layers.Flatten()(x)\n",
    "    _output = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    _class_output = layers.Dense(n_classes, activation=\"sigmoid\")(x)\n",
    "\n",
    "    crit = tf.keras.Model(_input, _output, name=\"local discriminator\")\n",
    "    style_crit = tf.keras.Model(_input, _class_output, name=\"Style Discriminator\")\n",
    "\n",
    "    return crit, style_crit\n",
    "\n",
    "\n",
    "def create_local_discriminators(n_discrs:int, sequence_length:int, n_style:int):\n",
    "    _local_discr = []\n",
    "    _style_disc = []\n",
    "\n",
    "    for i in range(n_discrs):\n",
    "        _crit, _s_crit = local_discriminator(sequence_length, n_style)\n",
    "\n",
    "        _local_discr.append(_crit)\n",
    "        _style_disc.append(_s_crit)\n",
    "\n",
    "    return _local_discr, _style_disc \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generators = [make_generator(16, 2, 1) for _ in range(df_simulated.shape[1])]\n",
    "\n",
    "local_discriminators, style_discrs = create_local_discriminators(df_simulated.shape[1], SEQUENCE_LENGTH, 2)\n",
    "\n",
    "global_discriminator, early_predictor = make_global_discriminator(SEQUENCE_LENGTH, df_simulated.shape[1], 2)\n",
    "\n",
    "global_discriminator.summary()\n",
    "local_discriminators[0].summary()\n",
    "style_discrs[0].summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot a Sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(content_wp, style, training=True):\n",
    "    style_labels = tf.zeros((NUM_SEQUENCE_TO_GENERATE, 1))+ style\n",
    "    signals = np.array([g([content_wp, style_labels], training=training) for g in generators])\n",
    "    signals = tf.transpose(signals, (1, 2, 0, 3))\n",
    "    signals = tf.reshape(signals, signals.shape[:-1])\n",
    "    return signals\n",
    "\n",
    "def local_discrimination(sequences, training=True):\n",
    "    crit = np.array([d(sequences[:, :, i], training=training) for i, d in enumerate(local_discriminators)])\n",
    "    # crit = tf.transpose(crit, (1, 0, 2))\n",
    "    return crit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated = generate(seed, 0.)\n",
    "\n",
    "# after_training_generations\n",
    "def plot_several_generations(generations:np.ndarray, nvertical:int=3, nhoriz:int=3):\n",
    "    legend = [f\"feat {j}\" for j in range(generations.shape[-1])]\n",
    "\n",
    "    plt.figure(figsize=(18, 10))\n",
    "    plt.suptitle(\"Several Generations\")\n",
    "\n",
    "    for i in range(nvertical* nhoriz):\n",
    "        ax = plt.subplot(nvertical, nhoriz, i+ 1)\n",
    "        ax.set_title(f\"sequence {i+1}\")\n",
    "\n",
    "        plt.plot(generations[i])\n",
    "        ax.grid(True)\n",
    "        plt.legend(legend)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_several_generations(generated) \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "\n",
    "def similarity_loss(extracted_features:np.ndarray):\n",
    "    anchor = extracted_features[0]\n",
    "    return tf.exp(-(tf.norm(extracted_features[1:]- anchor)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate the reference signature.\n",
    "real_sequence_style1 = next(iter(dset_simulated))\n",
    "real_sequence_style2 = next(iter(dset_simulated2))\n",
    "\n",
    "real_batch_signature_style1= signature_on_batch(real_sequence_style1, config.met_params.ins, config.met_params.outs, config.met_params.signature_length)\n",
    "real_batch_signature_style2= signature_on_batch(real_sequence_style2, config.met_params.ins, config.met_params.outs, config.met_params.signature_length)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_opt = [tf.keras.optimizers.RMSprop(1e-2) for _ in generators]\n",
    "discriminator_opt = [tf.keras.optimizers.legacy.RMSprop(2e-3) for _ in local_discriminators]\n",
    "global_discriminator_opt = tf.keras.optimizers.RMSprop(2e-3)\n",
    "\n",
    "# generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "# discriminator_optimizer = tf.keras.optimizers.Adam(1e-6)    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_metric = tf.keras.metrics.Mean()\n",
    "local_discriminator_metric = tf.keras.metrics.Mean()\n",
    "global_discriminator_metric = tf.keras.metrics.Mean()\n",
    "similarity_metric = tf.keras.metrics.Mean()\n",
    "\n",
    "correlation_metric_style1 = tf.keras.metrics.Mean()\n",
    "correlation_metric_style2 = tf.keras.metrics.Mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_str = datetime.now().strftime('%Y-%m-%d_%H_%M_%S')\n",
    "\n",
    "BASE_DIR = f\"logs - conditional COSCI-GAN/{date_str} - Conditional COSCI-GAN\"\n",
    "TRAIN_LOGS_DIR_PATH = f\"{BASE_DIR}/fit\"\n",
    "GENERATION_LOG = f\"{BASE_DIR}/Generations\"\n",
    "\n",
    "TRAIN_SUMMARY_WRITER = tf.summary.create_file_writer(TRAIN_LOGS_DIR_PATH)\n",
    "\n",
    "\n",
    "def plot_to_buff(generation_s1:np.ndarray, generation_s2:np.ndarray, nvertical:int=3, nhoriz:int=3):\n",
    "    legend = [f\"feat {j}\" for j in range(generation_s1.shape[-1])]\n",
    "\n",
    "    fig = plt.figure(figsize=(18, 10))\n",
    "    plt.suptitle(\"Generations After GAN Training.\")\n",
    "\n",
    "    for i in range(nvertical* nhoriz):\n",
    "        current_style = i% 2\n",
    "        ax = plt.subplot(nvertical, nhoriz, i+ 1)\n",
    "        if current_style == 0:\n",
    "            ax.set_title(f\"sequence {i+1}. Style 1.\")\n",
    "            plt.plot(generation_s1[i])\n",
    "        else: \n",
    "            ax.set_title(f\"sequence {i+1}. Style 2.\")\n",
    "            plt.plot(generation_s2[i])\n",
    "        ax.grid(True)\n",
    "        plt.legend(legend)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png')\n",
    "    buf.seek(0)\n",
    "    plt.close(fig)\n",
    "    return buf\n",
    "\n",
    "\n",
    "def log_losses(epoch, plot_buf):\n",
    "    image = tf.image.decode_png(plot_buf.getvalue(), channels=4)\n",
    "    image = tf.expand_dims(image, 0)\n",
    "\n",
    "    with TRAIN_SUMMARY_WRITER.as_default():\n",
    "        tf.summary.scalar(\"Generator Loss\", generator_metric.result(), step=epoch)\n",
    "\n",
    "        tf.summary.scalar(\"Local D loss\", local_discriminator_metric.result(), step=epoch)\n",
    "        tf.summary.scalar(\"Gobal D loss\", global_discriminator_metric.result(), step=epoch)\n",
    "        \n",
    "        tf.summary.scalar(\"Mode Colapsing ?\", similarity_metric.result(), step=epoch)\n",
    "        tf.summary.scalar(\"Correlation Metric Style 1\", correlation_metric_style1.result(), step=epoch)\n",
    "        tf.summary.scalar(\"Correlation Metric Style 2\", correlation_metric_style2.result(), step=epoch)\n",
    "\n",
    "        tf.summary.image(\"Training Generations\", image, step=epoch)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_plots(noise, save_to):\n",
    "    generated = generate(seed)\n",
    "\n",
    "    fig =plt.figure(figsize=(18, 5))\n",
    "    plt.title(\"Generation of the GAN during Training.\")\n",
    "    for i in range(generated.shape[-1]):\n",
    "        plt.plot(generated[0, :, i], label=f'feat {i+1}')\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "\n",
    "    plt.savefig(save_to)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc = tf.keras.losses.BinaryCrossentropy()\n",
    "error_classif = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "def local_discriminator_loss(crits_on_real, crits_on_fake):\n",
    "    individual_losses = []\n",
    "\n",
    "    for i in range(crits_on_real.shape[0]):\n",
    "        l1 = bc(tf.zeros_like(crits_on_real), crits_on_fake[i])\n",
    "        l2 = bc(tf.ones_like(crits_on_real), crits_on_real[i])\n",
    "        loss = (l1+ l2)/2\n",
    "        individual_losses.append(loss)\n",
    "        \n",
    "    return individual_losses\n",
    "\n",
    "\n",
    "def local_generator_loss(crit_on_fake):\n",
    "    individual_losses = []\n",
    "    true_label = tf.zeros(crit_on_fake[0].shape)\n",
    "\n",
    "    for i in range(crit_on_fake.shape[0]):\n",
    "        individual_losses.append(bc(true_label, crit_on_fake[i]))\n",
    "        \n",
    "    return np.array(individual_losses)\n",
    "\n",
    "def style_classsification_loss(y_pred, y_true):\n",
    "    return error_classif(y_true, y_pred)\n",
    "\n",
    "def global_discriminator_loss(crit_on_real, crit_on_fake):\n",
    "    l1 = bc(tf.zeros_like(crit_on_fake), crit_on_fake)\n",
    "    l2 = bc(tf.ones_like(crit_on_real), crit_on_real)\n",
    "    loss = (l1+ l2)/2\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def global_generator_loss(crit_on_fake):\n",
    "    loss = bc(tf.ones_like(crit_on_fake), crit_on_fake)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_local_predictions(sequences, training=True):\n",
    "    _crit_classif = []\n",
    "    _crit_styles = []\n",
    "\n",
    "    for i in range(len(local_discriminators)):\n",
    "        crit, styles = local_discriminators[i](sequences[:,:, i], training=training)\n",
    "        _crit_classif.append(crit)\n",
    "        _crit_styles.append(styles)\n",
    "\n",
    "    return np.array(_crit_classif), np.array(_crit_styles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function\n",
    "def discriminators_step(style1_ts, style2_ts, generators_, local_discriminators_, style_class_, global_discriminator_):\n",
    "    noise= wiener_process(BS, N_SAMPLE_WIENER, FEAT_WIENER)\n",
    "    style_labels = tf.zeros((BS,))\n",
    "\n",
    "    with tf.GradientTape(persistent=True) as d_tape:\n",
    "        style1_generated = [g([noise, style_labels], training=False) for g in generators_]\n",
    "        style2_generated = [g([noise, style_labels+ 1.], training=False) for g in generators_]\n",
    "\n",
    "        local_d1_gen = tf.convert_to_tensor([local_discriminators_[i](style1_generated[i], training=True) for i in range(len(local_discriminators_))])\n",
    "        local_d2_gen = tf.convert_to_tensor([local_discriminators_[i](style2_generated[i], training=True) for i in range(len(local_discriminators_))])\n",
    "\n",
    "        local_d1_real= tf.convert_to_tensor([local_discriminators_[i](style1_ts[:, :, i], training=True) for i in range(len(local_discriminators_))])\n",
    "        local_d2_real= tf.convert_to_tensor([local_discriminators_[i](style2_ts[:, :, i], training=True) for i in range(len(local_discriminators_))])\n",
    "\n",
    "        local_s1_real= tf.convert_to_tensor([style_class_[i](style1_ts[:, :, i], training=True) for i in range(len(style_class_))])\n",
    "        local_s2_real= tf.convert_to_tensor([style_class_[i](style2_ts[:, :, i], training=True) for i in range(len(style_class_))])\n",
    "\n",
    "        # Reshape the generated sequences for the global dicriminator.\n",
    "        generated_reshaped1 = tf.stack(style1_generated, -1)\n",
    "        generated_reshaped1 = tf.reshape(generated_reshaped1, (BS, SEQUENCE_LENGTH, generated_reshaped1.shape[-1]))\n",
    "\n",
    "        generated_reshaped2 = tf.stack(style2_generated, -1)\n",
    "        generated_reshaped2 = tf.reshape(generated_reshaped2, (BS, SEQUENCE_LENGTH, generated_reshaped2.shape[-1]))\n",
    "\n",
    "        g_crit_gen1, _ = global_discriminator_(generated_reshaped1, training=True)\n",
    "        g_crit_gen2, _ = global_discriminator_(generated_reshaped2, training=True)\n",
    "\n",
    "        g_crit_real1, g_style_real1= global_discriminator_(style1_ts, training=True)\n",
    "        g_crit_real2, g_style_real2= global_discriminator_(style2_ts, training=True)\n",
    "\n",
    "        # (LOCAL DISCRIMINATOR) Real / Fake Loss \n",
    "        local_d_losses1 = [local_discriminator_loss(local_d1_gen[i], local_d1_real[i]) for i in range(len(local_discriminators_))]\n",
    "        local_d_losses2 = [local_discriminator_loss(local_d2_gen[i], local_d2_real[i]) for i in range(len(local_discriminators_))]\n",
    "        local_d_losses = local_d_losses1 + local_d_losses2\n",
    "\n",
    "        #(LOCAL DISCRIMINATOR) Style CLassification:\n",
    "        style_labels = tf.zeros((BS, 1))\n",
    "        local_s1_loss = [style_classsification_loss(local_s1_real[i], style_labels) for i in range(len(local_discriminators_))]\n",
    "        local_s2_loss = [style_classsification_loss(local_s2_real[i], style_labels+ 1.) for i in range(len(local_discriminators_))]\n",
    "        local_style_loss = local_s1_loss + local_s2_loss\n",
    "\n",
    "        # (GLOBAL DISCRIMINATOR) Real/ Fake Loss\n",
    "        global_d_loss1 = global_discriminator_loss(g_crit_real1, g_crit_gen1)\n",
    "        global_d_loss2 = global_discriminator_loss(g_crit_real2, g_crit_gen2)\n",
    "        global_discrimination_loss = global_d_loss1+ global_d_loss2\n",
    "\n",
    "        # (GLOBAL DISCRIMINATOR) Style Classification\n",
    "        global_classif1 = style_classsification_loss(g_style_real1, style_labels)\n",
    "        global_classif2 = style_classsification_loss(g_style_real2, style_labels+ 1.)\n",
    "        global_style_loss = global_classif1+ global_classif2\n",
    "\n",
    "    # (GOBAL DISCRIMINATOR): Real / Fake and style\n",
    "    global_discr_gradient = d_tape.gradient([global_discrimination_loss, global_style_loss], global_discriminator_.trainable_variables)\n",
    "    global_discriminator_opt.apply_gradients(zip(global_discr_gradient, global_discriminator_.trainable_variables)) \n",
    "\n",
    "    for i in range(len(local_discriminators_)): \n",
    "        grads = d_tape.gradient(local_d_losses[i], local_discriminators_[i].trainable_variables)\n",
    "        discriminator_opt[i].apply_gradients(zip(grads, local_discriminators_[i].trainable_variables))\n",
    "\n",
    "        grads = d_tape.gradient(local_style_loss[i], style_class_[i].trainable_variables)\n",
    "        discriminator_opt[i].apply_gradients(zip(grads, style_class_[i].trainable_variables))\n",
    "\n",
    "    local_discriminator_metric(tf.reduce_mean(local_d_losses))\n",
    "    global_discriminator_metric(global_discrimination_loss)\n",
    "        \n",
    "# @tf.function\n",
    "def generator_step(_generators, _local_discriminator, _style_discriminators, _global_discriminator):\n",
    "    noise= wiener_process(BS, N_SAMPLE_WIENER, FEAT_WIENER)\n",
    "    style_labels = tf.zeros((BS,))\n",
    "    \n",
    "    with tf.GradientTape(persistent=True) as g_tape:\n",
    "        style1_generated = [g([noise, style_labels], training=True) for g in _generators]\n",
    "        style2_generated = [g([noise, style_labels+ 1.], training=True) for g in _generators]\n",
    "\n",
    "        # (LOCAL DISCRIMINATOR): Fakeness Predictions\n",
    "        local_d1_gen = tf.convert_to_tensor([_local_discriminator[i](style1_generated[i], training=False) for i in range(len(_local_discriminator))])\n",
    "        local_d2_gen = tf.convert_to_tensor([_local_discriminator[i](style2_generated[i], training=False) for i in range(len(_local_discriminator))])\n",
    "\n",
    "        # (LOCAL DISCRIMINATOR): Style Predictions\n",
    "        local_s1_gen = tf.convert_to_tensor([_style_discriminators[i](style1_generated[i], training=False) for i in range(len(_style_discriminators))])\n",
    "        local_s2_gen = tf.convert_to_tensor([_style_discriminators[i](style2_generated[i], training=False) for i in range(len(_style_discriminators))])\n",
    "\n",
    "        # Reshape the generated sequences for the global dicriminator.\n",
    "        generated_reshaped1 = tf.stack(style1_generated, -1)\n",
    "        generated_reshaped1 = tf.reshape(generated_reshaped1, (BS, SEQUENCE_LENGTH, generated_reshaped1.shape[-1]))\n",
    "\n",
    "        generated_reshaped2 = tf.stack(style2_generated, -1)\n",
    "        generated_reshaped2 = tf.reshape(generated_reshaped2, (BS, SEQUENCE_LENGTH, generated_reshaped2.shape[-1]))\n",
    "\n",
    "        # (GLOBAL DISCRIMINATOR): Predictions.\n",
    "        g_crit_gen1, g_style1 = _global_discriminator(generated_reshaped1, training=False)\n",
    "        g_crit_gen2, g_style2 = _global_discriminator(generated_reshaped2, training=False)\n",
    "\n",
    "        # (LOCAL DISCRIMINATOR): Real/ Fake.\n",
    "        local_g_loss1 = local_generator_loss(local_d1_gen)\n",
    "        local_g_loss2 = local_generator_loss(local_d2_gen)\n",
    "        local_g_loss = local_g_loss1+ local_g_loss2\n",
    "\n",
    "        # (LOCAL DISCRIMINATOR): Style.\n",
    "        style_labels = tf.zeros((BS, 1))\n",
    "        local_s1_loss = tf.convert_to_tensor([style_classsification_loss(local_s1_gen[i], style_labels) for i in range(len(_local_discriminator))])\n",
    "        local_s2_loss = tf.convert_to_tensor([style_classsification_loss(local_s2_gen[i], style_labels+ 1.) for i in range(len(_local_discriminator))])\n",
    "        local_style_preservation = local_s1_loss + local_s2_loss\n",
    "\n",
    "        # (GLOBAL DISCRIMINATOR): Real / Fake.\n",
    "        global_g_loss1= global_generator_loss(g_crit_gen1)\n",
    "        global_g_loss2= global_generator_loss(g_crit_gen2)\n",
    "        global_g_loss = global_g_loss1+ global_g_loss2\n",
    "\n",
    "        # (GLOBAL DISCRIMINATOR): Style Classification.\n",
    "        global_style1_preservation = style_classsification_loss(g_style1, style_labels)\n",
    "        global_style2_preservation = style_classsification_loss(g_style2, style_labels+ 1.)\n",
    "        global_style_preservation = global_style1_preservation + global_style2_preservation\n",
    "\n",
    "        global_reduced = LAMBDA_GLOBAL* (global_g_loss+ global_style_preservation)\n",
    "        g_loss = [LAMBDA_LOCAL* (l_loss + loc_sty_pre) + global_reduced for (l_loss, loc_sty_pre) in zip(local_g_loss, local_style_preservation)]\n",
    "        \n",
    "    # And Local Generators !\n",
    "    for i in range(len(generators)):   \n",
    "        grads = g_tape.gradient(g_loss[i], generators[i].trainable_variables)\n",
    "        generator_opt[i].apply_gradients(zip(grads, generators[i].trainable_variables))\n",
    "\n",
    "    generator_metric(tf.reduce_mean(local_g_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset1, dataset2, epochs):\n",
    "  losses = []\n",
    "  total_steps = \"?\"\n",
    "  for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "\n",
    "    generator_metric.reset_states()\n",
    "    global_discriminator_metric.reset_states()\n",
    "    local_discriminator_metric.reset_states()\n",
    "\n",
    "    for s, (sequence1, sequence2) in enumerate(zip(dataset1, dataset2)):\n",
    "      if s % UPDATE_DISCRIMINATOR == 0:\n",
    "        discriminators_step(sequence1, sequence2, generators, local_discriminators, style_discrs, global_discriminator)\n",
    "        \n",
    "      generator_step(generators, local_discriminators, style_discrs, global_discriminator)\n",
    "\n",
    "      print(f\"\\r e {epoch}/{epochs}, s {s}/{total_steps}: Gen {generator_metric.result():0.4f}; Global discriminator: {global_discriminator_metric.result():0.4f}; Local discriminator: {local_discriminator_metric.result():0.4f}; Sim loss: {similarity_metric.result():0.4f}\", end=\"\")\n",
    "      break\n",
    "    if epoch == 0:\n",
    "      total_steps = s\n",
    "\n",
    "    stop = time.time()\n",
    "    print()\n",
    "    # print(f\"\\r[+] Epoch {epoch}/{epochs} in {(stop-start):0.4f} seconds. ({(stop-start)/total_steps:0.4f} s/step)\")\n",
    "\n",
    "    # generate_plots(seed, f\"imgs/GAN_generations/{epoch}.png\")\n",
    "    # Make generations on seed\n",
    "    seed_generation1 = generate(seed, 0., training=False)\n",
    "    seed_generation2 = generate(seed, 1., training=False)\n",
    "    buff = plot_to_buff(seed_generation1, seed_generation2)\n",
    "\n",
    "    batch_signature_style1 = signature_on_batch(seed_generation1, [0, 1], [2, 3, 4, 5], config.met_params.signature_length)\n",
    "    signature_difference1 = signature_metric(real_batch_signature_style1, batch_signature_style1)\n",
    "\n",
    "    batch_signature_style2 = signature_on_batch(seed_generation2, [0, 1], [2, 3, 4, 5], config.met_params.signature_length)\n",
    "    signature_difference2 = signature_metric(real_batch_signature_style2, batch_signature_style2)\n",
    "\n",
    "    correlation_metric_style1(signature_difference1)\n",
    "    correlation_metric_style2(signature_difference2)\n",
    "\n",
    "    l = [generator_metric.result(), global_discriminator_metric.result(), local_discriminator_metric.result()]\n",
    "    losses.append(l)\n",
    "    log_losses(epoch, buff)\n",
    "\n",
    "  return np.array(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_losses = train(dset_simulated, dset_simulated2, EPOCHS)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 5))\n",
    "plt.title(\"Training Losses.\")\n",
    "plt.plot(training_losses[:, 0], \".-\", label=\"Generator Loss\")\n",
    "plt.plot(training_losses[:, 1], \".-\", label=\"Discriminator Loss\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's Test it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "after_training_generations = generator(seed, training=False)\n",
    "\n",
    "plt.figure(figsize=(18, 5))\n",
    "plt.title(\"Generation of the GAN whitout Training.\")\n",
    "for i in range(after_training_generations.shape[-1]):\n",
    "    plt.plot(after_training_generations[0, :, i], label=f'feat {i+1}')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "after_training_decision = discriminator(after_training_generations)\n",
    "after_training_decision[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test if Mode Colapsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after_training_generations\n",
    "def plot_several_generations(generations:np.ndarray, nvertical:int=3, nhoriz:int=3):\n",
    "\n",
    "    legend = [f\"feat {j}\" for j in range(generations.shape[-1])]\n",
    "\n",
    "    plt.figure(figsize=(18, 10))\n",
    "    plt.suptitle(\"Generations After GAN Training.\")\n",
    "\n",
    "    for i in range(nvertical* nhoriz):\n",
    "        ax = plt.subplot(nvertical, nhoriz, i+ 1)\n",
    "        ax.set_title(f\"sequence {i+1}\")\n",
    "\n",
    "        plt.plot(generations[i])\n",
    "        ax.grid(True)\n",
    "        plt.legend(legend)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_several_generations(after_training_generations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
