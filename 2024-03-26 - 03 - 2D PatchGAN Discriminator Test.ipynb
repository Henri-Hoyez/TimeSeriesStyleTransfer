{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patch Gan Discriminator Test.\n",
    "\n",
    "In this notebook, we want to see if the patchGAN Discriminator brings something. \n",
    "To do that, we will train the wiener MTSGAN on the same dataset as before. \n",
    "we will evaluate it and see if is brings something."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from configs.Metric import Metric\n",
    "from configs.SimulatedData import Proposed\n",
    "from utils.metric import signature_on_batch, signature_metric\n",
    "from datetime import datetime\n",
    "import io\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs.\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized.\n",
    "    print(e)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config= Proposed()\n",
    "\n",
    "SEQUENCE_LENGTH = config.sequence_lenght_in_sample\n",
    "GRANUARITY = config.granularity\n",
    "OVERLAP = config.overlap\n",
    "BS = config.batch_size\n",
    "EPOCHS = 10 # config.epochs\n",
    "NUM_SEQUENCE_TO_GENERATE = config.met_params.sequence_to_generate\n",
    "\n",
    "TRAIN_DISCRIMINATOR_STEP = 10\n",
    "\n",
    "SIMULATED_DATA_PATH = \"data/simulated_dataset/01 - Source Domain.h5\"\n",
    "N_SAMPLE_WIENER = SEQUENCE_LENGTH//4\n",
    "FEAT_WIENER = 2\n",
    "NOISE_DIM= (N_SAMPLE_WIENER, FEAT_WIENER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dataframe_to_tensorflow_sequences(df:pd.DataFrame, sequence_lenght_in_sample, granularity, shift_between_sequences, batch_size, shuffle=True):\n",
    "    sequence_lenght = int(sequence_lenght_in_sample*granularity)\n",
    "\n",
    "    dset = tf.data.Dataset.from_tensor_slices(df.values)\n",
    "    dset = dset.window(sequence_lenght , shift=shift_between_sequences, stride=granularity).flat_map(lambda x: x.batch(sequence_lenght_in_sample, drop_remainder=True))\n",
    "\n",
    "    if shuffle:\n",
    "        dset= dset.shuffle(256)\n",
    "\n",
    "    dset = dset.batch(batch_size, drop_remainder=True)\n",
    "\n",
    "    dset = dset.cache().prefetch(10)\n",
    "\n",
    "    return dset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_simulated = pd.read_hdf(SIMULATED_DATA_PATH)\n",
    "df_simulated = df_simulated.drop(columns=\"labels\")\n",
    "\n",
    "dset_simulated = convert_dataframe_to_tensorflow_sequences(\n",
    "    df_simulated, \n",
    "    SEQUENCE_LENGTH, \n",
    "    GRANUARITY, \n",
    "    int(OVERLAP* SEQUENCE_LENGTH),\n",
    "    BS\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify if it is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = next(iter(dset_simulated))[0]\n",
    "print(sequence.shape)\n",
    "\n",
    "plt.figure(figsize=(18, 5))\n",
    "plt.title(\"Simulated Sequence.\")\n",
    "for i in range(sequence.shape[1]):\n",
    "    plt.plot(sequence[:, i], label=f'feat {i+1}')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make some Wiener Noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wiener_process(batch:int, n_sample_wiener:int, n_feat_wiener:int):\n",
    "    d_noise = tf.random.normal([batch, n_sample_wiener, n_feat_wiener])\n",
    "    wiener_noise = tf.math.cumsum(d_noise, axis=1)\n",
    "    return wiener_noise\n",
    "\n",
    "seed = wiener_process(NUM_SEQUENCE_TO_GENERATE, N_SAMPLE_WIENER, FEAT_WIENER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_arrow(A, B, color=\"b\"):\n",
    "    plt.arrow(A[0], A[1], B[0] - A[0], B[1] - A[1],\n",
    "              length_includes_head=True, color=color)\n",
    "    \n",
    "def draw_arrows(xs, ys, color=\"b\"):\n",
    "    for i in range(xs.shape[0]-1):\n",
    "        point0 = [xs[i], ys[i]]\n",
    "        point1 = [xs[i+1], ys[i+1]]\n",
    "        draw_arrow(point0, point1, color=color)\n",
    "\n",
    "plt.figure(figsize=(18, 5))\n",
    "plt.title(\"Example of the wiener process.\")\n",
    "\n",
    "draw_arrows(seed[0,:,0], seed[0,:,1], color=\"tab:blue\")\n",
    "plt.scatter(seed[0,:,0], seed[0,:,1], label='Wiener Process.', color='tab:blue')\n",
    "\n",
    "plt.grid()\n",
    "plt.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Model Architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "def make_generator(noise_shape:tuple, n_signals:int, seq_length:int):\n",
    "    \n",
    "    _input = tf.keras.Input(noise_shape)\n",
    "\n",
    "    # Make a simple Wiener process projector.\n",
    "    wiener_encoded= layers.Flatten()(_input)\n",
    "    wiener_encoded= tf.keras.layers.Dense(tf.math.reduce_prod(noise_shape))(wiener_encoded)\n",
    "    wiener_encoded= layers.BatchNormalization()(wiener_encoded)\n",
    "    wiener_encoded= layers.LeakyReLU()(wiener_encoded)\n",
    "\n",
    "    x= tf.keras.layers.Dense(n_signals*seq_length//8)(wiener_encoded)\n",
    "    x= layers.BatchNormalization()(x)\n",
    "    x= layers.LeakyReLU()(x)\n",
    "\n",
    "    x= tf.keras.layers.Reshape((seq_length//8, n_signals))(x)\n",
    "\n",
    "    x= tf.keras.layers.Conv1DTranspose(256, 5, 1, padding='same')(x)\n",
    "    x= layers.BatchNormalization()(x)\n",
    "    x= layers.LeakyReLU()(x)\n",
    "\n",
    "    x= tf.keras.layers.Conv1DTranspose(128, 5, 2, padding='same')(x)\n",
    "    x= layers.BatchNormalization()(x)\n",
    "    x= layers.LeakyReLU()(x)\n",
    "    \n",
    "    x= tf.keras.layers.Conv1DTranspose(128, 5, 2, padding='same')(x)\n",
    "    x= layers.BatchNormalization()(x)\n",
    "    x= layers.LeakyReLU()(x)\n",
    "\n",
    "    x= tf.keras.layers.Conv1DTranspose(128, 5, 2, padding='same')(x)\n",
    "    x= layers.BatchNormalization()(x)\n",
    "    x= layers.LeakyReLU()(x)\n",
    "\n",
    "    x= tf.keras.layers.Conv1DTranspose(n_signals, 3, 1, padding='same')(x)\n",
    "    x= layers.BatchNormalization()(x)\n",
    "    x= layers.LeakyReLU()(x)\n",
    "\n",
    "    _model = tf.keras.Model(_input, x)\n",
    "    wiener_encoder = tf.keras.Model(_input, wiener_encoded)\n",
    "\n",
    "    return _model, wiener_encoder\n",
    "\n",
    "\n",
    "generator, wiener_encoder = make_generator(NOISE_DIM, sequence.shape[1], SEQUENCE_LENGTH)\n",
    "generator.summary()\n",
    "NOISE_DIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_discriminator(seq_length:int, n_feat:int):\n",
    "    _input = tf.keras.Input((seq_length, n_feat))\n",
    "    x = tf.keras.layers.Conv1D(128, 5, 2, padding='same')(_input)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    # x = layers.Dropout(0.3)(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv1D(256, 5, 2, padding='same')(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    # x = layers.Dropout(0.3)(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv1D(256, 5, 2, padding='same')(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "\n",
    "    _output = tf.keras.layers.Conv1D(n_feat, 5, 2, padding='same', activation='sigmoid')(x)\n",
    "\n",
    "    model = tf.keras.Model(_input, _output)\n",
    "    early_predictor = tf.keras.Model(_input, x)\n",
    "\n",
    "    return model, early_predictor\n",
    "\n",
    "discriminator, early_predictor = make_discriminator(SEQUENCE_LENGTH, sequence.shape[1])\n",
    "\n",
    "discriminator.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot a Sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated = generator(seed)\n",
    "\n",
    "# after_training_generations\n",
    "def plot_several_generations(generations:np.ndarray, nvertical:int=3, nhoriz:int=3):\n",
    "\n",
    "    legend = [f\"feat {j}\" for j in range(generations.shape[-1])]\n",
    "\n",
    "    plt.figure(figsize=(18, 10))\n",
    "    plt.suptitle(\"Several Generations\")\n",
    "\n",
    "    for i in range(nvertical* nhoriz):\n",
    "        ax = plt.subplot(nvertical, nhoriz, i+ 1)\n",
    "        ax.set_title(f\"sequence {i+1}\")\n",
    "\n",
    "        plt.plot(generations[i])\n",
    "        ax.grid(True)\n",
    "        plt.legend(df_simulated)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_several_generations(generated) \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "\n",
    "def similarity_loss(extracted_features:np.ndarray):\n",
    "    anchor = extracted_features[0]\n",
    "    return tf.exp(-(tf.norm(extracted_features[1:]- anchor)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate the reference signature.\n",
    "\n",
    "real_sequence_batch = next(iter(dset_simulated))\n",
    "\n",
    "real_batch_signature= signature_on_batch(real_sequence_batch, config.met_params.ins, config.met_params.outs, config.met_params.signature_length)\n",
    "generated_batch_signature= signature_on_batch(generated, config.met_params.ins, config.met_params.outs, config.met_params.signature_length)\n",
    "\n",
    "signature_metric(real_batch_signature, generated_batch_signature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.RMSprop(1e-2)\n",
    "discriminator_optimizer = tf.keras.optimizers.RMSprop(2e-3)    \n",
    "\n",
    "# generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "# discriminator_optimizer = tf.keras.optimizers.Adam(1e-6)    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_metric = tf.keras.metrics.Mean()\n",
    "discriminator_metric = tf.keras.metrics.Mean()\n",
    "similarity_metric = tf.keras.metrics.Mean()\n",
    "correlation_metric = tf.keras.metrics.Mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_str = datetime.now().strftime('%Y-%m-%d_%H_%M_%S')\n",
    "\n",
    "BASE_DIR = f\"log - patchGAN Test/{date_str} - wiener 2D PatchGAN Discr\"\n",
    "TRAIN_LOGS_DIR_PATH = f\"{BASE_DIR}/fit\"\n",
    "GENERATION_LOG = f\"{BASE_DIR}/Generations\"\n",
    "\n",
    "TRAIN_SUMMARY_WRITER = tf.summary.create_file_writer(TRAIN_LOGS_DIR_PATH)\n",
    "\n",
    "\n",
    "def plot_to_buff(generations:np.ndarray, nvertical:int=3, nhoriz:int=3):\n",
    "    legend = [f\"feat {j}\" for j in range(generations.shape[-1])]\n",
    "\n",
    "    fig = plt.figure(figsize=(18, 10))\n",
    "    plt.suptitle(\"Generations After GAN Training.\")\n",
    "\n",
    "    for i in range(nvertical* nhoriz):\n",
    "        ax = plt.subplot(nvertical, nhoriz, i+ 1)\n",
    "        ax.set_title(f\"sequence {i+1}\")\n",
    "\n",
    "        plt.plot(generations[i])\n",
    "        ax.grid(True)\n",
    "        plt.legend(legend)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png')\n",
    "    buf.seek(0)\n",
    "    plt.close(fig)\n",
    "    return buf\n",
    "\n",
    "\n",
    "def log_losses(epoch, plot_buf):\n",
    "    image = tf.image.decode_png(plot_buf.getvalue(), channels=4)\n",
    "    image = tf.expand_dims(image, 0)\n",
    "\n",
    "    with TRAIN_SUMMARY_WRITER.as_default():\n",
    "        tf.summary.scalar(\"Generator Loss\", generator_metric.result(), step=epoch)\n",
    "        tf.summary.scalar(\"Discriminator Loss\", discriminator_metric.result(), step=epoch)\n",
    "        tf.summary.scalar(\"Mode Colapsing ?\", similarity_metric.result(), step=epoch)\n",
    "        tf.summary.scalar(\"Correlation Metric\", correlation_metric.result(), step=epoch)\n",
    "\n",
    "        tf.summary.image(\"Training Generations\", image, step=epoch)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_plots(noise, save_to):\n",
    "    generated = generator(noise)\n",
    "\n",
    "    fig =plt.figure(figsize=(18, 5))\n",
    "    plt.title(\"Generation of the GAN during Training.\")\n",
    "    for i in range(generated.shape[-1]):\n",
    "        plt.plot(generated[0, :, i], label=f'feat {i+1}')\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "\n",
    "    plt.savefig(save_to)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function\n",
    "def train_step(images, update_discriminator:True):\n",
    "    alpha= 1  \n",
    "    # noise = tf.random.normal([BS, NOISE_DIM])\n",
    "    noise= wiener_process(BS, N_SAMPLE_WIENER, FEAT_WIENER)\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "      generated_images = generator(noise, training=True)\n",
    "\n",
    "      real_output = discriminator(images, training=update_discriminator)\n",
    "      fake_output = discriminator(generated_images, training=update_discriminator)\n",
    "      extracted_feat = early_predictor(generated_images, training=False)\n",
    "\n",
    "      sim_loss = similarity_loss(extracted_feat)\n",
    "      gen_loss = generator_loss(fake_output)+ alpha* sim_loss\n",
    "      disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    if update_discriminator == True:\n",
    "      gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "      discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "\n",
    "\n",
    "    # Save metric for display\n",
    "    generator_metric(gen_loss)\n",
    "    discriminator_metric(disc_loss)\n",
    "    similarity_metric(sim_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, epochs):\n",
    "  losses = []\n",
    "  total_steps = \"?\"\n",
    "  for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "\n",
    "    generator_metric.reset_states()\n",
    "    discriminator_metric.reset_states()\n",
    "\n",
    "    for s, image_batch in enumerate(dataset):\n",
    "      update_discriminator = s % TRAIN_DISCRIMINATOR_STEP == 0\n",
    "\n",
    "\n",
    "\n",
    "      train_step(image_batch, update_discriminator)\n",
    "      print(f\"\\r e {epoch}/{epochs}, s {s}/{total_steps}: Gen {generator_metric.result():0.4f}; Disc {discriminator_metric.result():0.4f} sim loss: {similarity_metric.result():0.4f}\", end=\"\")\n",
    "\n",
    "    if epoch == 0:\n",
    "      total_steps = s\n",
    "\n",
    "    stop = time.time()\n",
    "    print()\n",
    "    print(f\"\\r[+] Epoch {epoch}/{epochs} in {(stop-start):0.4f} seconds. ({(stop-start)/total_steps:0.4f} s/step)\")\n",
    "\n",
    "    generate_plots(seed, f\"imgs/GAN_generations/{epoch}.png\")\n",
    "    # Make generations on seed\n",
    "    seed_generation = generator(seed, training=False)\n",
    "    buff = plot_to_buff(seed_generation)\n",
    "\n",
    "    batch_signature = signature_on_batch(seed_generation, [0, 1], [2, 3, 4, 5], config.met_params.signature_length)\n",
    "    signature_difference = signature_metric(real_batch_signature, batch_signature)\n",
    "    correlation_metric(signature_difference)\n",
    "    \n",
    "    l = [generator_metric.result(), discriminator_metric.result()]\n",
    "    losses.append(l)\n",
    "    log_losses(epoch, buff)\n",
    "\n",
    "  return np.array(losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m training_losses \u001b[39m=\u001b[39m train(dset_simulated, EPOCHS)\n",
      "Cell \u001b[0;32mIn[18], line 15\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(dataset, epochs)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mfor\u001b[39;00m s, image_batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataset):\n\u001b[1;32m     11\u001b[0m   update_discriminator \u001b[39m=\u001b[39m s \u001b[39m%\u001b[39m TRAIN_DISCRIMINATOR_STEP \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m---> 15\u001b[0m   train_step(image_batch, update_discriminator)\n\u001b[1;32m     16\u001b[0m   \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\r\u001b[39;00m\u001b[39m e \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mepochs\u001b[39m}\u001b[39;00m\u001b[39m, s \u001b[39m\u001b[39m{\u001b[39;00ms\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mtotal_steps\u001b[39m}\u001b[39;00m\u001b[39m: Gen \u001b[39m\u001b[39m{\u001b[39;00mgenerator_metric\u001b[39m.\u001b[39mresult()\u001b[39m:\u001b[39;00m\u001b[39m0.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m; Disc \u001b[39m\u001b[39m{\u001b[39;00mdiscriminator_metric\u001b[39m.\u001b[39mresult()\u001b[39m:\u001b[39;00m\u001b[39m0.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m sim loss: \u001b[39m\u001b[39m{\u001b[39;00msimilarity_metric\u001b[39m.\u001b[39mresult()\u001b[39m:\u001b[39;00m\u001b[39m0.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m, end\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[39mif\u001b[39;00m epoch \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[17], line 22\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(images, update_discriminator)\u001b[0m\n\u001b[1;32m     19\u001b[0m   gradients_of_discriminator \u001b[39m=\u001b[39m disc_tape\u001b[39m.\u001b[39mgradient(disc_loss, discriminator\u001b[39m.\u001b[39mtrainable_variables)\n\u001b[1;32m     20\u001b[0m   discriminator_optimizer\u001b[39m.\u001b[39mapply_gradients(\u001b[39mzip\u001b[39m(gradients_of_discriminator, discriminator\u001b[39m.\u001b[39mtrainable_variables))\n\u001b[0;32m---> 22\u001b[0m gradients_of_generator \u001b[39m=\u001b[39m gen_tape\u001b[39m.\u001b[39;49mgradient(gen_loss, generator\u001b[39m.\u001b[39;49mtrainable_variables)\n\u001b[1;32m     23\u001b[0m generator_optimizer\u001b[39m.\u001b[39mapply_gradients(\u001b[39mzip\u001b[39m(gradients_of_generator, generator\u001b[39m.\u001b[39mtrainable_variables))\n\u001b[1;32m     26\u001b[0m \u001b[39m# Save metric for display\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tf2/lib/python3.10/site-packages/tensorflow/python/eager/backprop.py:1112\u001b[0m, in \u001b[0;36mGradientTape.gradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1106\u001b[0m   output_gradients \u001b[39m=\u001b[39m (\n\u001b[1;32m   1107\u001b[0m       composite_tensor_gradient\u001b[39m.\u001b[39mget_flat_tensors_for_gradients(\n\u001b[1;32m   1108\u001b[0m           output_gradients))\n\u001b[1;32m   1109\u001b[0m   output_gradients \u001b[39m=\u001b[39m [\u001b[39mNone\u001b[39;00m \u001b[39mif\u001b[39;00m x \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m ops\u001b[39m.\u001b[39mconvert_to_tensor(x)\n\u001b[1;32m   1110\u001b[0m                       \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m output_gradients]\n\u001b[0;32m-> 1112\u001b[0m flat_grad \u001b[39m=\u001b[39m imperative_grad\u001b[39m.\u001b[39;49mimperative_grad(\n\u001b[1;32m   1113\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tape,\n\u001b[1;32m   1114\u001b[0m     flat_targets,\n\u001b[1;32m   1115\u001b[0m     flat_sources,\n\u001b[1;32m   1116\u001b[0m     output_gradients\u001b[39m=\u001b[39;49moutput_gradients,\n\u001b[1;32m   1117\u001b[0m     sources_raw\u001b[39m=\u001b[39;49mflat_sources_raw,\n\u001b[1;32m   1118\u001b[0m     unconnected_gradients\u001b[39m=\u001b[39;49munconnected_gradients)\n\u001b[1;32m   1120\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_persistent:\n\u001b[1;32m   1121\u001b[0m   \u001b[39m# Keep track of watched variables before setting tape to None\u001b[39;00m\n\u001b[1;32m   1122\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_watched_variables \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tape\u001b[39m.\u001b[39mwatched_variables()\n",
      "File \u001b[0;32m~/anaconda3/envs/tf2/lib/python3.10/site-packages/tensorflow/python/eager/imperative_grad.py:67\u001b[0m, in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     65\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mUnknown value for unconnected_gradients: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m unconnected_gradients)\n\u001b[0;32m---> 67\u001b[0m \u001b[39mreturn\u001b[39;00m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_TapeGradient(\n\u001b[1;32m     68\u001b[0m     tape\u001b[39m.\u001b[39;49m_tape,  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m     69\u001b[0m     target,\n\u001b[1;32m     70\u001b[0m     sources,\n\u001b[1;32m     71\u001b[0m     output_gradients,\n\u001b[1;32m     72\u001b[0m     sources_raw,\n\u001b[1;32m     73\u001b[0m     compat\u001b[39m.\u001b[39;49mas_str(unconnected_gradients\u001b[39m.\u001b[39;49mvalue))\n",
      "File \u001b[0;32m~/anaconda3/envs/tf2/lib/python3.10/site-packages/tensorflow/python/eager/backprop.py:157\u001b[0m, in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[1;32m    155\u001b[0m     gradient_name_scope \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m forward_pass_name_scope \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    156\u001b[0m   \u001b[39mwith\u001b[39;00m ops\u001b[39m.\u001b[39mname_scope(gradient_name_scope):\n\u001b[0;32m--> 157\u001b[0m     \u001b[39mreturn\u001b[39;00m grad_fn(mock_op, \u001b[39m*\u001b[39;49mout_grads)\n\u001b[1;32m    158\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    159\u001b[0m   \u001b[39mreturn\u001b[39;00m grad_fn(mock_op, \u001b[39m*\u001b[39mout_grads)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf2/lib/python3.10/site-packages/tensorflow/python/ops/math_grad.py:1736\u001b[0m, in \u001b[0;36m_MatMulGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m   1734\u001b[0m b \u001b[39m=\u001b[39m math_ops\u001b[39m.\u001b[39mconj(op\u001b[39m.\u001b[39minputs[\u001b[39m1\u001b[39m])\n\u001b[1;32m   1735\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m t_a \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m t_b:\n\u001b[0;32m-> 1736\u001b[0m   grad_a \u001b[39m=\u001b[39m gen_math_ops\u001b[39m.\u001b[39;49mmat_mul(grad, b, transpose_b\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m   1737\u001b[0m   grad_b \u001b[39m=\u001b[39m gen_math_ops\u001b[39m.\u001b[39mmat_mul(a, grad, transpose_a\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m   1738\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m t_a \u001b[39mand\u001b[39;00m t_b:\n",
      "File \u001b[0;32m~/anaconda3/envs/tf2/lib/python3.10/site-packages/tensorflow/python/ops/gen_math_ops.py:6014\u001b[0m, in \u001b[0;36mmat_mul\u001b[0;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[1;32m   6012\u001b[0m \u001b[39mif\u001b[39;00m tld\u001b[39m.\u001b[39mis_eager:\n\u001b[1;32m   6013\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 6014\u001b[0m     _result \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_FastPathExecute(\n\u001b[1;32m   6015\u001b[0m       _ctx, \u001b[39m\"\u001b[39;49m\u001b[39mMatMul\u001b[39;49m\u001b[39m\"\u001b[39;49m, name, a, b, \u001b[39m\"\u001b[39;49m\u001b[39mtranspose_a\u001b[39;49m\u001b[39m\"\u001b[39;49m, transpose_a, \u001b[39m\"\u001b[39;49m\u001b[39mtranspose_b\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   6016\u001b[0m       transpose_b)\n\u001b[1;32m   6017\u001b[0m     \u001b[39mreturn\u001b[39;00m _result\n\u001b[1;32m   6018\u001b[0m   \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training_losses = train(dset_simulated, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.save(f\"{BASE_DIR}/generator.h5\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 5))\n",
    "plt.title(\"Training Losses.\")\n",
    "plt.plot(training_losses[:, 0], \".-\", label=\"Generator Loss\")\n",
    "plt.plot(training_losses[:, 1], \".-\", label=\"Discriminator Loss\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's Test it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "after_training_generations = generator(seed, training=False)\n",
    "\n",
    "plt.figure(figsize=(18, 5))\n",
    "plt.title(\"Generation of the GAN whitout Training.\")\n",
    "for i in range(after_training_generations.shape[-1]):\n",
    "    plt.plot(after_training_generations[0, :, i], label=f'feat {i+1}')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "after_training_decision = discriminator(after_training_generations)\n",
    "after_training_decision[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test if Mode Colapsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after_training_generations\n",
    "def plot_several_generations(generations:np.ndarray, nvertical:int=3, nhoriz:int=3):\n",
    "\n",
    "    legend = [f\"feat {j}\" for j in range(generations.shape[-1])]\n",
    "\n",
    "    plt.figure(figsize=(18, 10))\n",
    "    plt.suptitle(\"Generations After GAN Training.\")\n",
    "\n",
    "    for i in range(nvertical* nhoriz):\n",
    "        ax = plt.subplot(nvertical, nhoriz, i+ 1)\n",
    "        ax.set_title(f\"sequence {i+1}\")\n",
    "\n",
    "        plt.plot(generations[i])\n",
    "        ax.grid(True)\n",
    "        plt.legend(legend)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_several_generations(after_training_generations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
