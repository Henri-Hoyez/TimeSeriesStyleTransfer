{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MTS Generation with GAN.\n",
    "In this Notebook, we will try to generate Multivariate Time Series (MTS) with a simple GAN. This GAN will be only composed by:\n",
    "- A Generator which will ONLY take a vector as input and try to generate a MTS.\n",
    "- A Discriminator which will try to discriminate between the Fake and the Real Sample.\n",
    "\n",
    "This Notebook will train this Simple GAN on Simulated Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from configs.Metric import Metric\n",
    "from configs.SimulatedData import Proposed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Proposed()\n",
    "\n",
    "SIMULATED_DATA_PATH = \"data/simulated_dataset/01 - Source Domain.h5\"\n",
    "NOISE_DIM= 64\n",
    "\n",
    "SEQUENCE_LENGTH = config.sequence_lenght_in_sample\n",
    "GRANUARITY = config.granularity\n",
    "OVERLAP = config.overlap\n",
    "BS = config.batch_size\n",
    "EPOCHS = config.epochs\n",
    "NUM_SEQUENCE_TO_GENERATE = config.met_params.sequence_to_generate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "\n",
    "gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dataframe_to_tensorflow_sequences(df:pd.DataFrame, sequence_lenght_in_sample, granularity, shift_between_sequences, batch_size, shuffle=True):\n",
    "    sequence_lenght = int(sequence_lenght_in_sample*granularity)\n",
    "\n",
    "    dset = tf.data.Dataset.from_tensor_slices(df.values)\n",
    "    dset = dset.window(sequence_lenght , shift=shift_between_sequences, stride=granularity).flat_map(lambda x: x.batch(sequence_lenght_in_sample, drop_remainder=True))\n",
    "\n",
    "    if shuffle:\n",
    "        dset= dset.shuffle(256)\n",
    "\n",
    "    dset = dset.batch(batch_size, drop_remainder=True)\n",
    "\n",
    "    dset = dset.cache().prefetch(10)\n",
    "\n",
    "    return dset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_simulated = pd.read_hdf(SIMULATED_DATA_PATH)\n",
    "dset_simulated = convert_dataframe_to_tensorflow_sequences(\n",
    "    df_simulated, \n",
    "    SEQUENCE_LENGTH, \n",
    "    GRANUARITY, \n",
    "    int(OVERLAP* SEQUENCE_LENGTH),\n",
    "    BS\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify if it is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = next(iter(dset_simulated))[0]\n",
    "print(sequence.shape)\n",
    "\n",
    "plt.figure(figsize=(18, 5))\n",
    "plt.title(\"Simulated Sequence.\")\n",
    "for i in range(sequence.shape[1]):\n",
    "    plt.plot(sequence[:, i], label=f'feat {i+1}')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Model Architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "def make_generator(noise_shape:tuple, n_signals:int, seq_length:int):\n",
    "    _model = tf.keras.Sequential()\n",
    "\n",
    "    _model.add(tf.keras.Input(noise_shape))\n",
    "\n",
    "    _model.add(tf.keras.layers.Dense(n_signals*seq_length//8))\n",
    "    _model.add(layers.BatchNormalization())\n",
    "    _model.add(layers.LeakyReLU())\n",
    "\n",
    "    _model.add(tf.keras.layers.Reshape((seq_length//8, n_signals)))\n",
    "\n",
    "    _model.add(tf.keras.layers.Conv1DTranspose(256, 5, 1, padding='same'))\n",
    "    _model.add(layers.BatchNormalization())\n",
    "    _model.add(layers.LeakyReLU())\n",
    "\n",
    "    _model.add(tf.keras.layers.Conv1DTranspose(128, 5, 2, padding='same'))\n",
    "    _model.add(layers.BatchNormalization())\n",
    "    _model.add(layers.LeakyReLU())\n",
    "\n",
    "    _model.add(tf.keras.layers.Conv1DTranspose(128, 5, 2, padding='same'))\n",
    "    _model.add(layers.BatchNormalization())\n",
    "    _model.add(layers.LeakyReLU())\n",
    "\n",
    "    _model.add(tf.keras.layers.Conv1DTranspose(128, 5, 2, padding='same'))\n",
    "    _model.add(layers.BatchNormalization())\n",
    "    _model.add(layers.LeakyReLU())\n",
    "\n",
    "    _model.add(tf.keras.layers.Conv1DTranspose(n_signals, 3, 1, padding='same'))\n",
    "    _model.add(layers.BatchNormalization())\n",
    "    _model.add(layers.LeakyReLU())\n",
    "\n",
    "    return _model\n",
    "\n",
    "generator = make_generator(NOISE_DIM, sequence.shape[1], SEQUENCE_LENGTH)\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_discriminator(seq_length:int, n_feat:int):\n",
    "    _input = tf.keras.Input((seq_length, n_feat))\n",
    "    x = tf.keras.layers.Conv1D(128, 5, 2, padding='same')(_input)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv1D(128, 5, 2, padding='same')(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "\n",
    "    x = layers.Flatten()(x)\n",
    "    _output = layers.Dense(1)(x)\n",
    "\n",
    "    model = tf.keras.Model(_input, _output)\n",
    "    early_predictor = tf.keras.Model(_input, x)\n",
    "\n",
    "    return model, early_predictor\n",
    "\n",
    "discriminator, early_predictor = make_discriminator(SEQUENCE_LENGTH, sequence.shape[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot a Sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = tf.random.normal([NUM_SEQUENCE_TO_GENERATE, NOISE_DIM])\n",
    "generated = generator(seed)\n",
    "\n",
    "# after_training_generations\n",
    "def plot_several_generations(generations:np.ndarray, nvertical:int=3, nhoriz:int=3):\n",
    "\n",
    "    legend = [f\"feat {j}\" for j in range(generations.shape[-1])]\n",
    "\n",
    "    plt.figure(figsize=(18, 10))\n",
    "    plt.suptitle(\"Several Generations\")\n",
    "\n",
    "    for i in range(nvertical* nhoriz):\n",
    "        ax = plt.subplot(nvertical, nhoriz, i+ 1)\n",
    "        ax.set_title(f\"sequence {i+1}\")\n",
    "\n",
    "        plt.plot(generations[i])\n",
    "        ax.grid(True)\n",
    "        plt.legend(legend)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_several_generations(generated) \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision = discriminator(generated)\n",
    "decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_loss(extracted_features:np.ndarray):\n",
    "    anchor = extracted_features[0]\n",
    "    return tf.exp(-(tf.norm(extracted_features[1:]- anchor)))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corelation metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimized_cov(a,v):\n",
    "    nan_mean_a = np.nanmean(a, axis=1).reshape((-1,1))\n",
    "    nan_mean_b = np.nanmean(v, axis=1).reshape((-1,1))\n",
    "    return np.nansum((a- nan_mean_a)* (v- nan_mean_b), axis=1)\n",
    "\n",
    "def mean_difference(a,v):\n",
    "    return np.nanmean(a) - np.nanmean(v)\n",
    "\n",
    "def optimized_windowed_cov(a, v, beta=Metric.mean_senssibility_factor):\n",
    "    if a.shape[1] > v.shape[1]:\n",
    "        _a, _v = v, a \n",
    "    else: \n",
    "        _a, _v = a, v\n",
    "\n",
    "    n = _a.shape[1]\n",
    "    corrs = []\n",
    "\n",
    "    for k in range(_v.shape[1] - _a.shape[1]):\n",
    "        __v = _v[:, k: n+k]\n",
    "        # Compute the covariance \n",
    "        augmented_cov = optimized_cov(_a,__v)+ beta* mean_difference(_a,__v)\n",
    "\n",
    "        corrs.append(augmented_cov)\n",
    "        \n",
    "    return np.array(corrs)\n",
    "\n",
    "def signature_on_batch(x:np.ndarray, ins:list, outs:list, sig_seq_len:int):\n",
    "    \"\"\"Compute the signature from a given batch of MTS sequences `x`\n",
    "\n",
    "    Args:\n",
    "        x (np.ndarray): the batch\n",
    "        ins (list): input columns\n",
    "        outs (list): output label solumns\n",
    "        sig_seq_len (int): the desired signature length\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: the min, max, mean signature.\n",
    "    \"\"\"\n",
    "    sigs = []\n",
    "    shift = sig_seq_len//2\n",
    "    childrens = x[:, shift:-shift]\n",
    "\n",
    "    for _in in ins:\n",
    "        for _out in outs:\n",
    "            c1 = x[:, :, _in]\n",
    "            c2 = childrens[:, :, _out]\n",
    "            \n",
    "            sig = optimized_windowed_cov(c1, c2)\n",
    "\n",
    "            sigs.append(sig)\n",
    "\n",
    "    mins = np.min(sigs, axis=-1)\n",
    "    maxs = np.max(sigs, axis=-1)\n",
    "    means= np.mean(sigs, axis=-1)\n",
    "\n",
    "    signatures = np.stack([mins, maxs, means], axis=-1)\n",
    "\n",
    "    return signatures\n",
    "\n",
    "def signature_metric(source_sig:np.ndarray, target_sig:np.ndarray):\n",
    "    # Shape: (n_features, sign_seq_lenght, 3)\n",
    "    min_source = source_sig[0]\n",
    "    max_source = source_sig[1]\n",
    "    mean_source = source_sig[2]\n",
    "\n",
    "    min_target = target_sig[0]\n",
    "    max_target = target_sig[1]\n",
    "    mean_target = target_sig[2]\n",
    "\n",
    "    mean_differences = np.mean(mean_target- mean_source)\n",
    "    area_source = np.mean(max_source- min_source)\n",
    "    area_target = np.mean(max_target- min_target)\n",
    "\n",
    "    met = np.power(mean_differences, 2) + Metric.noise_senssitivity*np.power(area_target- area_source, 2)\n",
    "\n",
    "    return met"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate the reference signature.\n",
    "\n",
    "real_sequence_batch = next(iter(dset_simulated))\n",
    "\n",
    "real_batch_signature= signature_on_batch(real_sequence_batch, config.met_params.ins, config.met_params.outs, config.met_params.signature_length)\n",
    "generated_batch_signature= signature_on_batch(generated, config.met_params.ins, config.met_params.outs, config.met_params.signature_length)\n",
    "\n",
    "signature_metric(real_batch_signature, generated_batch_signature)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.RMSprop(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.RMSprop(2e-6)    \n",
    "\n",
    "# generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "# discriminator_optimizer = tf.keras.optimizers.Adam(1e-6)    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_metric = tf.keras.metrics.Mean()\n",
    "discriminator_metric = tf.keras.metrics.Mean()\n",
    "similarity_metric = tf.keras.metrics.Mean()\n",
    "correlation_metric = tf.keras.metrics.Mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import io\n",
    "\n",
    "date_str = datetime.now().strftime('%Y-%m-%d_%H_%M_%S')\n",
    "\n",
    "BASE_DIR = f\"logs/{date_str}\"\n",
    "TRAIN_LOGS_DIR_PATH = f\"{BASE_DIR}/fit\"\n",
    "GENERATION_LOG = f\"{BASE_DIR}/Generations\"\n",
    "TRAIN_SUMMARY_WRITER = tf.summary.create_file_writer(TRAIN_LOGS_DIR_PATH)\n",
    "\n",
    "\n",
    "def plot_to_buff(generations:np.ndarray, nvertical:int=3, nhoriz:int=3):\n",
    "    legend = [f\"feat {j}\" for j in range(generations.shape[-1])]\n",
    "\n",
    "    fig = plt.figure(figsize=(18, 10))\n",
    "    plt.suptitle(\"Generations After GAN Training.\")\n",
    "\n",
    "    for i in range(nvertical* nhoriz):\n",
    "        ax = plt.subplot(nvertical, nhoriz, i+ 1)\n",
    "        ax.set_title(f\"sequence {i+1}\")\n",
    "\n",
    "        plt.plot(generations[i])\n",
    "        ax.grid(True)\n",
    "        plt.legend(legend)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png')\n",
    "    buf.seek(0)\n",
    "    plt.close(fig)\n",
    "    return buf\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def log_losses(epoch, plot_buf):\n",
    "    image = tf.image.decode_png(plot_buf.getvalue(), channels=4)\n",
    "    image = tf.expand_dims(image, 0)\n",
    "\n",
    "\n",
    "    with TRAIN_SUMMARY_WRITER.as_default():\n",
    "        tf.summary.scalar(\"Generator Loss\", generator_metric.result(), step=epoch)\n",
    "        tf.summary.scalar(\"Discriminator Loss\", discriminator_metric.result(), step=epoch)\n",
    "        tf.summary.scalar(\"Mode Colapsing ?\", similarity_metric.result(), step=epoch)\n",
    "        tf.summary.scalar(\"Correlation Metric\", correlation_metric.result(), step=epoch)\n",
    "\n",
    "        tf.summary.image(\"Training Generations\", image, step=epoch)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_plots(noise, save_to):\n",
    "    generated = generator(noise)\n",
    "\n",
    "    fig =plt.figure(figsize=(18, 5))\n",
    "    plt.title(\"Generation of the GAN during Training.\")\n",
    "    for i in range(generated.shape[-1]):\n",
    "        plt.plot(generated[0, :, i], label=f'feat {i+1}')\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "\n",
    "    plt.savefig(save_to)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(images):\n",
    "    alpha= 1  \n",
    "    noise = tf.random.normal([BS, NOISE_DIM])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "      generated_images = generator(noise, training=True)\n",
    "\n",
    "      real_output = discriminator(images, training=True)\n",
    "      fake_output = discriminator(generated_images, training=True)\n",
    "      extracted_feat = early_predictor(generated_images, training=False)\n",
    "\n",
    "      sim_loss = similarity_loss(extracted_feat)\n",
    "      gen_loss = generator_loss(fake_output)+ alpha* sim_loss\n",
    "      disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "\n",
    "\n",
    "    # Save metric for display\n",
    "    generator_metric(gen_loss)\n",
    "    discriminator_metric(disc_loss)\n",
    "    similarity_metric(sim_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, epochs):\n",
    "  losses = []\n",
    "  for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "\n",
    "    generator_metric.reset_states()\n",
    "    discriminator_metric.reset_states()\n",
    "\n",
    "    for s, image_batch in enumerate(dataset):\n",
    "      train_step(image_batch)\n",
    "      print(f\"\\r e {epoch}/{epochs}: step {s}\", end=\"\")\n",
    "\n",
    "    stop = time.time()\n",
    "    print(f\"\\r[+] Epoch {epoch}/{epochs}: Gen {generator_metric.result():0.4f}; Disc {discriminator_metric.result():0.4f} sim loss: {similarity_metric.result():0.4f} in {(stop-start):0.4f} seconds; {(stop-start)/s:0.4f} s/step\")\n",
    "\n",
    "\n",
    "    # Make generations on seed\n",
    "    seed_generation = generator(seed, training=False)\n",
    "    buff = plot_to_buff(seed_generation)\n",
    "\n",
    "    batch_signature = signature_on_batch(seed_generation, [0, 1], [2, 3, 4, 5], config.met_params.signature_length)\n",
    "    signature_difference = signature_metric(real_batch_signature, batch_signature)\n",
    "    correlation_metric(signature_difference)\n",
    "\n",
    "    l = [generator_metric.result(), discriminator_metric.result()]\n",
    "    losses.append(l)\n",
    "    log_losses(epoch, buff)\n",
    "\n",
    "  return np.array(losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_losses = train(dset_simulated, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.save(f\"{BASE_DIR}/generator.h5\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 5))\n",
    "plt.title(\"Training Losses.\")\n",
    "plt.plot(training_losses[:, 0], \".-\", label=\"Generator Loss\")\n",
    "plt.plot(training_losses[:, 1], \".-\", label=\"Discriminator Loss\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's Test it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "after_training_generations = generator(seed, training=False)\n",
    "\n",
    "plt.figure(figsize=(18, 5))\n",
    "plt.title(\"Generation of the GAN whitout Training.\")\n",
    "for i in range(after_training_generations.shape[-1]):\n",
    "    plt.plot(after_training_generations[0, :, i], label=f'feat {i+1}')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "after_training_decision = discriminator(after_training_generations)\n",
    "after_training_decision[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test if Mode Colapsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after_training_generations\n",
    "def plot_several_generations(generations:np.ndarray, nvertical:int=3, nhoriz:int=3):\n",
    "\n",
    "    legend = [f\"feat {j}\" for j in range(generations.shape[-1])]\n",
    "\n",
    "    plt.figure(figsize=(18, 10))\n",
    "    plt.suptitle(\"Generations After GAN Training.\")\n",
    "\n",
    "    for i in range(nvertical* nhoriz):\n",
    "        ax = plt.subplot(nvertical, nhoriz, i+ 1)\n",
    "        ax.set_title(f\"sequence {i+1}\")\n",
    "\n",
    "        plt.plot(generations[i])\n",
    "        ax.grid(True)\n",
    "        plt.legend(legend)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_several_generations(after_training_generations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
